# grpo_trainer.py - åˆ é™¤Probability Alignmentåçš„æ¸…æ´ç‰ˆæœ¬
import torch
import torch.nn.functional as F
from typing import List, Dict, Any, Tuple
import numpy as np


class StepwiseProbabilityWordClassifier:
    """åŸºäºstep-wiseå‡†ç¡®ç‡çš„è¯æ±‡éš¾åº¦åˆ†ç±»å™¨ï¼ˆç®€åŒ–ç‰ˆï¼‰"""

    def __init__(self):
        self.performance_history = {}
        self.update_count = {}

    def classify_word(self, word: str, trajectories: List[Dict]) -> str:
        """åŸºäºstep-wiseå‡†ç¡®ç‡åˆ†ç±»è¯æ±‡éš¾åº¦"""
        # è®¡ç®—å½“å‰step-wiseæŒ‡æ ‡
        all_step_accuracies = []

        for traj in trajectories:
            step_accuracies = traj.get("step_accuracies", [])
            all_step_accuracies.extend([1.0 if acc else 0.0 for acc in step_accuracies])

        current_step_accuracy = np.mean(all_step_accuracies) if all_step_accuracies else 0.0
        word_length = len(word)

        # ç»“åˆå†å²æ€§èƒ½
        if word in self.performance_history:
            historical_score = self.performance_history[word]
            hist_weight = min(0.5, self.update_count[word] * 0.1)
            final_score = (1 - hist_weight) * current_step_accuracy + hist_weight * historical_score
        else:
            final_score = current_step_accuracy

        # åŸºäºåˆ†æ•°çš„åˆ†ç±»é€»è¾‘
        if final_score >= 0.75:
            return "PROTECT"  # é«˜æ€§èƒ½ï¼Œéœ€è¦ä¿æŠ¤
        elif final_score >= 0.45:
            return "IMPROVE"  # ä¸­ç­‰æ€§èƒ½ï¼Œæ¸©å’Œæå‡
        elif word_length <= 5:
            return "FOCUS"  # çŸ­è¯ä½æ€§èƒ½ï¼Œé‡ç‚¹æ”»å…‹
        else:
            return "IMPROVE"  # é•¿è¯ä½æ€§èƒ½ï¼Œæ¸©å’Œæå‡

    def update_history(self, word: str, step_accuracy: float):
        """æ›´æ–°è¯æ±‡å†å²æ€§èƒ½"""
        if word in self.performance_history:
            self.performance_history[word] = 0.8 * self.performance_history[word] + 0.2 * step_accuracy
            self.update_count[word] += 1
        else:
            self.performance_history[word] = step_accuracy
            self.update_count[word] = 1

    def get_stats(self) -> Dict[str, Any]:
        """è·å–åˆ†ç±»å™¨ç»Ÿè®¡ä¿¡æ¯"""
        if not self.performance_history:
            return {"total_words": 0}

        categories = {}
        for word, score in self.performance_history.items():
            if score >= 0.75:
                cat = "PROTECT"
            elif score >= 0.45:
                cat = "IMPROVE"
            elif len(word) <= 5:
                cat = "FOCUS"
            else:
                cat = "IMPROVE"
            categories[cat] = categories.get(cat, 0) + 1

        return {
            "total_words": len(self.performance_history),
            "protect_words": categories.get("PROTECT", 0),
            "focus_words": categories.get("FOCUS", 0),
            "improve_words": categories.get("IMPROVE", 0),
            "avg_score": np.mean(list(self.performance_history.values()))
        }


class AdaptiveKLScheduler:
    """è‡ªé€‚åº”KLè°ƒåº¦å™¨"""

    def __init__(self, base_kl_coeff=0.08):
        self.base_kl_coeff = base_kl_coeff
        self.current_kl_coeff = base_kl_coeff
        self.recent_kl_losses = []
        self.recent_policy_losses = []

    def update_kl_coeff(self, epoch_stats):
        """æ ¹æ®è®­ç»ƒæƒ…å†µè‡ªé€‚åº”è°ƒæ•´KLç³»æ•°"""
        avg_kl_loss = epoch_stats.get('avg_kl_loss', 0.0)
        avg_policy_loss = epoch_stats.get('avg_policy_loss', 0.0)

        self.recent_kl_losses.append(avg_kl_loss)
        self.recent_policy_losses.append(avg_policy_loss)

        if len(self.recent_kl_losses) > 5:
            self.recent_kl_losses.pop(0)
            self.recent_policy_losses.pop(0)

        old_kl_coeff = self.current_kl_coeff

        if avg_kl_loss < 0.002:
            self.current_kl_coeff *= 0.85
            reason = "KL too small, reducing constraint"
        elif avg_policy_loss > 0.8:
            self.current_kl_coeff *= 1.25
            reason = "Policy loss too large, increasing constraint"
        else:
            reason = "No adjustment needed"

        self.current_kl_coeff = np.clip(self.current_kl_coeff, 0.01, 0.3)

        if abs(self.current_kl_coeff - old_kl_coeff) > 1e-6:
            print(f"ğŸ”§ KL coeff adjusted: {old_kl_coeff:.4f} â†’ {self.current_kl_coeff:.4f} ({reason})")

        return self.current_kl_coeff


class CleanStepwisePPOTrainer:
    """æ¸…æ´çš„Step-wise PPOè®­ç»ƒå™¨ï¼ˆåˆ é™¤å†—ä½™KLå¯¹é½ï¼‰"""

    def __init__(self, model, reference_model, tokenizer, optimizer,
                 kl_coeff=0.08, clip_epsilon=0.15, device='cuda',
                 max_grad_norm=0.5, base_learning_rate=1e-4,
                 reward_calculator=None):

        self.model = model
        self.reference_model = reference_model
        self.tokenizer = tokenizer
        self.optimizer = optimizer
        self.kl_coeff = kl_coeff
        self.clip_epsilon = clip_epsilon
        self.device = device
        self.max_grad_norm = max_grad_norm
        self.base_learning_rate = base_learning_rate
        self.eps = 1e-8

        # å¥–åŠ±è®¡ç®—å™¨
        self.reward_calculator = reward_calculator

        # è¯æ±‡åˆ†ç±»å™¨å’ŒKLè°ƒåº¦å™¨
        self.difficulty_classifier = StepwiseProbabilityWordClassifier()
        self.kl_scheduler = AdaptiveKLScheduler(kl_coeff)

        # å­—ç¬¦æ˜ å°„
        self.a_z_chars = 'abcdefghijklmnopqrstuvwxyz'
        self.char_to_id = {}
        self._initialize_char_mappings()

        # è®­ç»ƒç»Ÿè®¡
        self.training_stats = {
            "protect_words_trained": 0,
            "focus_words_trained": 0,
            "improve_words_trained": 0,
            "total_updates": 0,
            "total_steps_trained": 0,
            "total_correct_steps": 0,
            "extreme_loss_count": 0,
            "ppo_computation_failures": 0
        }

        print(f"ğŸ¯ Clean Step-wise PPO Trainer initialized:")
        print(f"   A-Z chars mapped: {len(self.char_to_id)}/26")
        print(f"   KL coefficient: {kl_coeff}")
        print(f"   Clip epsilon: {clip_epsilon}")

    def _initialize_char_mappings(self):
        """åˆå§‹åŒ–å­—ç¬¦æ˜ å°„"""
        try:
            if hasattr(self.tokenizer, 'char_to_id') and isinstance(self.tokenizer.char_to_id, dict):
                for c in self.a_z_chars:
                    if c in self.tokenizer.char_to_id:
                        self.char_to_id[c] = self.tokenizer.char_to_id[c]
                print("âœ… Using tokenizer.char_to_id (dict)")
                return
            else:
                print("âš ï¸ Tokenizer char_to_id not found, using fallback mapping")
                self._create_fallback_mappings()
        except Exception as e:
            print(f"âŒ Error initializing character mappings: {e}")
            self._create_fallback_mappings()

    def _create_fallback_mappings(self):
        """åˆ›å»ºfallbackå­—ç¬¦æ˜ å°„"""
        print("ğŸ”§ Creating fallback character mappings...")
        for i, c in enumerate(self.a_z_chars):
            self.char_to_id[c] = 4 + i  # è·³è¿‡æ§åˆ¶tokens

    def train_step_clean_ppo(self, trajectory_groups_batch: List[Dict]) -> Dict[str, Any]:
        """æ¸…æ´çš„Step-wise PPOè®­ç»ƒæ­¥éª¤ï¼ˆåˆ é™¤å†—ä½™å¯¹é½ï¼‰"""
        try:
            if not trajectory_groups_batch:
                return {"total_loss": 0.0, "backprop_success": False}

            print(f"ğŸ”„ Clean Step-wise PPO Training: Processing {len(trajectory_groups_batch)} trajectory groups")

            # ä½¿ç”¨reward calculatorè®¡ç®—å¥–åŠ±
            processed_groups = []
            for group in trajectory_groups_batch:
                if self.reward_calculator:
                    processed_group = self.reward_calculator.calculate_trajectory_group_rewards(group)
                    processed_groups.append(processed_group)
                else:
                    processed_groups.append(group)

            all_policy_losses = []
            all_kl_losses = []
            word_categories = {}
            category_stats = {"PROTECT": 0, "FOCUS": 0, "IMPROVE": 0}
            total_steps = 0
            total_correct_steps = 0
            valid_step_updates = 0

            for group_idx, processed_group in enumerate(processed_groups):
                trajectories = processed_group.get("trajectories", [])
                word = processed_group.get("word", trajectory_groups_batch[group_idx]["word"])

                # åˆ†ç±»è¯æ±‡
                category = self.difficulty_classifier.classify_word(word, trajectories)
                word_categories[word] = category
                category_stats[category] += 1

                print(f"ğŸ¯ Processing group {group_idx + 1}: '{word}' (Category: {category}, {len(trajectories)} trajectories)")

                # æ ¹æ®ç±»åˆ«è°ƒæ•´å­¦ä¹ ç‡
                self._adjust_learning_rate(category)

                # è®¡ç®—step-wise advantages
                if self.reward_calculator:
                    advantages_list = self.reward_calculator.get_stepwise_advantages(
                        {"word": word, "trajectories": trajectories},
                        advantage_mode="combined"
                    )
                else:
                    advantages_list = self._fallback_advantages(trajectories)

                if not advantages_list:
                    print(f"   âš ï¸ No valid advantages for '{word}', skipping")
                    continue

                # æ ¹æ®ç±»åˆ«è®¾ç½®KLæƒé‡
                kl_weight = self._get_kl_weight(category)

                # è®¡ç®—step-wise policy losses
                group_policy_losses = []
                group_kl_losses = []
                group_steps = 0
                group_correct_steps = 0

                for traj_idx, (traj, step_advantages) in enumerate(zip(trajectories, advantages_list)):
                    step_accuracies = traj.get('step_accuracies', [])
                    steps = traj.get('steps', [])

                    group_steps += len(step_accuracies)
                    group_correct_steps += sum(step_accuracies)

                    # å¯¹æ¯ä¸€æ­¥è®¡ç®—PPO loss
                    for step_idx, (step, advantage) in enumerate(zip(steps, step_advantages)):
                        step_policy_loss, step_kl_loss = self._compute_step_ppo_loss(
                            step, advantage, traj_idx, step_idx, word, category
                        )

                        if step_policy_loss is not None:
                            group_policy_losses.append(step_policy_loss)
                            group_kl_losses.append(step_kl_loss * kl_weight)
                            valid_step_updates += 1
                        else:
                            self.training_stats["ppo_computation_failures"] += 1

                total_steps += group_steps
                total_correct_steps += group_correct_steps

                if group_policy_losses:
                    group_avg_policy = torch.stack(group_policy_losses).mean()
                    group_avg_kl = torch.stack(group_kl_losses).mean()

                    # åº”ç”¨ç±»åˆ«æƒé‡
                    category_weight = self._get_category_weight(category)
                    weighted_policy_loss = group_avg_policy * category_weight

                    # æ£€æŸ¥losså¼‚å¸¸
                    if torch.abs(weighted_policy_loss) > 3.0:
                        print(f"   âš ï¸ Extreme loss detected for '{word}' ({category}): {weighted_policy_loss.item():.4f}, clamping")
                        weighted_policy_loss = torch.clamp(weighted_policy_loss, -3.0, 3.0)
                        self.training_stats["extreme_loss_count"] += 1

                    all_policy_losses.append(weighted_policy_loss)
                    all_kl_losses.append(group_avg_kl)

                    step_accuracy = group_correct_steps / max(group_steps, 1)
                    print(f"   âœ… Group '{word}' ({category}): {valid_step_updates} step updates, "
                          f"step_accuracy={step_accuracy:.1%}, "
                          f"policy_loss={group_avg_policy.item():.4f}, kl_loss={group_avg_kl.item():.4f}")
                else:
                    print(f"   âŒ No valid policy losses for '{word}' - all steps failed")

                # æ›´æ–°å†å²æ€§èƒ½
                if group_steps > 0:
                    current_step_accuracy = group_correct_steps / group_steps
                    self.difficulty_classifier.update_history(word, current_step_accuracy)

            if not all_policy_losses:
                print("âŒ No valid step-wise PPO losses computed")
                return {
                    "total_loss": 0.0,
                    "backprop_success": False,
                    "valid_step_updates": 0,
                    "ppo_computation_failures": self.training_stats["ppo_computation_failures"]
                }

            # åˆå¹¶losses - åªæœ‰Policy Loss + KL Loss
            total_policy_loss = torch.stack(all_policy_losses).mean()
            total_kl_loss = torch.stack(all_kl_losses).mean()

            # æœ€ç»ˆæŸå¤±ï¼šçº¯PPO
            current_kl_coeff = self.kl_scheduler.current_kl_coeff
            total_loss = total_policy_loss + current_kl_coeff * total_kl_loss

            # å®‰å…¨æ£€æŸ¥
            if torch.isnan(total_loss) or torch.isinf(total_loss) or torch.abs(total_loss) > 15.0:
                print(f"âŒ Invalid final loss: {total_loss}, skipping update")
                return {"total_loss": 0.0, "backprop_success": False}

            # æ‰§è¡Œåå‘ä¼ æ’­
            success = self._perform_safe_backprop(total_loss)

            # æ›´æ–°è®­ç»ƒç»Ÿè®¡
            self.training_stats["total_updates"] += 1
            self.training_stats["total_steps_trained"] += total_steps
            self.training_stats["total_correct_steps"] += total_correct_steps
            for category, count in category_stats.items():
                self.training_stats[f"{category.lower()}_words_trained"] += count

            overall_step_accuracy = total_correct_steps / max(total_steps, 1)

            return {
                "total_loss": total_loss,
                "policy_loss": total_policy_loss,
                "kl_loss": total_kl_loss,
                "current_kl_coeff": current_kl_coeff,
                "backprop_success": success,
                "total_steps": total_steps,
                "total_correct_steps": total_correct_steps,
                "overall_step_accuracy": overall_step_accuracy,
                "valid_step_updates": valid_step_updates,
                "word_categories": word_categories,
                "category_stats": category_stats,
                "extreme_loss_count": self.training_stats["extreme_loss_count"],
                "ppo_computation_failures": self.training_stats["ppo_computation_failures"]
            }

        except Exception as e:
            print(f"âŒ Error in clean step-wise PPO training step: {e}")
            import traceback
            traceback.print_exc()
            return {"total_loss": 0.0, "backprop_success": False}

    def _compute_step_ppo_loss(self, step: Dict, advantage: float,
                               traj_idx: int, step_idx: int, word: str, category: str) -> Tuple:
        """è®¡ç®—å•ä¸ªæ­¥éª¤çš„PPOæŸå¤±ï¼ˆçº¯PPOï¼Œæ— é¢å¤–å¯¹é½ï¼‰"""
        try:
            # è®¡ç®—å½“å‰ç­–ç•¥å’Œå‚è€ƒç­–ç•¥çš„logæ¦‚ç‡
            current_log_prob = self._compute_step_log_prob(step, use_reference=False)
            reference_log_prob = self._compute_step_log_prob(step, use_reference=True)

            if current_log_prob is None or reference_log_prob is None:
                return None, None

            # è½¬æ¢advantageä¸ºtensor
            if not isinstance(advantage, torch.Tensor):
                advantage = torch.tensor(float(advantage), dtype=torch.float32, device=self.device)

            # è®¡ç®—policy ratio
            log_ratio = current_log_prob - reference_log_prob
            log_ratio = torch.clamp(log_ratio, -10.0, 10.0)
            ratio = torch.exp(log_ratio)

            # æ ¹æ®ç±»åˆ«è°ƒæ•´clippingèŒƒå›´
            if category == "PROTECT":
                clip_epsilon = self.clip_epsilon * 0.7  # æ›´ä¿å®ˆ
            elif category == "FOCUS":
                clip_epsilon = self.clip_epsilon * 1.3  # ç¨å¾®æ”¾å®½
            else:
                clip_epsilon = self.clip_epsilon

            # æ ‡å‡†PPO clipped policy loss
            policy_loss_1 = ratio * advantage
            policy_loss_2 = torch.clamp(ratio, 1 - clip_epsilon, 1 + clip_epsilon) * advantage
            policy_loss = -torch.min(policy_loss_1, policy_loss_2)

            # é™åˆ¶å•æ­¥loss
            if category == "PROTECT":
                policy_loss = torch.clamp(policy_loss, -1.5, 1.5)
            elif category == "FOCUS":
                policy_loss = torch.clamp(policy_loss, -2.5, 2.5)
            else:
                policy_loss = torch.clamp(policy_loss, -2.0, 2.0)

            # è®¡ç®—KLæ•£åº¦ï¼ˆåªæœ‰Policy KLï¼‰
            kl_loss = self._compute_step_kl_divergence(step)
            if kl_loss is None:
                kl_loss = torch.tensor(0.0, device=self.device)

            # è°ƒè¯•ä¿¡æ¯
            if step_idx < 2 and traj_idx < 2:
                print(f"   Step {step_idx}: action='{step.get('action', 'unknown')}', "
                      f"advantage={advantage.item():.3f}, "
                      f"policy_loss={policy_loss.item():.4f}, "
                      f"kl_loss={kl_loss.item():.4f}, "
                      f"ratio={ratio.item():.3f}")

            return policy_loss, kl_loss

        except Exception as e:
            print(f"âŒ Error computing step PPO loss for step {step_idx} in '{word}': {e}")
            return None, None

    def _compute_step_log_prob(self, step: Dict, use_reference: bool = False) -> torch.Tensor:
        """è®¡ç®—å•ä¸ªæ­¥éª¤çš„logæ¦‚ç‡"""
        try:
            model = self.reference_model if use_reference else self.model
            prompt = step["prompt"]
            action = step["action"]

            # ç¼–ç prompt
            encoded = self._encode_prompt(prompt)
            if not encoded:
                return None

            input_ids = torch.tensor([encoded], dtype=torch.long).to(self.device)

            # è·å–action token id
            action_token_id = self.char_to_id.get(action.lower(), None)
            if action_token_id is None:
                return None

            # å‰å‘ä¼ æ’­
            with torch.no_grad() if use_reference else torch.enable_grad():
                try:
                    outputs = model(x=input_ids)

                    # æ£€æŸ¥è¾“å‡ºç±»å‹å¹¶æå–logits
                    if hasattr(outputs, 'logits'):
                        logits = outputs.logits
                    elif hasattr(outputs, 'last_hidden_state'):
                        logits = outputs.last_hidden_state
                    else:
                        logits = outputs

                    if logits.dim() != 3:
                        return None

                    last_logits = logits[0, -1, :]
                    log_probs = F.log_softmax(last_logits, dim=-1)
                    action_log_prob = log_probs[action_token_id]

                    if torch.isnan(action_log_prob) or torch.isinf(action_log_prob):
                        return None

                    return action_log_prob

                except Exception as e:
                    return None

        except Exception as e:
            return None

    def _compute_step_kl_divergence(self, step: Dict) -> torch.Tensor:
        """è®¡ç®—å•ä¸ªæ­¥éª¤çš„KLæ•£åº¦ï¼ˆPolicy KL onlyï¼‰"""
        try:
            prompt = step["prompt"]
            encoded = self._encode_prompt(prompt)
            if not encoded:
                return None

            input_ids = torch.tensor([encoded], dtype=torch.long).to(self.device)

            try:
                # å½“å‰ç­–ç•¥
                current_outputs = self.model(x=input_ids)
                if hasattr(current_outputs, 'logits'):
                    current_logits = current_outputs.logits
                elif hasattr(current_outputs, 'last_hidden_state'):
                    current_logits = current_outputs.last_hidden_state
                else:
                    current_logits = current_outputs

                current_last_logits = current_logits[0, -1, :]
                current_log_probs = F.log_softmax(current_last_logits, dim=-1)

                # å‚è€ƒç­–ç•¥
                with torch.no_grad():
                    ref_outputs = self.reference_model(x=input_ids)
                    if hasattr(ref_outputs, 'logits'):
                        ref_logits = ref_outputs.logits
                    elif hasattr(ref_outputs, 'last_hidden_state'):
                        ref_logits = ref_outputs.last_hidden_state
                    else:
                        ref_logits = ref_outputs

                    ref_last_logits = ref_logits[0, -1, :]
                    ref_probs = F.softmax(ref_last_logits, dim=-1)

                # è®¡ç®—KLæ•£åº¦ï¼ˆåªæœ‰Policy KLï¼‰
                step_kl = F.kl_div(current_log_probs, ref_probs, reduction='sum')

                if torch.isnan(step_kl) or torch.isinf(step_kl):
                    return None

                return step_kl

            except Exception as e:
                return None

        except Exception as e:
            return None

    def _encode_prompt(self, prompt: str) -> List[int]:
        """ç¼–ç æç¤º"""
        try:
            if hasattr(self.tokenizer, 'encode'):
                encoded = self.tokenizer.encode(prompt, add_special_tokens=False)
                return encoded
            else:
                return []
        except Exception as e:
            return []

    def _fallback_advantages(self, trajectories: List[Dict]) -> List[List[float]]:
        """å¤‡ç”¨advantageè®¡ç®—"""
        trajectory_advantages = []
        for traj in trajectories:
            steps = traj.get('steps', [])
            step_accuracies = traj.get('step_accuracies', [True] * len(steps))

            advantages = []
            for is_correct in step_accuracies:
                advantage = 0.5 if is_correct else -0.5
                advantages.append(advantage)

            trajectory_advantages.append(advantages)

        return trajectory_advantages

    def _adjust_learning_rate(self, category: str):
        """æ ¹æ®ç±»åˆ«è°ƒæ•´å­¦ä¹ ç‡"""
        if category == "PROTECT":
            lr_multiplier = 0.6
        elif category == "FOCUS":
            lr_multiplier = 1.4
        else:
            lr_multiplier = 1.0

        new_lr = self.base_learning_rate * lr_multiplier
        for param_group in self.optimizer.param_groups:
            param_group['lr'] = new_lr

    def _get_kl_weight(self, category: str) -> float:
        """è·å–KLæƒé‡"""
        current_kl_coeff = self.kl_scheduler.current_kl_coeff

        if category == "PROTECT":
            return current_kl_coeff * 1.5
        elif category == "FOCUS":
            return current_kl_coeff * 0.7
        else:
            return current_kl_coeff * 1.0

    def _get_category_weight(self, category: str) -> float:
        """è·å–ç±»åˆ«æƒé‡"""
        if category == "FOCUS":
            return 1.5
        elif category == "PROTECT":
            return 0.7
        else:
            return 1.0

    def _perform_safe_backprop(self, total_loss) -> bool:
        """å®‰å…¨çš„åå‘ä¼ æ’­"""
        try:
            if isinstance(total_loss, torch.Tensor) and not (torch.isnan(total_loss) or torch.isinf(total_loss)):
                self.optimizer.zero_grad()
                total_loss.backward()

                grad_norm = torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.max_grad_norm)

                if torch.isnan(grad_norm) or torch.isinf(grad_norm) or grad_norm > 100.0:
                    print(f"âŒ Extreme gradient norm detected: {grad_norm:.4f}, skipping update")
                    return False

                self.optimizer.step()
                print(f"âœ… Clean Step-wise PPO Backprop successful, loss: {total_loss.item():.6f}, grad_norm: {grad_norm:.4f}")
                return True
            else:
                print(f"âŒ Invalid loss for backprop: {total_loss}")
                return False

        except Exception as e:
            print(f"âŒ Safe backprop failed: {e}")
            return False

    def update_epoch_stats(self, epoch_stats):
        """æ›´æ–°epochç»Ÿè®¡ä¿¡æ¯å¹¶è°ƒæ•´KLç³»æ•°"""
        new_kl_coeff = self.kl_scheduler.update_kl_coeff(epoch_stats)
        return new_kl_coeff

    def get_training_stats(self) -> Dict[str, Any]:
        """è·å–è®­ç»ƒç»Ÿè®¡ä¿¡æ¯"""
        classifier_stats = self.difficulty_classifier.get_stats()

        total_steps = self.training_stats["total_steps_trained"]
        total_correct = self.training_stats["total_correct_steps"]

        overall_step_accuracy = total_correct / max(total_steps, 1)

        return {
            **self.training_stats,
            **classifier_stats,
            "overall_step_accuracy": overall_step_accuracy,
            "current_kl_coeff": self.kl_scheduler.current_kl_coeff,
            "current_base_lr": self.base_learning_rate
        }

    def reset_training_stats(self):
        """é‡ç½®è®­ç»ƒç»Ÿè®¡"""
        self.training_stats = {
            "protect_words_trained": 0,
            "focus_words_trained": 0,
            "improve_words_trained": 0,
            "total_updates": 0,
            "total_steps_trained": 0,
            "total_correct_steps": 0,
            "extreme_loss_count": 0,
            "ppo_computation_failures": 0
        }


# ä½¿ç”¨æ¸…æ´ç‰ˆæœ¬çš„è®­ç»ƒå™¨
StepwisePPOTrainer = CleanStepwisePPOTrainer