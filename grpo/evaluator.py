# evaluator.py - Step-wiseå‡†ç¡®ç‡è¯„ä¼°å™¨ï¼ˆå®Œæ•´ä¿®å¤ç‰ˆï¼‰
import torch
import torch.nn.functional as F
import numpy as np
from typing import List, Dict, Any, Set
from collections import defaultdict, Counter


class StepWiseHangmanEvaluator:
    """åŸºäºStep-wiseå‡†ç¡®ç‡çš„Hangmanè¯„ä¼°å™¨"""

    def __init__(self, model, tokenizer, device='cuda',
                 temperature: float = 1.0,
                 max_steps: int = 10):
        self.model = model
        self.tokenizer = tokenizer
        self.device = device
        self.temperature = temperature
        self.max_steps = max_steps

        # ğŸ¯ é¢„è®¡ç®—a-zå­—ç¬¦æ˜ å°„ - åŸºäºå®é™…tokenizerç»“æ„
        self.a_z_chars = 'abcdefghijklmnopqrstuvwxyz'
        self.char_to_id = {}
        self.id_to_char = {}
        self._initialize_char_mappings()

        print(f"ğŸ¯ StepWiseHangmanEvaluator initialized:")
        print(f"   Device: {device}")
        print(f"   Temperature: {temperature}")
        print(f"   Max steps: {max_steps}")
        print(f"   A-Z chars mapped: {len(self.char_to_id)}/26")

    def _initialize_char_mappings(self):
        """ğŸ”§ åˆå§‹åŒ–å­—ç¬¦æ˜ å°„ - åŸºäºå®é™…tokenizerç»“æ„"""
        try:
            # ğŸ”§ ä¿®å¤ï¼šç›´æ¥è®¿é—®tokenizerçš„char_to_idå­—å…¸
            if hasattr(self.tokenizer, 'char_to_id') and isinstance(self.tokenizer.char_to_id, dict):
                for c in self.a_z_chars:
                    if c in self.tokenizer.char_to_id:
                        token_id = self.tokenizer.char_to_id[c]
                        self.char_to_id[c] = token_id
                        self.id_to_char[token_id] = c
                print("âœ… Using tokenizer.char_to_id (dict)")

                # æ˜¾ç¤ºæ˜ å°„ç¤ºä¾‹
                sample_mappings = {c: self.char_to_id[c] for c in ['a', 'z'] if c in self.char_to_id}
                print(f"   Sample mappings: {sample_mappings}")
                return
            else:
                print("âš ï¸ Tokenizer char_to_id not found or not a dict, using fallback mapping")
                self._create_fallback_mappings()

        except Exception as e:
            print(f"âŒ Error initializing character mappings: {e}")
            self._create_fallback_mappings()

    def _create_fallback_mappings(self):
        """åˆ›å»ºfallbackå­—ç¬¦æ˜ å°„"""
        print("ğŸ”§ Creating fallback character mappings...")
        # æ ¹æ®å®é™…tokenizerç»“æ„ï¼Œa-zåœ¨ä½ç½®4-29ï¼ˆè·³è¿‡æ§åˆ¶tokensï¼‰
        for i, c in enumerate(self.a_z_chars):
            token_id = 4 + i  # è·³è¿‡ <pad>, <bos>, <eos>, <unk>
            self.char_to_id[c] = token_id
            self.id_to_char[token_id] = c

        sample_mappings = {c: self.char_to_id[c] for c in ['a', 'z']}
        print(f"âœ… Fallback mappings created: {sample_mappings}")

    def evaluate(self, word_list: List[str], num_games_per_word: int = 1) -> Dict[str, Any]:
        """
        è¯„ä¼°æ¨¡å‹åœ¨å•æ­¥å‡†ç¡®ç‡ä¸Šçš„è¡¨ç°

        Returns:
            {
                'step_accuracy': float,  # å•æ­¥å‡†ç¡®ç‡
                'game_success_rate': float,  # æ¸¸æˆèƒœç‡ï¼ˆå‘åå…¼å®¹ï¼‰
                'avg_steps_per_game': float,  # å¹³å‡æ­¥æ•°
                'avg_kl_divergence': float,  # å¹³å‡KLæ•£åº¦
                'avg_prediction_confidence': float,  # å¹³å‡é¢„æµ‹ç½®ä¿¡åº¦
                'total_games': int,  # æ€»æ¸¸æˆæ•°
                'total_steps': int,  # æ€»æ­¥æ•°
                'correct_steps': int,  # æ­£ç¡®æ­¥æ•°
                'detailed_stats': Dict,  # è¯¦ç»†ç»Ÿè®¡
            }
        """
        self.model.eval()

        # ç»Ÿè®¡å˜é‡
        total_games = 0
        successful_games = 0
        total_steps = 0
        correct_steps = 0
        total_kl_divergence = 0.0
        total_confidence = 0.0
        step_count = 0

        # è¯¦ç»†ç»Ÿè®¡
        word_length_stats = defaultdict(list)
        step_position_accuracy = defaultdict(list)
        kl_divergence_history = []
        confidence_history = []

        game_results = []

        print(f"ğŸ¯ Starting step-wise evaluation on {len(word_list)} words...")

        for word_idx, word in enumerate(word_list):
            word = word.lower()

            for game_idx in range(num_games_per_word):
                try:
                    game_result = self._evaluate_single_game(word, word_idx, game_idx)

                    if game_result:
                        game_results.append(game_result)
                        total_games += 1

                        # æ¸¸æˆçº§åˆ«ç»Ÿè®¡
                        if game_result['success']:
                            successful_games += 1

                        # Stepçº§åˆ«ç»Ÿè®¡
                        game_steps = game_result['total_steps']
                        game_correct = game_result['correct_steps']

                        total_steps += game_steps
                        correct_steps += game_correct

                        # KLæ•£åº¦å’Œç½®ä¿¡åº¦
                        if game_result['avg_kl_divergence'] > 0:
                            total_kl_divergence += game_result['avg_kl_divergence']
                            kl_divergence_history.append(game_result['avg_kl_divergence'])

                        if game_result['avg_confidence'] > 0:
                            total_confidence += game_result['avg_confidence']
                            confidence_history.append(game_result['avg_confidence'])

                        step_count += 1

                        # è¯é•¿åº¦ç»Ÿè®¡
                        word_length_stats[len(word)].append(game_correct / max(game_steps, 1))

                        # æ­¥éª¤ä½ç½®å‡†ç¡®ç‡
                        for step_info in game_result['step_details']:
                            pos = step_info['step_position']
                            is_correct = step_info['is_correct']
                            step_position_accuracy[pos].append(1.0 if is_correct else 0.0)

                except Exception as e:
                    print(f"âŒ Error evaluating word '{word}' game {game_idx}: {e}")
                    continue

            # è¿›åº¦æŠ¥å‘Š
            if (word_idx + 1) % 20 == 0:
                current_step_acc = correct_steps / max(total_steps, 1)
                current_game_acc = successful_games / max(total_games, 1)
                print(f"   Progress: {word_idx + 1}/{len(word_list)} words, "
                      f"step_acc={current_step_acc:.3f}, game_acc={current_game_acc:.3f}")

        # è®¡ç®—æœ€ç»ˆæŒ‡æ ‡
        step_accuracy = correct_steps / max(total_steps, 1)
        game_success_rate = successful_games / max(total_games, 1)
        avg_steps_per_game = total_steps / max(total_games, 1)
        avg_kl_divergence = total_kl_divergence / max(step_count, 1)
        avg_confidence = total_confidence / max(step_count, 1)

        # è¯¦ç»†ç»Ÿè®¡
        detailed_stats = {
            'word_length_accuracy': {
                length: np.mean(accs) for length, accs in word_length_stats.items()
            },
            'step_position_accuracy': {
                pos: np.mean(accs) for pos, accs in step_position_accuracy.items()
            },
            'kl_divergence_std': np.std(kl_divergence_history) if kl_divergence_history else 0.0,
            'confidence_std': np.std(confidence_history) if confidence_history else 0.0,
            'total_violations': sum(gr.get('violations', 0) for gr in game_results),
        }

        results = {
            'step_accuracy': step_accuracy,
            'game_success_rate': game_success_rate,
            'avg_steps_per_game': avg_steps_per_game,
            'avg_kl_divergence': avg_kl_divergence,
            'avg_prediction_confidence': avg_confidence,
            'total_games': total_games,
            'total_steps': total_steps,
            'correct_steps': correct_steps,
            'successful_games': successful_games,
            'detailed_stats': detailed_stats,

            # å‘åå…¼å®¹å­—æ®µ
            'success_rate': game_success_rate,
            'avg_steps': avg_steps_per_game,
            'avg_reward': step_accuracy * 10 - 5,  # ç®€å•çš„rewardæ˜ å°„
        }

        print(f"\nğŸ¯ Step-wise Evaluation Results:")
        print(f"   Step Accuracy: {step_accuracy:.1%} ({correct_steps}/{total_steps})")
        print(f"   Game Success Rate: {game_success_rate:.1%} ({successful_games}/{total_games})")
        print(f"   Avg Steps per Game: {avg_steps_per_game:.2f}")
        print(f"   Avg KL Divergence: {avg_kl_divergence:.4f}")
        print(f"   Avg Prediction Confidence: {avg_confidence:.3f}")

        return results

    def _evaluate_single_game(self, word: str, word_idx: int, game_idx: int) -> Dict[str, Any]:
        """è¯„ä¼°å•ä¸ªæ¸¸æˆ"""
        guessed_letters = set()
        wrong_guesses = 0
        max_wrong = 6

        game_result = {
            'word': word,
            'word_idx': word_idx,
            'game_idx': game_idx,
            'success': False,
            'total_steps': 0,
            'correct_steps': 0,
            'wrong_guesses': 0,
            'step_details': [],
            'avg_kl_divergence': 0.0,
            'avg_confidence': 0.0,
            'violations': 0,
            'failure_reason': ''
        }

        # åˆå§‹åŒ–æ¸¸æˆçŠ¶æ€
        current_state = ['_' if c.isalpha() else c for c in word]
        kl_divergences = []
        confidences = []

        for step_idx in range(self.max_steps):
            # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„Hangman promptæ ¼å¼
            prompt = self._build_hangman_prompt(current_state, guessed_letters, wrong_guesses)

            # ğŸ¯ é¢„æµ‹å¹¶è®°å½•æ¦‚ç‡åˆ†å¸ƒ
            prediction_result = self._predict_with_probabilities(prompt, word, guessed_letters)

            if not prediction_result:
                game_result['failure_reason'] = 'prediction_failed'
                break

            predicted_char = prediction_result['predicted_char']
            model_dist = prediction_result.get('model_distribution', {})
            ideal_dist = prediction_result.get('ideal_distribution', {})
            kl_div = prediction_result.get('kl_divergence', 0.0)
            confidence = prediction_result.get('confidence', 0.0)

            # æ£€æŸ¥è¿è§„
            is_violation = predicted_char in guessed_letters
            if is_violation:
                game_result['violations'] += 1

            # åˆ¤æ–­æ˜¯å¦æ­£ç¡®
            is_correct = predicted_char in word and not is_violation

            # è®°å½•stepè¯¦æƒ…
            step_detail = {
                'step_position': step_idx,
                'predicted_char': predicted_char,
                'is_correct': is_correct,
                'is_violation': is_violation,
                'confidence': confidence,
                'kl_divergence': kl_div,
                'prompt': prompt,
                'current_state': ''.join(current_state),
                'model_distribution': model_dist,
                'ideal_distribution': ideal_dist
            }
            game_result['step_details'].append(step_detail)

            # æ›´æ–°ç»Ÿè®¡
            game_result['total_steps'] += 1
            if is_correct:
                game_result['correct_steps'] += 1

            if kl_div > 0:
                kl_divergences.append(kl_div)
            if confidence > 0:
                confidences.append(confidence)

            # æ›´æ–°æ¸¸æˆçŠ¶æ€
            guessed_letters.add(predicted_char)

            if predicted_char in word:
                # çŒœå¯¹äº†
                for i, c in enumerate(word):
                    if c == predicted_char:
                        current_state[i] = c

                # æ£€æŸ¥æ¸¸æˆæ˜¯å¦å®Œæˆ
                if '_' not in current_state:
                    game_result['success'] = True
                    break
            else:
                # çŒœé”™äº†
                wrong_guesses += 1
                game_result['wrong_guesses'] = wrong_guesses
                if wrong_guesses >= max_wrong:
                    game_result['failure_reason'] = 'too_many_wrong'
                    break

            # è¿è§„ä¹Ÿç®—å¤±è´¥
            if is_violation:
                game_result['failure_reason'] = 'violation'
                break

        # å¦‚æœæ²¡æœ‰æ˜ç¡®å¤±è´¥åŸå› ä¸”æœªæˆåŠŸ
        if not game_result['success'] and not game_result['failure_reason']:
            game_result['failure_reason'] = 'max_steps_reached'

        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        game_result['avg_kl_divergence'] = np.mean(kl_divergences) if kl_divergences else 0.0
        game_result['avg_confidence'] = np.mean(confidences) if confidences else 0.0

        return game_result

    def _build_hangman_prompt(self, current_state: List[str], guessed_letters: Set[str],
                              wrong_guesses: int) -> str:
        """ğŸ”§ æ„å»ºæ ‡å‡†Hangman promptæ ¼å¼ï¼šword_state[SEP]guessed_letters[SEP]wrong/max"""
        # ç¬¬ä¸€éƒ¨åˆ†ï¼šå½“å‰è¯çŠ¶æ€
        word_state = ''.join(current_state)

        # ç¬¬äºŒéƒ¨åˆ†ï¼šå·²çŒœå­—æ¯ï¼ˆæŒ‰å­—æ¯é¡ºåºï¼‰
        if guessed_letters:
            guessed_str = ','.join(sorted(guessed_letters))
        else:
            guessed_str = ''

        # ç¬¬ä¸‰éƒ¨åˆ†ï¼šé”™è¯¯æ¬¡æ•°
        wrong_str = f"{wrong_guesses}/6"

        # ğŸ¯ æ ‡å‡†Hangmanæ ¼å¼
        prompt = f"{word_state}[SEP]{guessed_str}[SEP]{wrong_str}"

        return prompt

    def _predict_with_probabilities(self, prompt: str, word: str,
                                    guessed_letters: Set[str]) -> Dict[str, Any]:
        """ğŸ¯ å¸¦æ¦‚ç‡åˆ†å¸ƒçš„é¢„æµ‹"""
        try:
            # ç¼–ç è¾“å…¥
            encoded = self._encode_prompt(prompt)
            if not encoded:
                return None

            input_ids = torch.tensor([encoded], dtype=torch.long).to(self.device)

            with torch.no_grad():
                # ğŸ”§ ä¿®å¤ï¼šå¤„ç†ModelOutputå¯¹è±¡
                outputs = self.model(x=input_ids)

                # æ£€æŸ¥è¾“å‡ºç±»å‹å¹¶æå–logits
                if hasattr(outputs, 'logits'):
                    logits = outputs.logits
                elif hasattr(outputs, 'last_hidden_state'):
                    logits = outputs.last_hidden_state
                else:
                    logits = outputs

                # ç¡®ä¿logitså½¢çŠ¶æ­£ç¡®
                if logits.dim() != 3:
                    print(f"âŒ Unexpected logits shape: {logits.shape}")
                    return None

                last_logits = logits[0, -1, :]  # æœ€åä½ç½®çš„logits

                # åº”ç”¨æ¸©åº¦
                scaled_logits = last_logits / self.temperature

                # æå–a-zå­—ç¬¦çš„logits
                a_z_indices = []
                valid_chars = []

                for c in self.a_z_chars:
                    if c in self.char_to_id:
                        a_z_indices.append(self.char_to_id[c])
                        valid_chars.append(c)

                if not a_z_indices:
                    return None

                a_z_logits = scaled_logits[a_z_indices]
                a_z_probs = F.softmax(a_z_logits, dim=-1)

                # æ„å»ºæ¨¡å‹åˆ†å¸ƒ
                model_distribution = {}
                for i, char in enumerate(valid_chars):
                    model_distribution[char] = float(a_z_probs[i])

                # ä¸ºæ²¡æœ‰æ˜ å°„çš„å­—ç¬¦å¡«å……æå°å€¼
                for c in self.a_z_chars:
                    if c not in model_distribution:
                        model_distribution[c] = 1e-8

                # è®¡ç®—ç†æƒ³åˆ†å¸ƒ
                ideal_distribution = self._calculate_ideal_distribution(word, guessed_letters)

                # è®¡ç®—KLæ•£åº¦
                kl_divergence = self._calculate_kl_divergence(model_distribution, ideal_distribution)

                # é¢„æµ‹å­—ç¬¦ï¼ˆè´ªå¿ƒç­–ç•¥ç”¨äºè¯„ä¼°ï¼‰
                pred_idx = torch.argmax(a_z_probs).item()
                predicted_char = valid_chars[pred_idx]
                confidence = float(a_z_probs[pred_idx])

                return {
                    'predicted_char': predicted_char,
                    'model_distribution': model_distribution,
                    'ideal_distribution': ideal_distribution,
                    'kl_divergence': kl_divergence,
                    'confidence': confidence
                }

        except Exception as e:
            print(f"âŒ Error in probability prediction: {e}")
            return None

    def _calculate_ideal_distribution(self, word: str, guessed_letters: Set[str]) -> Dict[str, float]:
        """è®¡ç®—ç†æƒ³æ¦‚ç‡åˆ†å¸ƒ"""
        unguessed_chars = set(word) - guessed_letters

        if not unguessed_chars:
            return {c: 1.0 / 26 for c in self.a_z_chars}

        # è®¡ç®—é¢‘ç‡
        char_counts = Counter(c for c in word if c in unguessed_chars)
        total_unguessed = sum(char_counts.values())

        distribution = {}
        for c in self.a_z_chars:
            if c in char_counts:
                distribution[c] = char_counts[c] / total_unguessed * 0.8
            elif c in guessed_letters:
                distribution[c] = 0.01 / 26
            else:
                distribution[c] = 0.2 / 26

        # å½’ä¸€åŒ–
        total_prob = sum(distribution.values())
        if total_prob > 0:
            distribution = {c: p / total_prob for c, p in distribution.items()}

        return distribution

    def _calculate_kl_divergence(self, model_dist: Dict[str, float],
                                 ideal_dist: Dict[str, float]) -> float:
        """è®¡ç®—KLæ•£åº¦"""
        try:
            kl_div = 0.0
            for char in self.a_z_chars:
                p = ideal_dist.get(char, 1e-8)
                q = model_dist.get(char, 1e-8)

                p = max(p, 1e-8)
                q = max(q, 1e-8)

                kl_div += p * np.log(p / q)

            return float(kl_div)
        except:
            return 0.0

    def _encode_prompt(self, prompt: str) -> List[int]:
        """ç¼–ç æç¤º"""
        try:
            if hasattr(self.tokenizer, 'encode'):
                encoded = self.tokenizer.encode(prompt, add_special_tokens=False)
                return encoded
            else:
                print(f"âŒ Tokenizer does not have encode method")
                return []
        except Exception as e:
            print(f"âŒ Error encoding prompt: {e}")
            return []

    def quick_evaluate(self, word_list: List[str], max_words: int = 50) -> Dict[str, Any]:
        """å¿«é€Ÿè¯„ä¼°ï¼ˆç”¨äºè®­ç»ƒæœŸé—´ï¼‰"""
        limited_words = word_list[:max_words] if len(word_list) > max_words else word_list
        return self.evaluate(limited_words, num_games_per_word=1)

    def _compute_eval_metrics(self, dataloader, metric_key_prefix="eval"):
        """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
        model = self.model
        model.eval()

        total_samples = 0
        correct_predictions = 0
        total_loss = 0.0
        num_batches = 0

        try:
            model_device = next(model.parameters()).device
        except StopIteration:
            model_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        print(f"ğŸ” Computing eval metrics on device: {model_device}")

        with torch.no_grad():
            for batch_idx, batch in enumerate(dataloader):
                try:
                    # é™åˆ¶è¯„ä¼°batchæ•°é‡ä»¥èŠ‚çœæ—¶é—´
                    if batch_idx >= 50:
                        break

                    # ç§»åŠ¨åˆ°è®¾å¤‡
                    input_ids = batch['input_ids'].to(model_device)
                    attention_mask = batch['attention_mask'].to(model_device)
                    soft_targets_tensor = batch['soft_targets_tensor'].to(model_device)
                    prompt_lengths = batch['prompt_length'].to(model_device)

                    if input_ids.size(0) == 0:
                        continue

                    # ğŸ”§ ä¿®å¤ï¼šå¤„ç†ModelOutputå¯¹è±¡
                    outputs = model(x=input_ids, attn_mask=attention_mask)

                    if hasattr(outputs, 'logits'):
                        logits = outputs.logits
                    else:
                        logits = outputs

                    # è®¡ç®—loss
                    batch_loss = self._compute_soft_targets_loss(
                        logits, soft_targets_tensor, prompt_lengths
                    )

                    if batch_loss is not None:
                        total_loss += batch_loss.item()
                        num_batches += 1

                    # é€æ ·æœ¬åˆ†æ
                    batch_size = input_ids.size(0)
                    for i in range(batch_size):
                        try:
                            # è·å–æ ·æœ¬ä¿¡æ¯
                            prompt = batch['prompt'][i]
                            true_completion = batch['completion'][i]
                            prompt_length = prompt_lengths[i].item()

                            if prompt_length > 0 and prompt_length <= logits.size(1):
                                # é¢„æµ‹completion
                                pred_logits = logits[i, prompt_length - 1, :]

                                # é™åˆ¶è¾“å‡ºåˆ°a-zå­—ç¬¦
                                a_z_indices = [self.tokenizer.char_to_id[c] for c in
                                               'abcdefghijklmnopqrstuvwxyz'
                                               if c in self.tokenizer.char_to_id]

                                # åˆ›å»ºmask
                                masked_logits = torch.full_like(pred_logits, float('-inf'))
                                if len(a_z_indices) > 0:
                                    masked_logits[a_z_indices] = pred_logits[a_z_indices]

                                pred_char_id = torch.argmax(masked_logits).item()
                                pred_char = self.tokenizer.decode([pred_char_id])

                                # ç»Ÿè®¡
                                total_samples += 1
                                if pred_char == true_completion:
                                    correct_predictions += 1

                        except Exception as e:
                            continue

                except Exception as e:
                    print(f"âš ï¸  Warning: Error in eval batch {batch_idx}: {e}")
                    continue

        # è®¡ç®—æœ€ç»ˆæŒ‡æ ‡
        char_accuracy = correct_predictions / total_samples if total_samples > 0 else 0.0
        avg_loss = total_loss / num_batches if num_batches > 0 else 0.0

        # è®°å½•æŒ‡æ ‡
        metrics = {
            f'{metric_key_prefix}_char_accuracy': char_accuracy,
            f'{metric_key_prefix}_total_samples': total_samples,
            f'{metric_key_prefix}_correct_predictions': correct_predictions,
            f'{metric_key_prefix}_avg_loss': avg_loss,
        }

        print(f"\nğŸ“Š {metric_key_prefix.title()} Custom Metrics:")
        print(f"   Character Accuracy: {char_accuracy:.4f} ({correct_predictions}/{total_samples})")
        print(f"   Average Loss: {avg_loss:.4f}")
        print(f"   ğŸ“¤ Returning metrics: {list(metrics.keys())}")

        return metrics


# å‘åå…¼å®¹çš„åŒ…è£…å™¨
class HangmanEvaluator(StepWiseHangmanEvaluator):
    """å‘åå…¼å®¹çš„è¯„ä¼°å™¨"""
    pass