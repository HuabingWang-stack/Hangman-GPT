# config.py - Clean Step-wise PPOé…ç½®å‚æ•°
from dataclasses import dataclass
import torch


@dataclass
class CleanPPOConfig:
    # ğŸ¯ Clean Step-wise PPOæ ¸å¿ƒå‚æ•°
    kl_coeff: float = 0.08  # KLæ­£åˆ™åŒ–ç³»æ•°
    clip_epsilon: float = 0.15  # PPO clippingå‚æ•°
    learning_rate: float = 8e-5  # å­¦ä¹ ç‡
    base_learning_rate: float = 8e-5  # åŸºç¡€å­¦ä¹ ç‡ï¼ˆå…¼å®¹æ€§ï¼‰

    # ğŸ¯ è¯æ±‡åˆ†ç±»é˜ˆå€¼
    protect_threshold: float = 0.75  # PROTECTç±»åˆ«é˜ˆå€¼
    focus_threshold: float = 0.45  # FOCUSç±»åˆ«é˜ˆå€¼
    short_word_length: int = 5  # çŸ­è¯é•¿åº¦é˜ˆå€¼

    # ğŸ¯ åˆ†å±‚è®­ç»ƒæƒé‡
    protect_category_weight: float = 0.7  # ä¿æŠ¤è¯æ±‡è®­ç»ƒæƒé‡
    focus_category_weight: float = 1.5  # é‡ç‚¹è¯æ±‡è®­ç»ƒæƒé‡
    improve_category_weight: float = 1.0  # å¹³è¡¡è¯æ±‡è®­ç»ƒæƒé‡

    # ğŸ¯ åˆ†å±‚KLæƒé‡å€æ•°
    protect_kl_multiplier: float = 1.5  # ä¿æŠ¤è¯æ±‡KLçº¦æŸå€æ•°
    focus_kl_multiplier: float = 0.7  # é‡ç‚¹è¯æ±‡KLçº¦æŸå€æ•°
    improve_kl_multiplier: float = 1.0  # å¹³è¡¡è¯æ±‡KLçº¦æŸå€æ•°

    # ğŸ¯ åˆ†å±‚å­¦ä¹ ç‡å€æ•°
    protect_lr_multiplier: float = 0.6  # ä¿æŠ¤è¯æ±‡å­¦ä¹ ç‡å€æ•°
    focus_lr_multiplier: float = 1.4  # é‡ç‚¹è¯æ±‡å­¦ä¹ ç‡å€æ•°
    improve_lr_multiplier: float = 1.0  # å¹³è¡¡è¯æ±‡å­¦ä¹ ç‡å€æ•°

    # ğŸ¯ å¥–åŠ±è®¡ç®—å‚æ•°ï¼ˆç®€åŒ–ç‰ˆï¼‰
    correct_prediction_reward: float = 2.0  # çŒœå¯¹å­—ç¬¦çš„åŸºç¡€å¥–åŠ±
    wrong_prediction_penalty: float = -0.5  # çŒœé”™çš„æƒ©ç½š
    repeat_guess_penalty: float = -1.0  # é‡å¤çŒœæµ‹çš„æƒ©ç½š
    efficiency_bonus_factor: float = 0.2  # æ•ˆç‡å¥–åŠ±å› å­

    # ğŸ¯ PPOåŸºç¡€å‚æ•°
    k_trajectories: int = 4  # æ¯ä¸ªè¯ç”Ÿæˆçš„è½¨è¿¹æ•°
    alpha: float = 0.2  # å¥–åŠ±ç¼©æ”¾å› å­ï¼ˆå‘åå…¼å®¹ï¼‰
    max_grad_norm: float = 0.5  # æ¢¯åº¦è£å‰ªèŒƒæ•°
    max_steps: int = 10  # æœ€å¤§æ­¥æ•°

    # ğŸ¯ è®­ç»ƒå‚æ•°
    batch_size: int = 6  # æ‰¹æ¬¡å¤§å°ï¼ˆtrajectory groupsï¼‰
    num_epochs: int = 25  # è®­ç»ƒè½®æ•°
    weight_decay: float = 0.01  # æƒé‡è¡°å‡

    # ğŸ¯ è·¯å¾„å‚æ•°
    output_dir: str = ""  # è¾“å‡ºç›®å½•
    sft_model_path: str = "sft/models/4xdataset-8layer-final/checkpoint-6000"
    pretrain_path: str = "dataset/225k_10k_5k_10k/pretrain.txt"
    sft_path: str = "dataset/225k_10k_5k_10k/sft.txt"
    grpo_path: str = "dataset/225k_10k_5k_10k/grpo.txt"

    # ğŸ¯ æ§åˆ¶å‚æ•°
    eval_interval: int = 3  # è¯„ä¼°é—´éš”
    save_interval: int = 5  # ä¿å­˜é—´éš”
    log_trajectory_every: int = 20  # è½¨è¿¹è®°å½•é—´éš”

    # ğŸ¯ å­¦ä¹ ç‡è°ƒåº¦
    use_lr_scheduler: bool = True  # ä½¿ç”¨å­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler_patience: int = 5  # è°ƒåº¦å™¨è€å¿ƒå€¼
    scheduler_factor: float = 0.75  # è°ƒåº¦å™¨è¡°å‡å› å­
    min_lr: float = 1e-6  # æœ€å°å­¦ä¹ ç‡

    # ğŸ¯ Early stopping
    use_early_stopping: bool = True  # ä½¿ç”¨æ—©åœ
    early_stop_patience: int = 8  # æ—©åœè€å¿ƒå€¼
    early_stop_metric: str = "step_accuracy"  # åŸºäºstep accuracy

    # ğŸ¯ è°ƒè¯•å’Œç›‘æ§
    debug_mode: bool = False  # è°ƒè¯•æ¨¡å¼
    wandb_project: str = "hangman-clean-stepwise-ppo"  # é¡¹ç›®å

    # ğŸ¯ å®‰å…¨ç›‘æ§å‚æ•°
    enable_safety_monitoring: bool = True  # å¯ç”¨å®‰å…¨ç›‘æ§
    max_loss_per_step: float = 3.0  # å•ä¸ªæ­¥éª¤æœ€å¤§loss
    max_loss_std_per_batch: float = 1.5  # æ‰¹æ¬¡lossæ ‡å‡†å·®é˜ˆå€¼
    gradient_norm_threshold: float = 100.0  # æ¢¯åº¦èŒƒæ•°å¼‚å¸¸é˜ˆå€¼

    # ğŸ¯ Step-wiseè¯„ä¼°å‚æ•°
    step_accuracy_target: float = 0.65  # ç›®æ ‡å•æ­¥å‡†ç¡®ç‡

    # ğŸ¯ å†…å­˜ä¼˜åŒ–å‚æ•°
    enable_gradient_checkpointing: bool = False  # æ¢¯åº¦æ£€æŸ¥ç‚¹
    max_memory_usage_gb: float = 8.0  # æœ€å¤§å†…å­˜ä½¿ç”¨

    # ğŸ¯ é‡‡æ ·å‚æ•°
    exploration_rate: float = 0.1  # æ¢ç´¢ç‡ï¼ˆvsè´ªå¿ƒï¼‰

    # ğŸ¯ è®¾å¤‡é…ç½®
    device: str = "auto"  # è®¾å¤‡é€‰æ‹©

    @classmethod
    def get_trainer_config(cls):
        """è·å–è®­ç»ƒå™¨é…ç½®"""
        return {
            'kl_coeff': cls.kl_coeff,
            'clip_epsilon': cls.clip_epsilon,
            'max_grad_norm': cls.max_grad_norm,
            'base_learning_rate': cls.base_learning_rate,
        }

    @classmethod
    def get_reward_calculator_config(cls):
        """è·å–å¥–åŠ±è®¡ç®—å™¨é…ç½®"""
        return {
            'correct_prediction_reward': cls.correct_prediction_reward,
            'wrong_prediction_penalty': cls.wrong_prediction_penalty,
            'repeat_guess_penalty': cls.repeat_guess_penalty,
            'efficiency_bonus_factor': cls.efficiency_bonus_factor,
        }

    @classmethod
    def get_difficulty_config(cls):
        """è·å–éš¾åº¦åˆ†ç±»é…ç½®"""
        return {
            'protect_threshold': cls.protect_threshold,
            'focus_threshold': cls.focus_threshold,
            'short_word_length': cls.short_word_length,
            'protect_lr_multiplier': cls.protect_lr_multiplier,
            'focus_lr_multiplier': cls.focus_lr_multiplier,
            'improve_lr_multiplier': cls.improve_lr_multiplier,
            'protect_category_weight': cls.protect_category_weight,
            'focus_category_weight': cls.focus_category_weight,
            'improve_category_weight': cls.improve_category_weight,
            'protect_kl_multiplier': cls.protect_kl_multiplier,
            'focus_kl_multiplier': cls.focus_kl_multiplier,
            'improve_kl_multiplier': cls.improve_kl_multiplier,
        }

    @classmethod
    def get_full_config(cls):
        """è·å–å®Œæ•´é…ç½®å­—å…¸"""
        config_dict = {}
        for field in cls.__dataclass_fields__:
            config_dict[field] = getattr(cls, field)
        return config_dict

    def __post_init__(self):
        """åˆå§‹åŒ–åå¤„ç†"""
        # ç¡®ä¿å…¼å®¹æ€§
        if hasattr(self, 'base_learning_rate') and self.base_learning_rate != self.learning_rate:
            self.learning_rate = self.base_learning_rate

        # è®¾å¤‡é…ç½®
        if self.device == "auto":
            self.device = "cuda" if torch.cuda.is_available() else "cpu"

    def to_dict(self):
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {field: getattr(self, field) for field in self.__dataclass_fields__}

    def update_from_args(self, args):
        """ä»å‘½ä»¤è¡Œå‚æ•°æ›´æ–°é…ç½®"""
        overridden_params = []

        # å‚æ•°æ˜ å°„
        param_mapping = {
            'kl_coeff': 'kl_coeff',
            'clip_epsilon': 'clip_epsilon',
            'learning_rate': 'learning_rate',
            'batch_size': 'batch_size',
            'num_epochs': 'num_epochs',
            'weight_decay': 'weight_decay',
            'max_grad_norm': 'max_grad_norm',
            'correct_prediction_reward': 'correct_prediction_reward',
            'wrong_prediction_penalty': 'wrong_prediction_penalty',
            'repeat_guess_penalty': 'repeat_guess_penalty',
            'efficiency_bonus_factor': 'efficiency_bonus_factor',
            'protect_threshold': 'protect_threshold',
            'focus_threshold': 'focus_threshold',
            'short_word_length': 'short_word_length',
            'k_trajectories': 'k_trajectories',
            'max_steps': 'max_steps',
            'eval_interval': 'eval_interval',
            'save_interval': 'save_interval',
            'log_trajectory_every': 'log_trajectory_every',
            'use_lr_scheduler': 'use_lr_scheduler',
            'scheduler_patience': 'scheduler_patience',
            'scheduler_factor': 'scheduler_factor',
            'min_lr': 'min_lr',
            'use_early_stopping': 'use_early_stopping',
            'early_stop_patience': 'early_stop_patience',
            'wandb_project': 'wandb_project',
            'debug_mode': 'debug_mode',
            'enable_safety_monitoring': 'enable_safety_monitoring',
            'output_dir': 'output_dir',
        }

        # åº”ç”¨å‚æ•°è¦†ç›–
        for arg_name, config_attr in param_mapping.items():
            if hasattr(args, arg_name) and getattr(args, arg_name) is not None:
                old_value = getattr(self, config_attr, None)
                new_value = getattr(args, arg_name)

                if old_value != new_value:
                    setattr(self, config_attr, new_value)
                    overridden_params.append(f"   {config_attr}: {old_value} â†’ {new_value}")

        # ç‰¹æ®Šå¤„ç†
        if hasattr(args, 'base_learning_rate') and args.base_learning_rate is not None:
            if args.base_learning_rate != self.base_learning_rate:
                old_value = self.base_learning_rate
                self.base_learning_rate = args.base_learning_rate
                self.learning_rate = args.base_learning_rate  # åŒæ­¥
                overridden_params.append(f"   base_learning_rate: {old_value} â†’ {args.base_learning_rate}")

        return overridden_params


# å‘åå…¼å®¹
GRPOConfig = CleanPPOConfig
Config = CleanPPOConfig