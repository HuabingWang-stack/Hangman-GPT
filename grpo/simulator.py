# simulator.py - Step-wise + Probability Distribution Hangmanæ¸¸æˆæ¨¡æ‹Ÿå™¨ï¼ˆå®Œæ•´ä¿®å¤ç‰ˆï¼‰
import torch
import torch.nn.functional as F
import numpy as np
import random
from typing import List, Dict, Any, Set, Optional
from collections import Counter, defaultdict


class StepwiseProbabilityHangmanSimulator:
    """åŸºäºStep-wiseå‡†ç¡®ç‡å’Œæ¦‚ç‡åˆ†å¸ƒçš„Hangmanæ¸¸æˆæ¨¡æ‹Ÿå™¨"""

    def __init__(self, model, tokenizer, device='cuda',
                 temperature: float = 1.0,
                 max_wrong_guesses: int = 6,
                 max_steps: int = 10,
                 record_probabilities: bool = True):
        self.model = model
        self.tokenizer = tokenizer
        self.device = device
        self.temperature = temperature
        self.max_wrong_guesses = max_wrong_guesses
        self.max_steps = max_steps
        self.record_probabilities = record_probabilities

        # ğŸ¯ é¢„è®¡ç®—a-zå­—ç¬¦æ˜ å°„ - åŸºäºå®é™…tokenizerç»“æ„
        self.a_z_chars = 'abcdefghijklmnopqrstuvwxyz'
        self.char_to_id = {}
        self.id_to_char = {}
        self._initialize_char_mappings()

        print(f"ğŸ¯ StepwiseProbabilityHangmanSimulator initialized:")
        print(f"   Temperature: {temperature}")
        print(f"   Max wrong guesses: {max_wrong_guesses}")
        print(f"   Max steps: {max_steps}")
        print(f"   Record probabilities: {record_probabilities}")
        print(f"   A-Z chars mapped: {len(self.char_to_id)}/26")

    def _initialize_char_mappings(self):
        """ğŸ”§ åˆå§‹åŒ–å­—ç¬¦æ˜ å°„ - åŸºäºå®é™…tokenizerç»“æ„"""
        try:
            # ğŸ”§ ä¿®å¤ï¼šç›´æ¥è®¿é—®tokenizerçš„char_to_idå­—å…¸
            if hasattr(self.tokenizer, 'char_to_id') and isinstance(self.tokenizer.char_to_id, dict):
                for c in self.a_z_chars:
                    if c in self.tokenizer.char_to_id:
                        token_id = self.tokenizer.char_to_id[c]
                        self.char_to_id[c] = token_id
                        self.id_to_char[token_id] = c
                print("âœ… Using tokenizer.char_to_id (dict)")

                # æ˜¾ç¤ºæ˜ å°„ç¤ºä¾‹
                sample_mappings = {c: self.char_to_id[c] for c in ['a', 'z'] if c in self.char_to_id}
                print(f"   Sample mappings: {sample_mappings}")
                return
            else:
                print("âš ï¸ Tokenizer char_to_id not found or not a dict, using fallback mapping")
                self._create_fallback_mappings()

        except Exception as e:
            print(f"âŒ Error initializing character mappings: {e}")
            self._create_fallback_mappings()

    def _create_fallback_mappings(self):
        """åˆ›å»ºfallbackå­—ç¬¦æ˜ å°„"""
        print("ğŸ”§ Creating fallback character mappings...")
        # æ ¹æ®å®é™…tokenizerç»“æ„ï¼Œa-zåœ¨ä½ç½®4-29ï¼ˆè·³è¿‡æ§åˆ¶tokensï¼‰
        for i, c in enumerate(self.a_z_chars):
            token_id = 4 + i  # è·³è¿‡ <pad>, <bos>, <eos>, <unk>
            self.char_to_id[c] = token_id
            self.id_to_char[token_id] = c

        sample_mappings = {c: self.char_to_id[c] for c in ['a', 'z']}
        print(f"âœ… Fallback mappings created: {sample_mappings}")

    def simulate_game(self, word: str, strategy: str = "greedy") -> Dict[str, Any]:
        """
        æ¨¡æ‹Ÿå•ä¸ªHangmanæ¸¸æˆ

        Args:
            word: ç›®æ ‡è¯æ±‡
            strategy: é¢„æµ‹ç­–ç•¥ ("greedy", "sampling", "mixed")

        Returns:
            æ¸¸æˆç»“æœå­—å…¸ï¼ŒåŒ…å«step-wiseç»Ÿè®¡
        """
        word = word.lower()
        guessed_letters = set()
        wrong_guesses = 0

        game_result = {
            "word": word,
            "strategy": strategy,
            "success": False,
            "failure_reason": "",
            "steps": [],
            "total_steps": 0,
            "correct_steps": 0,
            "wrong_guesses": 0,
            "step_accuracies": [],
            "step_prob_alignments": [],
            "step_kl_divergences": [],
            "step_confidences": [],
            "violations": [],
            "final_state": "",
            "game_stats": {}
        }

        # åˆå§‹åŒ–æ¸¸æˆçŠ¶æ€
        current_state = ['_' if c.isalpha() else c for c in word]

        self.model.eval()

        for step_idx in range(self.max_steps):
            # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„Hangman promptæ ¼å¼
            prompt = self._build_hangman_prompt(current_state, guessed_letters, wrong_guesses)

            # ğŸ¯ é¢„æµ‹ä¸‹ä¸€ä¸ªå­—ç¬¦
            prediction_result = self._predict_next_char(
                prompt, word, guessed_letters, strategy
            )

            if not prediction_result:
                game_result["failure_reason"] = "prediction_failed"
                break

            predicted_char = prediction_result["predicted_char"]
            model_distribution = prediction_result.get("model_distribution", {})
            ideal_distribution = prediction_result.get("ideal_distribution", {})
            kl_divergence = prediction_result.get("kl_divergence", 0.0)
            confidence = prediction_result.get("confidence", 0.0)
            prob_alignment = prediction_result.get("prob_alignment", 0.0)

            # æ£€æŸ¥è¿è§„
            is_violation = predicted_char in guessed_letters
            if is_violation:
                game_result["violations"].append({
                    "step": step_idx,
                    "type": "repeat_guess",
                    "char": predicted_char
                })

            # åˆ¤æ–­æ˜¯å¦æ­£ç¡®
            is_correct = predicted_char in word and not is_violation

            # è®°å½•æ­¥éª¤è¯¦æƒ…
            step_detail = {
                "step_id": step_idx,
                "prompt": prompt,
                "predicted_char": predicted_char,
                "is_correct": is_correct,
                "is_violation": is_violation,
                "current_state": ''.join(current_state),
                "guessed_letters": list(guessed_letters),
                "wrong_guesses": wrong_guesses,
                "confidence": confidence,
                "kl_divergence": kl_divergence,
                "prob_alignment": prob_alignment,
            }

            # ğŸ¯ æ·»åŠ æ¦‚ç‡åˆ†å¸ƒä¿¡æ¯
            if self.record_probabilities:
                step_detail.update({
                    "model_distribution": model_distribution,
                    "ideal_distribution": ideal_distribution,
                    "distribution_entropy": self._calculate_entropy(model_distribution),
                    "ideal_entropy": self._calculate_entropy(ideal_distribution)
                })

            game_result["steps"].append(step_detail)

            # æ›´æ–°step-wiseç»Ÿè®¡
            game_result["step_accuracies"].append(is_correct)
            game_result["step_prob_alignments"].append(prob_alignment)
            game_result["step_kl_divergences"].append(kl_divergence)
            game_result["step_confidences"].append(confidence)

            # æ›´æ–°æ¸¸æˆçŠ¶æ€
            guessed_letters.add(predicted_char)

            if predicted_char in word:
                # çŒœå¯¹äº†
                for i, c in enumerate(word):
                    if c == predicted_char:
                        current_state[i] = c

                # æ£€æŸ¥æ¸¸æˆæ˜¯å¦å®Œæˆ
                if '_' not in current_state:
                    game_result["success"] = True
                    break
            else:
                # çŒœé”™äº†
                wrong_guesses += 1
                if wrong_guesses >= self.max_wrong_guesses:
                    game_result["failure_reason"] = "too_many_wrong"
                    break

            # è¿è§„ä¹Ÿç®—å¤±è´¥
            if is_violation:
                game_result["failure_reason"] = "violation"
                break

        # å¦‚æœæ²¡æœ‰æ˜ç¡®å¤±è´¥åŸå› ä¸”æœªæˆåŠŸ
        if not game_result["success"] and not game_result["failure_reason"]:
            game_result["failure_reason"] = "max_steps_reached"

        # è®¡ç®—æœ€ç»ˆç»Ÿè®¡
        game_result["total_steps"] = len(game_result["steps"])
        game_result["correct_steps"] = sum(game_result["step_accuracies"])
        game_result["wrong_guesses"] = wrong_guesses
        game_result["final_state"] = ''.join(current_state)

        # ğŸ¯ è®¡ç®—æ¸¸æˆçº§åˆ«çš„step-wiseæŒ‡æ ‡
        game_result["game_stats"] = {
            "step_accuracy": game_result["correct_steps"] / max(game_result["total_steps"], 1),
            "avg_prob_alignment": np.mean(game_result["step_prob_alignments"]) if game_result[
                "step_prob_alignments"] else 0.0,
            "avg_kl_divergence": np.mean(game_result["step_kl_divergences"]) if game_result[
                "step_kl_divergences"] else 0.0,
            "avg_confidence": np.mean(game_result["step_confidences"]) if game_result["step_confidences"] else 0.0,
            "violation_count": len(game_result["violations"]),
            "efficiency_score": self._calculate_efficiency_score(game_result)
        }

        return game_result

    def _build_hangman_prompt(self, current_state: List[str], guessed_letters: Set[str],
                              wrong_guesses: int) -> str:
        """ğŸ”§ æ„å»ºæ ‡å‡†Hangman promptæ ¼å¼ï¼šword_state[SEP]guessed_letters[SEP]wrong/max"""
        # ç¬¬ä¸€éƒ¨åˆ†ï¼šå½“å‰è¯çŠ¶æ€
        word_state = ''.join(current_state)

        # ç¬¬äºŒéƒ¨åˆ†ï¼šå·²çŒœå­—æ¯ï¼ˆæŒ‰å­—æ¯é¡ºåºï¼‰
        if guessed_letters:
            guessed_str = ','.join(sorted(guessed_letters))
        else:
            guessed_str = ''

        # ç¬¬ä¸‰éƒ¨åˆ†ï¼šé”™è¯¯æ¬¡æ•°
        wrong_str = f"{wrong_guesses}/6"

        # ğŸ¯ æ ‡å‡†Hangmanæ ¼å¼
        prompt = f"{word_state}[SEP]{guessed_str}[SEP]{wrong_str}"

        return prompt

    def _predict_next_char(self, prompt: str, word: str, guessed_letters: Set[str],
                           strategy: str) -> Optional[Dict[str, Any]]:
        """ğŸ¯ é¢„æµ‹ä¸‹ä¸€ä¸ªå­—ç¬¦ï¼ˆæ”¯æŒå¤šç§ç­–ç•¥ï¼‰"""
        try:
            # ç¼–ç è¾“å…¥
            encoded = self._encode_prompt(prompt)
            if not encoded:
                return None

            input_ids = torch.tensor([encoded], dtype=torch.long).to(self.device)

            with torch.no_grad():
                # ğŸ”§ ä¿®å¤ï¼šå¤„ç†ModelOutputå¯¹è±¡
                outputs = self.model(x=input_ids)

                # æ£€æŸ¥è¾“å‡ºç±»å‹å¹¶æå–logits
                if hasattr(outputs, 'logits'):
                    logits = outputs.logits
                elif hasattr(outputs, 'last_hidden_state'):
                    logits = outputs.last_hidden_state
                else:
                    logits = outputs

                # ç¡®ä¿logitså½¢çŠ¶æ­£ç¡®
                if logits.dim() != 3:
                    print(f"âŒ Unexpected logits shape: {logits.shape}")
                    return None

                last_logits = logits[0, -1, :]  # æœ€åä½ç½®çš„logits

                # åº”ç”¨æ¸©åº¦
                scaled_logits = last_logits / self.temperature

                # æå–a-zå­—ç¬¦çš„logits
                a_z_indices = []
                valid_chars = []

                for c in self.a_z_chars:
                    if c in self.char_to_id:
                        a_z_indices.append(self.char_to_id[c])
                        valid_chars.append(c)

                if not a_z_indices:
                    return None

                a_z_logits = scaled_logits[a_z_indices]
                a_z_probs = F.softmax(a_z_logits, dim=-1)

                # æ„å»ºæ¨¡å‹åˆ†å¸ƒ
                model_distribution = {}
                for i, char in enumerate(valid_chars):
                    model_distribution[char] = float(a_z_probs[i])

                # ä¸ºæ²¡æœ‰æ˜ å°„çš„å­—ç¬¦å¡«å……æå°å€¼
                for c in self.a_z_chars:
                    if c not in model_distribution:
                        model_distribution[c] = 1e-8

                # è®¡ç®—ç†æƒ³åˆ†å¸ƒ
                ideal_distribution = self._calculate_ideal_distribution(word, guessed_letters)

                # è®¡ç®—æ¦‚ç‡å¯¹é½åˆ†æ•°
                prob_alignment = self._calculate_probability_alignment(model_distribution, ideal_distribution)

                # è®¡ç®—KLæ•£åº¦
                kl_divergence = self._calculate_kl_divergence(model_distribution, ideal_distribution)

                # ğŸ¯ æ ¹æ®ç­–ç•¥é€‰æ‹©å­—ç¬¦
                if strategy == "greedy":
                    pred_idx = torch.argmax(a_z_probs).item()
                elif strategy == "sampling":
                    pred_idx = torch.multinomial(a_z_probs, 1).item()
                elif strategy == "mixed":
                    if random.random() < 0.7:  # 70%è´ªå¿ƒï¼Œ30%é‡‡æ ·
                        pred_idx = torch.argmax(a_z_probs).item()
                    else:
                        pred_idx = torch.multinomial(a_z_probs, 1).item()
                else:
                    pred_idx = torch.argmax(a_z_probs).item()

                predicted_char = valid_chars[pred_idx]
                confidence = float(a_z_probs[pred_idx])

                return {
                    "predicted_char": predicted_char,
                    "model_distribution": model_distribution,
                    "ideal_distribution": ideal_distribution,
                    "kl_divergence": kl_divergence,
                    "confidence": confidence,
                    "prob_alignment": prob_alignment,
                    "strategy_used": strategy
                }

        except Exception as e:
            print(f"âŒ Error in prediction: {e}")
            return None

    def _calculate_ideal_distribution(self, word: str, guessed_letters: Set[str]) -> Dict[str, float]:
        """è®¡ç®—ç†æƒ³æ¦‚ç‡åˆ†å¸ƒ"""
        unguessed_chars = set(word) - guessed_letters

        if not unguessed_chars:
            return {c: 1.0 / 26 for c in self.a_z_chars}

        # è®¡ç®—é¢‘ç‡
        char_counts = Counter(c for c in word if c in unguessed_chars)
        total_unguessed = sum(char_counts.values())

        distribution = {}
        for c in self.a_z_chars:
            if c in char_counts:
                distribution[c] = char_counts[c] / total_unguessed * 0.8
            elif c in guessed_letters:
                distribution[c] = 0.01 / 26
            else:
                distribution[c] = 0.2 / 26

        # å½’ä¸€åŒ–
        total_prob = sum(distribution.values())
        if total_prob > 0:
            distribution = {c: p / total_prob for c, p in distribution.items()}

        return distribution

    def _calculate_probability_alignment(self, model_dist: Dict[str, float],
                                         ideal_dist: Dict[str, float]) -> float:
        """è®¡ç®—æ¦‚ç‡åˆ†å¸ƒå¯¹é½åˆ†æ•°"""
        try:
            # ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦ä½œä¸ºå¯¹é½åˆ†æ•°
            model_probs = np.array([model_dist.get(c, 1e-8) for c in self.a_z_chars])
            ideal_probs = np.array([ideal_dist.get(c, 1e-8) for c in self.a_z_chars])

            # å½’ä¸€åŒ–
            model_probs = model_probs / np.sum(model_probs)
            ideal_probs = ideal_probs / np.sum(ideal_probs)

            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            dot_product = np.dot(model_probs, ideal_probs)
            norm_model = np.linalg.norm(model_probs)
            norm_ideal = np.linalg.norm(ideal_probs)

            if norm_model > 0 and norm_ideal > 0:
                cosine_sim = dot_product / (norm_model * norm_ideal)
                return float(np.clip(cosine_sim, 0.0, 1.0))
            else:
                return 0.0

        except Exception as e:
            return 0.0

    def _calculate_kl_divergence(self, model_dist: Dict[str, float],
                                 ideal_dist: Dict[str, float]) -> float:
        """è®¡ç®—KLæ•£åº¦"""
        try:
            kl_div = 0.0
            for char in self.a_z_chars:
                p = ideal_dist.get(char, 1e-8)
                q = model_dist.get(char, 1e-8)

                p = max(p, 1e-8)
                q = max(q, 1e-8)

                kl_div += p * np.log(p / q)

            return float(kl_div)
        except:
            return 0.0

    def _calculate_entropy(self, distribution: Dict[str, float]) -> float:
        """è®¡ç®—åˆ†å¸ƒçš„ç†µ"""
        try:
            entropy = 0.0
            for char in self.a_z_chars:
                p = distribution.get(char, 1e-8)
                p = max(p, 1e-8)
                entropy -= p * np.log2(p)
            return float(entropy)
        except:
            return 0.0

    def _calculate_efficiency_score(self, game_result: Dict[str, Any]) -> float:
        """è®¡ç®—æ¸¸æˆæ•ˆç‡åˆ†æ•°"""
        if not game_result["success"]:
            return 0.0

        total_steps = game_result["total_steps"]
        word_length = len(set(game_result["word"]))  # å”¯ä¸€å­—ç¬¦æ•°

        if total_steps == 0:
            return 0.0

        # æ•ˆç‡åˆ†æ•°ï¼šè¶Šå°‘æ­¥éª¤å®Œæˆè¶Šå¥½
        max_possible_steps = min(word_length, self.max_steps)
        efficiency = max_possible_steps / total_steps

        return min(efficiency, 1.0)

    def _encode_prompt(self, prompt: str) -> List[int]:
        """ç¼–ç æç¤º"""
        try:
            if hasattr(self.tokenizer, 'encode'):
                encoded = self.tokenizer.encode(prompt, add_special_tokens=False)
                return encoded
            else:
                print(f"âŒ Tokenizer does not have encode method")
                return []
        except Exception as e:
            print(f"âŒ Error encoding prompt: {e}")
            return []

    # å…¶ä»–æ–¹æ³•ä¿æŒç›¸åŒï¼Œåªæ˜¯ç®€åŒ–äº†ä¸ç›¸å…³çš„éƒ¨åˆ†...

    def simulate_batch(self, words: List[str],
                       games_per_word: int = 1,
                       strategy: str = "greedy") -> Dict[str, Any]:
        """æ‰¹é‡æ¨¡æ‹Ÿå¤šä¸ªè¯æ±‡çš„æ¸¸æˆ"""
        all_results = []

        print(f"ğŸ¯ Simulating {len(words)} words Ã— {games_per_word} games with strategy '{strategy}'...")

        for word_idx, word in enumerate(words):
            for game_idx in range(games_per_word):
                try:
                    result = self.simulate_game(word, strategy)
                    all_results.append(result)
                except Exception as e:
                    print(f"âŒ Error simulating '{word}' game {game_idx}: {e}")
                    continue

        # è®¡ç®—æ‰¹é‡ç»Ÿè®¡
        batch_stats = self._calculate_batch_statistics(all_results)

        return {
            "individual_results": all_results,
            "batch_statistics": batch_stats,
            "simulation_config": {
                "strategy": strategy,
                "games_per_word": games_per_word,
                "total_words": len(words),
                "total_games": len(all_results),
                "temperature": self.temperature,
                "max_wrong_guesses": self.max_wrong_guesses,
                "max_steps": self.max_steps
            }
        }

    def _calculate_batch_statistics(self, all_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ğŸ¯ è®¡ç®—æ‰¹é‡ç»Ÿè®¡ï¼Œé‡ç‚¹å…³æ³¨step-wiseæŒ‡æ ‡"""
        if not all_results:
            return {}

        # åŸºç¡€ç»Ÿè®¡
        total_games = len(all_results)
        successful_games = sum(1 for r in all_results if r["success"])

        # Step-wiseç»Ÿè®¡
        all_step_accuracies = []
        all_prob_alignments = []
        all_kl_divergences = []
        all_confidences = []

        for result in all_results:
            game_stats = result["game_stats"]
            all_step_accuracies.append(game_stats["step_accuracy"])
            all_prob_alignments.append(game_stats["avg_prob_alignment"])
            all_kl_divergences.append(game_stats["avg_kl_divergence"])
            all_confidences.append(game_stats["avg_confidence"])

        # è®¡ç®—ç»¼åˆç»Ÿè®¡
        stats = {
            # ğŸ¯ Step-wiseæ ¸å¿ƒæŒ‡æ ‡
            "step_accuracy": np.mean(all_step_accuracies),
            "step_accuracy_std": np.std(all_step_accuracies),
            "avg_prob_alignment": np.mean(all_prob_alignments),
            "avg_kl_divergence": np.mean(all_kl_divergences),
            "avg_confidence": np.mean(all_confidences),

            # æ¸¸æˆçº§åˆ«æŒ‡æ ‡
            "game_success_rate": successful_games / total_games,
            "total_games": total_games,
            "successful_games": successful_games,
        }

        return stats


# å‘åå…¼å®¹çš„åŒ…è£…å™¨
class HangmanSimulator(StepwiseProbabilityHangmanSimulator):
    """å‘åå…¼å®¹çš„æ¨¡æ‹Ÿå™¨"""

    def __init__(self, model, tokenizer, device='cuda', **kwargs):
        # æå–ä¼ ç»Ÿå‚æ•°
        temperature = kwargs.get('temperature', 1.0)
        max_wrong_guesses = kwargs.get('max_wrong_guesses', 6)
        max_steps = kwargs.get('max_steps', 10)

        super().__init__(
            model=model,
            tokenizer=tokenizer,
            device=device,
            temperature=temperature,
            max_wrong_guesses=max_wrong_guesses,
            max_steps=max_steps,
            record_probabilities=True
        )

    def simulate(self, word: str) -> Dict[str, Any]:
        """å‘åå…¼å®¹çš„æ¨¡æ‹Ÿæ–¹æ³•"""
        result = self.simulate_game(word, strategy="greedy")

        # è½¬æ¢ä¸ºæ—§æ ¼å¼
        return {
            "word": result["word"],
            "success": result["success"],
            "steps": result["total_steps"],
            "wrong_guesses": result["wrong_guesses"],
            "guesses": [step["predicted_char"] for step in result["steps"]],
            "final_state": result["final_state"],
            # æ–°å¢step-wiseæŒ‡æ ‡
            "step_accuracy": result["game_stats"]["step_accuracy"],
            "prob_alignment": result["game_stats"]["avg_prob_alignment"]
        }