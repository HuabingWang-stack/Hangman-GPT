"""
sft/sft_train.py - Pure Soft Targetsç‰ˆæœ¬
ä½¿ç”¨TRLè¿›è¡ŒSFTè®­ç»ƒ - Wordleå•å­—ç¬¦é¢„æµ‹ä»»åŠ¡
ä»checkpoints/pretrain.pthåŠ è½½é¢„è®­ç»ƒæ¨¡å‹ï¼Œè¿›è¡Œç›‘ç£å¾®è°ƒ
ğŸ¯ åªä½¿ç”¨soft_targetsåˆ†å¸ƒè®¡ç®—lossï¼Œå®Œå…¨æ‘’å¼ƒhard targets
æ”¯æŒKLæ•£åº¦å’Œäº¤å‰ç†µä¸¤ç§lossç±»å‹
è¾“å‡ºé™åˆ¶ä¸ºå•ä¸ªa-zå­—ç¬¦
æ¯ä¸ªepochç»“æŸæ—¶sample validationæ•°æ®å¹¶è®°å½•åˆ°wandb
ğŸ›‘ æ”¯æŒearly stoppingé˜²æ­¢è¿‡æ‹Ÿåˆ
"""

import os
import sys
import json
import time
import argparse
import math
import random
from typing import Dict, List, Optional, Any
from pathlib import Path
from dataclasses import dataclass, field
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset
import numpy as np

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

# TRL imports
from trl import SFTTrainer, SFTConfig
from transformers import (
    TrainingArguments,
    PreTrainedModel,
    PreTrainedTokenizerBase,
    Trainer,
    DataCollatorForLanguageModeling,
    TrainerCallback,
    TrainerControl,
    TrainerState
)
from datasets import Dataset as HFDataset
import wandb

# æˆ‘ä»¬çš„æ¨¡å—
from scripts.tokenizer import EnhancedCharacterTokenizer
from trainer.pretrain import GPT, ModelConfig, load_pretrained_model


@dataclass
class SFTTrainingConfig:
    """SFTè®­ç»ƒé…ç½® - Pure Soft Targets"""

    # åŸºç¡€è·¯å¾„
    pretrain_model_path: str = "checkpoints/pretrain.pth"
    tokenizer_path: str = "scripts/enhanced_tokenizer/tokenizer.json"
    train_data_path: str = "sft/data/train.jsonl"
    test_data_path: str = "sft/data/test.jsonl"
    output_dir: str = "sft/models"

    # è®­ç»ƒå‚æ•° - RTX 4090ä¼˜åŒ–ç‰ˆæœ¬
    learning_rate: float = 2e-5
    batch_size: int = 1024  # RTX 4090å¯ä»¥æ”¯æŒæ›´å¤§batch
    eval_batch_size: int = 64  # evaluationæ—¶å¯ä»¥æ›´å¤§
    num_epochs: int = 3
    warmup_steps: int = 100
    max_seq_length: int = 64
    gradient_accumulation_steps: int = 2  # å®é™…batch_size = 32*2 = 64
    weight_decay: float = 0.01
    logging_steps: int = 50
    eval_steps: int = 500
    save_steps: int = 1000

    # SFTç‰¹å®šå‚æ•°
    max_prompt_length: int = 48

    # ğŸ¯ Pure Soft targets lossé…ç½®
    loss_type: str = "kl_divergence"  # "kl_divergence" or "cross_entropy"
    temperature_scaling: float = 1.0  # æ¸©åº¦ç¼©æ”¾ï¼Œç”¨äºè°ƒæ•´è¾“å‡ºåˆ†å¸ƒçš„é”åº¦
    label_smoothing: float = 0.0  # æ ‡ç­¾å¹³æ»‘ï¼ˆä»…ç”¨äºcross_entropyï¼‰

    # ğŸ›‘ Early Stoppingé…ç½®
    early_stopping_enabled: bool = True  # æ˜¯å¦å¯ç”¨early stopping
    early_stopping_patience: int = 5  # è¿ç»­å¤šå°‘æ¬¡è¯„ä¼°æ²¡æœ‰æ”¹å–„å°±åœæ­¢
    early_stopping_threshold: float = 0.0001  # æ”¹å–„çš„æœ€å°é˜ˆå€¼
    early_stopping_metric: str = "eval_char_accuracy"  # ç›‘æ§çš„æŒ‡æ ‡
    early_stopping_greater_is_better: bool = True  # æŒ‡æ ‡è¶Šå¤§è¶Šå¥½è¿˜æ˜¯è¶Šå°è¶Šå¥½

    # éªŒè¯å’Œæµ‹è¯•
    eval_strategy: str = "steps"
    load_best_model_at_end: bool = True
    metric_for_best_model: str = "eval_char_accuracy"
    greater_is_better: bool = True

    # wandbé…ç½®
    use_wandb: bool = True
    project_name: str = "wordle-sft-pure-soft-targets"
    experiment_name: Optional[str] = None
    tags: List[str] = field(default_factory=lambda: ["sft", "wordle", "pure-soft-targets", "enhanced-tokenizer"])

    # é‡‡æ ·é…ç½®
    samples_per_epoch: int = 5  # æ¯ä¸ªepoché‡‡æ ·5ä¸ªéªŒè¯æ ·æœ¬

    # å…¶ä»–
    seed: int = 42
    dataloader_num_workers: int = 8  # RTX 4090å¯ä»¥æ”¯æŒæ›´å¤šworker
    fp16: bool = True  # ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
    gradient_checkpointing: bool = False  # ğŸ”§ å…³é—­gradient_checkpointingé¿å…å…¼å®¹æ€§é—®é¢˜
    remove_unused_columns: bool = False


class EarlyStoppingCallback(TrainerCallback):
    """ğŸ›‘ Early Stoppingå›è°ƒ - é˜²æ­¢è¿‡æ‹Ÿåˆ"""

    def __init__(self, config: SFTTrainingConfig):
        self.config = config
        self.patience = config.early_stopping_patience
        self.threshold = config.early_stopping_threshold
        self.metric = config.early_stopping_metric
        self.greater_is_better = config.early_stopping_greater_is_better

        # çŠ¶æ€è¿½è¸ª
        self.best_metric = None
        self.wait_count = 0
        self.stopped_epoch = 0
        self.should_stop = False

        # å†å²è®°å½•
        self.metric_history = []

        print(f"ğŸ›‘ Early Stopping initialized:")
        print(f"   Metric: {self.metric}")
        print(f"   Patience: {self.patience}")
        print(f"   Threshold: {self.threshold}")
        print(f"   Greater is better: {self.greater_is_better}")

    def on_evaluate(self, args, state: TrainerState, control: TrainerControl, logs=None, **kwargs):
        """åœ¨æ¯æ¬¡è¯„ä¼°åæ£€æŸ¥æ˜¯å¦éœ€è¦æ—©åœ"""
        if not self.config.early_stopping_enabled:
            return control

        # ğŸ”§ ä¿®å¤ï¼šæ£€æŸ¥logsæ˜¯å¦ä¸ºNone
        if logs is None:
            print(f"âš ï¸  Warning: Early stopping received None logs, skipping this evaluation")
            print(f"   Step: {state.global_step}, Epoch: {state.epoch:.2f}")
            return control

        print(f"ğŸ” Early stopping debug info:")
        print(f"   Step: {state.global_step}, Epoch: {state.epoch:.2f}")
        print(f"   Available metrics: {list(logs.keys())}")
        print(f"   Looking for metric: '{self.metric}'")

        current_metric = logs.get(self.metric)
        if current_metric is None:
            print(f"âš ï¸  Warning: Early stopping metric '{self.metric}' not found in logs")
            print(f"   Available metrics: {list(logs.keys())}")
            # ğŸ”§ å°è¯•æŸ¥æ‰¾ç›¸ä¼¼çš„metricåç§°
            similar_metrics = [k for k in logs.keys() if 'char_accuracy' in k or 'accuracy' in k]
            if similar_metrics:
                print(f"   Similar metrics found: {similar_metrics}")
                print(f"   Consider using one of these as early_stopping_metric")
            return control

        print(f"âœ… Found metric '{self.metric}' = {current_metric:.6f}")

        # è®°å½•æŒ‡æ ‡å†å²
        self.metric_history.append({
            'step': state.global_step,
            'epoch': state.epoch,
            'metric': current_metric
        })

        # åˆå§‹åŒ–æœ€ä½³æŒ‡æ ‡
        if self.best_metric is None:
            self.best_metric = current_metric
            self.wait_count = 0
            print(f"ğŸ¯ Early stopping baseline set: {self.metric}={current_metric:.6f}")
            self._log_to_wandb(state, current_metric, improved=True)
            return control

        # æ£€æŸ¥æ˜¯å¦æœ‰æ”¹å–„
        improved = self._is_improvement(current_metric, self.best_metric)

        if improved:
            self.best_metric = current_metric
            self.wait_count = 0
            print(f"âœ… Early stopping metric improved: {self.metric}={current_metric:.6f} (best so far)")
            self._log_to_wandb(state, current_metric, improved=True)
        else:
            self.wait_count += 1
            print(
                f"â³ Early stopping patience: {self.wait_count}/{self.patience} (no improvement: {self.metric}={current_metric:.6f} vs best {self.best_metric:.6f})")
            self._log_to_wandb(state, current_metric, improved=False)

            if self.wait_count >= self.patience:
                self.should_stop = True
                self.stopped_epoch = state.epoch
                control.should_training_stop = True
                print(f"ğŸ›‘ Early stopping triggered!")
                print(f"   Stopped at epoch: {self.stopped_epoch:.2f}")
                print(f"   Best {self.metric}: {self.best_metric:.6f}")
                print(f"   Total patience exceeded: {self.wait_count}/{self.patience}")

                # è®°å½•æ—©åœä¿¡æ¯åˆ°wandb
                if self.config.use_wandb:
                    try:
                        wandb.log({
                            'early_stopping_triggered': True,
                            'early_stopping_epoch': self.stopped_epoch,
                            'early_stopping_step': state.global_step,
                            'early_stopping_best_metric': self.best_metric,
                            'early_stopping_patience_used': self.wait_count
                        })
                    except:
                        pass

        return control

    def _is_improvement(self, current_metric, best_metric):
        """åˆ¤æ–­å½“å‰æŒ‡æ ‡æ˜¯å¦æ¯”æœ€ä½³æŒ‡æ ‡æœ‰æ”¹å–„"""
        if self.greater_is_better:
            return current_metric > best_metric + self.threshold
        else:
            return current_metric < best_metric - self.threshold

    def _log_to_wandb(self, state, current_metric, improved):
        """è®°å½•early stoppingçŠ¶æ€åˆ°wandb"""
        if not self.config.use_wandb:
            return

        try:
            wandb.log({
                'early_stopping_metric': current_metric,
                'early_stopping_best_metric': self.best_metric,
                'early_stopping_wait_count': self.wait_count,
                'early_stopping_improved': improved,
                'step': state.global_step
            })
        except:
            pass

    def on_train_end(self, args, state: TrainerState, control: TrainerControl, **kwargs):
        """è®­ç»ƒç»“æŸæ—¶çš„æ€»ç»“"""
        if not self.config.early_stopping_enabled:
            return

        print(f"\nğŸ›‘ Early Stopping Summary:")

        # ğŸ”§ ä¿®å¤ï¼šæ£€æŸ¥best_metricæ˜¯å¦ä¸ºNone
        if self.best_metric is not None:
            print(f"   Final best {self.metric}: {self.best_metric:.6f}")

            if self.should_stop:
                print(f"   Training stopped early at epoch {self.stopped_epoch:.2f}")
                print(f"   Patience used: {self.wait_count}/{self.patience}")
            else:
                print(f"   Training completed normally")
                print(f"   Final patience count: {self.wait_count}/{self.patience}")

            # æ˜¾ç¤ºæŒ‡æ ‡å†å²è¶‹åŠ¿
            if len(self.metric_history) > 1:
                recent_metrics = [h['metric'] for h in self.metric_history[-5:]]  # æœ€è¿‘5æ¬¡
                print(f"   Recent {self.metric} trend: {recent_metrics}")
        else:
            print(f"   âš ï¸  No valid metrics received for {self.metric}")
            print(f"   Early stopping was not able to monitor training progress")
            print(f"   Possible reasons:")
            print(f"     - Evaluation was not triggered during training")
            print(f"     - The metric '{self.metric}' was not found in evaluation results")
            print(f"     - Evaluation failed or returned None logs")

        # æœ€ç»ˆwandbè®°å½•
        if self.config.use_wandb:
            try:
                summary_data = {
                    'early_stopping_final_wait_count': self.wait_count,
                    'early_stopping_stopped_early': self.should_stop,
                    'early_stopping_metric_received': self.best_metric is not None,
                }

                if self.best_metric is not None:
                    summary_data.update({
                        'early_stopping_best_metric': self.best_metric,
                        'early_stopping_stopped_epoch': self.stopped_epoch if self.should_stop else None
                    })

                wandb.run.summary.update(summary_data)
            except:
                pass


class WordleSFTDataset(Dataset):
    """Wordle SFTæ•°æ®é›† - Pure Soft Targetsä¸“ç”¨"""

    def __init__(self,
                 data_path: str,
                 tokenizer: EnhancedCharacterTokenizer,
                 config: SFTTrainingConfig,
                 is_training: bool = True):

        self.tokenizer = tokenizer
        self.config = config
        self.is_training = is_training

        # ğŸ¯ é¢„è®¡ç®—a-zå­—ç¬¦çš„token idsæ˜ å°„
        self.a_z_char_to_id = {}
        self.a_z_id_to_char = {}
        for c in 'abcdefghijklmnopqrstuvwxyz':
            if c in tokenizer.char_to_id:
                token_id = tokenizer.char_to_id[c]
                self.a_z_char_to_id[c] = token_id
                self.a_z_id_to_char[token_id] = c

        print(f"ğŸ”¤ Found {len(self.a_z_char_to_id)} a-z characters in tokenizer")

        # åŠ è½½æ•°æ®
        self.raw_data = self._load_jsonl(data_path)
        self.data = self.raw_data.copy()

        # å¤„ç†æ•°æ®
        self.processed_data = self._process_data()

        print(f"ğŸ“š {'Train' if is_training else 'Test'} Dataset:")
        print(f"   Raw samples: {len(self.data):,}")
        print(f"   Processed samples: {len(self.processed_data):,}")
        print(f"   Max sequence length: {config.max_seq_length}")
        print(f"   ğŸ¯ Loss type: {config.loss_type}")
        print(f"   ğŸ¯ Temperature scaling: {config.temperature_scaling}")

        # åªæ˜¾ç¤ºå°‘é‡æ ·æœ¬
        if len(self.processed_data) > 0:
            self._show_samples(2)

    def _load_jsonl(self, file_path: str) -> List[Dict]:
        """åŠ è½½JSONLæ–‡ä»¶"""
        data = []
        with open(file_path, 'r', encoding='utf-8') as f:
            for line_idx, line in enumerate(f):
                try:
                    item = json.loads(line.strip())
                    data.append(item)
                except json.JSONDecodeError as e:
                    print(f"âš ï¸  Warning: Failed to parse line {line_idx + 1}: {e}")
                    continue
        return data

    def get_raw_sample(self, idx: int) -> Dict:
        """è·å–åŸå§‹æ•°æ®æ ·æœ¬ï¼ˆç”¨äºwandbè®°å½•ï¼‰"""
        return self.raw_data[idx]

    def sample_raw_data(self, num_samples: int = 5) -> List[Dict]:
        """éšæœºé‡‡æ ·åŸå§‹æ•°æ®"""
        if len(self.raw_data) <= num_samples:
            return self.raw_data.copy()
        return random.sample(self.raw_data, num_samples)

    def _process_data(self) -> List[Dict]:
        """å¤„ç†æ•°æ®ä¸ºSFTæ ¼å¼ - åªä¿ç•™æœ‰soft_targetsçš„æ ·æœ¬"""
        processed = []
        no_soft_targets_count = 0
        invalid_soft_targets_count = 0

        for idx, item in enumerate(self.data):
            try:
                # æå–åŸºæœ¬ä¿¡æ¯
                guess_data = item["guess"]
                prompt = guess_data["prompt"]
                completion = guess_data["completion"]
                label = item["label"]
                soft_targets = item.get("soft_targets", {})

                # ğŸ¯ å¿…é¡»æœ‰soft_targetsæ‰èƒ½è®­ç»ƒ
                if not soft_targets or len(soft_targets) == 0:
                    no_soft_targets_count += 1
                    continue

                # éªŒè¯completionæ˜¯å•ä¸ªå­—ç¬¦ä¸”åœ¨a-zèŒƒå›´å†…
                if len(completion) != 1:
                    continue

                if completion not in 'abcdefghijklmnopqrstuvwxyz':
                    continue

                # ğŸ¯ éªŒè¯soft_targetsçš„æœ‰æ•ˆæ€§
                valid_soft_targets = {}
                total_prob = 0.0
                for char, prob in soft_targets.items():
                    if char in 'abcdefghijklmnopqrstuvwxyz' and prob > 0:
                        valid_soft_targets[char] = float(prob)
                        total_prob += float(prob)

                if len(valid_soft_targets) == 0 or total_prob <= 0:
                    invalid_soft_targets_count += 1
                    continue

                # ç¼–ç promptï¼ˆä¸åŒ…å«completionï¼‰
                encoded_prompt = self.tokenizer.encode(prompt, add_special_tokens=False)

                # æ£€æŸ¥é•¿åº¦
                if len(encoded_prompt) >= self.config.max_seq_length:
                    continue

                # ğŸ¯ è½¬æ¢soft_targetsä¸ºtensoræ ¼å¼
                soft_targets_tensor = self._convert_soft_targets_to_tensor(valid_soft_targets)

                processed_item = {
                    'input_ids': encoded_prompt,  # ğŸ¯ åªåŒ…å«promptï¼Œä¸åŒ…å«completion
                    'prompt_length': len(encoded_prompt),
                    'prompt': prompt,
                    'completion': completion,
                    'label': label,
                    'soft_targets': valid_soft_targets,
                    'soft_targets_tensor': soft_targets_tensor,  # [26]
                    'sample_id': idx,
                    'original_data': item
                }

                processed.append(processed_item)

            except Exception as e:
                print(f"âŒ Error processing sample {idx}: {e}")
                continue

        print(f"ğŸ“Š Data filtering results:")
        print(f"   âŒ No soft targets: {no_soft_targets_count}")
        print(f"   âŒ Invalid soft targets: {invalid_soft_targets_count}")
        print(f"   âœ… Valid samples: {len(processed)}")

        return processed

    def _convert_soft_targets_to_tensor(self, soft_targets: Dict[str, float]) -> torch.Tensor:
        """
        å°†soft_targetså­—å…¸è½¬æ¢ä¸ºtensoræ ¼å¼
        è¿”å›é•¿åº¦ä¸º26çš„tensorï¼Œå¯¹åº”a-zå­—ç¬¦çš„æ¦‚ç‡åˆ†å¸ƒ
        """
        # åˆ›å»º26ç»´çš„é›¶å‘é‡
        soft_tensor = torch.zeros(26, dtype=torch.float32)

        # å¡«å……æ¦‚ç‡å€¼
        total_prob = 0.0
        for char, prob in soft_targets.items():
            if char in 'abcdefghijklmnopqrstuvwxyz':
                char_idx = ord(char) - ord('a')  # a=0, b=1, ..., z=25
                soft_tensor[char_idx] = float(prob)
                total_prob += float(prob)

        # å½’ä¸€åŒ–ç¡®ä¿æ¦‚ç‡å’Œä¸º1
        if total_prob > 0:
            soft_tensor = soft_tensor / total_prob
        else:
            # å¦‚æœæ¦‚ç‡å’Œä¸º0ï¼Œä½¿ç”¨å‡åŒ€åˆ†å¸ƒä½œä¸ºfallback
            soft_tensor = torch.ones(26, dtype=torch.float32) / 26

        return soft_tensor

    def _show_samples(self, num_samples: int = 2):
        """æ˜¾ç¤ºæ ·æœ¬"""
        print(f"\nğŸ“ Sample Data:")
        for i in range(min(num_samples, len(self.processed_data))):
            item = self.processed_data[i]
            soft_targets = item['soft_targets']

            print(f"   Sample {i + 1}: '{item['prompt']}' â†’ '{item['completion']}' (label: {item['label']})")

            # æ˜¾ç¤ºå‰3ä¸ªæœ€é«˜æ¦‚ç‡çš„å­—ç¬¦
            sorted_targets = sorted(soft_targets.items(), key=lambda x: x[1], reverse=True)[:3]
            print(f"     Top soft targets: {sorted_targets}")
            print(f"     Target distribution sum: {sum(soft_targets.values()):.6f}")

    def __len__(self):
        return len(self.processed_data)

    def __getitem__(self, idx):
        item = self.processed_data[idx]

        # ğŸ¯ åªå‡†å¤‡promptï¼Œä¸åŒ…å«completion
        input_ids = item['input_ids'].copy()
        prompt_length = len(input_ids)

        # Padding
        max_len = self.config.max_seq_length
        if len(input_ids) < max_len:
            padding_length = max_len - len(input_ids)
            input_ids.extend([self.tokenizer.pad_token_id] * padding_length)

        # ç¡®ä¿é•¿åº¦æ­£ç¡®
        input_ids = input_ids[:max_len]

        return {
            'input_ids': torch.tensor(input_ids, dtype=torch.long),
            'attention_mask': torch.tensor([1 if id != self.tokenizer.pad_token_id else 0 for id in input_ids],
                                           dtype=torch.long),
            'prompt': item['prompt'],
            'completion': item['completion'],
            'label': item['label'],
            'soft_targets': item['soft_targets'],
            'soft_targets_tensor': item['soft_targets_tensor'],  # [26]
            'prompt_length': prompt_length,
            'sample_id': item['sample_id'],
            'original_data': item['original_data']
        }


class WordleTokenizerWrapper:
    """åŒ…è£…EnhancedCharacterTokenizerä»¥å…¼å®¹HuggingFaceæ ¼å¼"""

    def __init__(self, tokenizer: EnhancedCharacterTokenizer):
        self.tokenizer = tokenizer

        # HuggingFaceå…¼å®¹å±æ€§
        self.pad_token = '<pad>'
        self.eos_token = '<eos>'
        self.bos_token = '<bos>'
        self.unk_token = '<unk>'

        self.pad_token_id = tokenizer.pad_token_id
        self.eos_token_id = tokenizer.eos_token_id
        self.bos_token_id = tokenizer.bos_token_id
        self.unk_token_id = tokenizer.unk_token_id

        self.vocab_size = tokenizer.vocab_size

        # ğŸ”§ ä¿®å¤ï¼šæ·»åŠ ç¼ºå¤±çš„å±æ€§
        self.model_input_names = ["input_ids"]  # ç”¨äºgroup_by_lengthåŠŸèƒ½
        self.model_max_length = 512  # é»˜è®¤æœ€å¤§é•¿åº¦
        self.name_or_path = "wordle_tokenizer"  # tokenizeråç§°
        self.is_fast = False  # ä¸æ˜¯fast tokenizer

        # æ·»åŠ æ›´å¤šHuggingFace tokenizeréœ€è¦çš„å±æ€§
        self.deprecation_warnings = {}
        self.added_tokens_encoder = {}
        self.added_tokens_decoder = {}

        # ç‰¹æ®Štokenå±æ€§
        self.special_tokens_map = {
            "pad_token": self.pad_token,
            "eos_token": self.eos_token,
            "bos_token": self.bos_token,
            "unk_token": self.unk_token,
        }

    def encode(self, text, add_special_tokens=True, **kwargs):
        return self.tokenizer.encode(text, add_special_tokens=add_special_tokens)

    def decode(self, token_ids, skip_special_tokens=False, **kwargs):
        return self.tokenizer.decode(token_ids, skip_special_tokens=skip_special_tokens)

    def __call__(self, text, **kwargs):
        if isinstance(text, str):
            return {'input_ids': self.encode(text, **kwargs)}
        elif isinstance(text, list):
            return {'input_ids': [self.encode(t, **kwargs) for t in text]}

    def save_pretrained(self, save_directory, **kwargs):
        """ä¿å­˜tokenizer"""
        os.makedirs(save_directory, exist_ok=True)
        tokenizer_path = os.path.join(save_directory, "tokenizer.json")
        try:
            self.tokenizer.save_config(tokenizer_path)
            print(f"âœ… Tokenizer saved to {tokenizer_path}")
        except Exception as e:
            print(f"âš ï¸  Warning: Failed to save tokenizer: {e}")

    @classmethod
    def from_pretrained(cls, pretrained_model_name_or_path, **kwargs):
        """åŠ è½½tokenizer"""
        tokenizer_path = os.path.join(pretrained_model_name_or_path, "tokenizer.json")
        if os.path.exists(tokenizer_path):
            tokenizer = EnhancedCharacterTokenizer(config_path=tokenizer_path)
            return cls(tokenizer)
        else:
            raise FileNotFoundError(f"Tokenizer not found at {tokenizer_path}")

    def __len__(self):
        """è¿”å›vocab size"""
        return self.vocab_size

    def convert_tokens_to_ids(self, tokens):
        """è½¬æ¢tokensä¸ºids"""
        if isinstance(tokens, str):
            return self.tokenizer.char_to_id.get(tokens, self.unk_token_id)
        return [self.tokenizer.char_to_id.get(token, self.unk_token_id) for token in tokens]

    def convert_ids_to_tokens(self, ids):
        """è½¬æ¢idsä¸ºtokens"""
        if isinstance(ids, int):
            return self.tokenizer.id_to_char.get(ids, self.unk_token)
        return [self.tokenizer.id_to_char.get(id, self.unk_token) for id in ids]


class ValidationSamplingCallback(TrainerCallback):
    """æ¯ä¸ªepochç»“æŸæ—¶é‡‡æ ·validationæ•°æ®å¹¶è®°å½•åˆ°wandbçš„å›è°ƒ"""

    def __init__(self, eval_dataset, enhanced_tokenizer, config):
        self.eval_dataset = eval_dataset
        self.enhanced_tokenizer = enhanced_tokenizer
        self.config = config

    def on_epoch_end(self, args, state, control, model=None, **kwargs):
        """åœ¨æ¯ä¸ªepochç»“æŸæ—¶æ‰§è¡Œ"""
        if not self.config.use_wandb:
            return

        try:
            print(f"\nğŸ“Š Sampling {self.config.samples_per_epoch} validation samples for epoch {state.epoch}...")

            # éšæœºé‡‡æ ·åŸå§‹æ•°æ®
            sampled_data = self.eval_dataset.sample_raw_data(self.config.samples_per_epoch)

            # å¯¹æ¯ä¸ªæ ·æœ¬è¿›è¡Œé¢„æµ‹
            predictions = self._predict_samples(model, sampled_data)

            # åˆ›å»ºwandbè¡¨æ ¼
            self._log_to_wandb(sampled_data, predictions, state.epoch)

        except Exception as e:
            print(f"âŒ Error in validation sampling: {e}")
            import traceback
            traceback.print_exc()

    def on_evaluate(self, args, state, control, logs=None, model=None, **kwargs):
        """ğŸ”§ é‡å†™on_evaluateä»¥ç¡®ä¿åœ¨æ¯æ¬¡è¯„ä¼°æ—¶éƒ½æœ‰é‡‡æ ·ï¼ˆä¸ä»…ä»…æ˜¯epochç»“æŸï¼‰"""
        # åªåœ¨stepsç­–ç•¥ä¸‹è¿›è¡Œé‡‡æ ·ï¼Œé¿å…é‡å¤
        if (hasattr(args, 'eval_strategy') and args.eval_strategy == "steps" and
                self.config.use_wandb and state.global_step % args.eval_steps == 0):

            try:
                print(
                    f"\nğŸ“Š Sampling {self.config.samples_per_epoch} validation samples for step {state.global_step}...")

                # éšæœºé‡‡æ ·åŸå§‹æ•°æ®
                sampled_data = self.eval_dataset.sample_raw_data(self.config.samples_per_epoch)

                # å¯¹æ¯ä¸ªæ ·æœ¬è¿›è¡Œé¢„æµ‹
                if model is not None:
                    predictions = self._predict_samples(model, sampled_data)

                    # åˆ›å»ºwandbè¡¨æ ¼
                    self._log_to_wandb(sampled_data, predictions, f"step_{state.global_step}")

            except Exception as e:
                print(f"âŒ Error in validation sampling on evaluate: {e}")

    def _predict_samples(self, model, sampled_data: List[Dict]) -> List[str]:
        """å¯¹é‡‡æ ·æ•°æ®è¿›è¡Œé¢„æµ‹"""
        if model is None:
            return ['?'] * len(sampled_data)

        model.eval()
        predictions = []

        try:
            model_device = next(model.parameters()).device
        except (StopIteration, AttributeError):
            model_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        with torch.no_grad():
            for sample in sampled_data:
                try:
                    # æå–ä¿¡æ¯
                    guess_data = sample.get("guess", {})
                    prompt = guess_data.get("prompt", "")

                    if not prompt:
                        predictions.append('?')
                        continue

                    # ç¼–ç prompt
                    prompt_encoded = self.enhanced_tokenizer.encode(prompt, add_special_tokens=False)

                    if len(prompt_encoded) == 0:
                        predictions.append('?')
                        continue

                    # å‡†å¤‡è¾“å…¥
                    input_ids = torch.tensor([prompt_encoded], dtype=torch.long, device=model_device)

                    # ä½¿ç”¨æˆ‘ä»¬GPTæ¨¡å‹çš„å‚æ•°å
                    logits = model(x=input_ids)

                    # è·å–æœ€åä¸€ä¸ªä½ç½®çš„é¢„æµ‹
                    last_position_logits = logits[0, -1, :]

                    # é™åˆ¶è¾“å‡ºåˆ°a-zå­—ç¬¦
                    a_z_indices = []
                    for c in 'abcdefghijklmnopqrstuvwxyz':
                        if c in self.enhanced_tokenizer.char_to_id:
                            a_z_indices.append(self.enhanced_tokenizer.char_to_id[c])

                    if len(a_z_indices) == 0:
                        predictions.append('?')
                        continue

                    # åˆ›å»ºmaskï¼Œåªå…è®¸a-zå­—ç¬¦
                    masked_logits = torch.full_like(last_position_logits, float('-inf'))
                    masked_logits[a_z_indices] = last_position_logits[a_z_indices]

                    # é¢„æµ‹
                    pred_char_id = torch.argmax(masked_logits).item()
                    pred_char = self.enhanced_tokenizer.decode([pred_char_id])

                    # ç¡®ä¿é¢„æµ‹æ˜¯å•ä¸ªa-zå­—ç¬¦
                    if len(pred_char) == 1 and pred_char in 'abcdefghijklmnopqrstuvwxyz':
                        predictions.append(pred_char)
                    else:
                        predictions.append('?')

                except Exception as e:
                    predictions.append('?')

        return predictions

    def _log_to_wandb(self, sampled_data: List[Dict], predictions: List[str], epoch_or_step):
        """è®°å½•åˆ°wandb"""
        try:
            if not sampled_data or not predictions:
                return

            # åˆ›å»ºè¡¨æ ¼ - åŒ…å«æ‰€æœ‰requiredå­—æ®µ
            table = wandb.Table(columns=[
                "epoch_or_step",
                "prompt",
                "completion",
                "predicted_char",
                "is_correct",
                "label",
                "soft_targets"
            ])

            # æ·»åŠ æ•°æ®
            for i, (sample, prediction) in enumerate(zip(sampled_data, predictions)):
                try:
                    guess_data = sample.get("guess", {})
                    prompt = guess_data.get("prompt", "")
                    completion = guess_data.get("completion", "")
                    label = sample.get("label", "")
                    soft_targets = sample.get("soft_targets", {})

                    # åˆ¤æ–­é¢„æµ‹æ˜¯å¦æ­£ç¡®
                    is_correct = prediction == completion

                    # æ ¼å¼åŒ–soft_targetsä¸ºå­—ç¬¦ä¸²
                    soft_targets_str = json.dumps(soft_targets, separators=(',', ':'))

                    table.add_data(
                        str(epoch_or_step),
                        prompt,
                        completion,
                        prediction,
                        is_correct,
                        label,
                        soft_targets_str
                    )
                except Exception as e:
                    continue

            # è®°å½•åˆ°wandb
            wandb.log({
                f"validation_samples_{epoch_or_step}": table,
            })

            # è®¡ç®—å‡†ç¡®ç‡
            correct_predictions = sum(1 for p, s in zip(predictions, sampled_data)
                                      if p == s.get("guess", {}).get("completion", ""))
            accuracy = correct_predictions / len(predictions) if predictions else 0

            # è®°å½•ç»Ÿè®¡ä¿¡æ¯
            wandb.log({
                f"validation_sample_accuracy_{epoch_or_step}": accuracy,
                f"validation_samples_count_{epoch_or_step}": len(predictions),
                f"validation_correct_count_{epoch_or_step}": correct_predictions,
            })

            print(f"âœ… Logged {len(predictions)} validation samples to wandb")
            print(f"   Sample accuracy: {accuracy:.3f} ({correct_predictions}/{len(predictions)})")

            # æ˜¾ç¤ºæ ·æœ¬è¯¦æƒ…
            print(f"   Predictions:")
            for i, (sample, prediction) in enumerate(zip(sampled_data, predictions)):
                try:
                    completion = sample.get("guess", {}).get("completion", "")
                    label = sample.get("label", "")
                    status = "âœ…" if prediction == completion else "âŒ"
                    print(f"     {i + 1}. {status} '{completion}' â†’ '{prediction}' (label: '{label}')")
                except Exception as e:
                    print(f"     {i + 1}. âŒ Error displaying sample")

        except Exception as e:
            print(f"âŒ Error logging to wandb: {e}")
            import traceback
            traceback.print_exc()


class WordleSFTTrainer(Trainer):
    """ğŸ¯ Pure Soft Targetsè®­ç»ƒå™¨"""

    def __init__(self, *args, sft_config=None, enhanced_tokenizer=None, **kwargs):
        super().__init__(*args, **kwargs)
        self.sft_config = sft_config
        self.enhanced_tokenizer = enhanced_tokenizer
        self.label_names = []  # ğŸ¯ ä¸ä½¿ç”¨labels

        # ğŸ¯ é¢„è®¡ç®—a-zå­—ç¬¦çš„token indices
        self.a_z_token_indices = []
        for c in 'abcdefghijklmnopqrstuvwxyz':
            if c in enhanced_tokenizer.char_to_id:
                self.a_z_token_indices.append(enhanced_tokenizer.char_to_id[c])

        print(f"ğŸ¯ Pure soft targets training:")
        print(f"   Loss type: {sft_config.loss_type}")
        print(f"   A-Z token indices: {len(self.a_z_token_indices)}")
        print(f"   Temperature scaling: {sft_config.temperature_scaling}")
        if sft_config.loss_type == "cross_entropy":
            print(f"   Label smoothing: {sft_config.label_smoothing}")

    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):
        """ğŸ¯ è®¡ç®—pure soft targets loss"""

        # æå–è¾“å…¥
        input_ids = inputs['input_ids']  # [batch, seq_len]
        attention_mask = inputs.get('attention_mask', None)
        soft_targets_tensor = inputs['soft_targets_tensor']  # [batch, 26]
        prompt_lengths = inputs['prompt_length']  # [batch]

        # ä½¿ç”¨æˆ‘ä»¬GPTæ¨¡å‹çš„å‚æ•°å
        outputs = model(x=input_ids, attn_mask=attention_mask)
        logits = outputs  # [batch, seq_len, vocab_size]

        # ğŸ¯ è®¡ç®—Soft Targets Loss
        soft_loss = self._compute_soft_targets_loss(
            logits, soft_targets_tensor, prompt_lengths
        )

        if soft_loss is None:
            # Fallback: å¦‚æœæ— æ³•è®¡ç®—soft lossï¼Œä½¿ç”¨å°çš„dummy loss
            soft_loss = torch.tensor(0.01, device=logits.device, requires_grad=True)

        # è®°å½•lossåˆ°wandb
        if self.sft_config.use_wandb:
            try:
                wandb.log({
                    'soft_loss': soft_loss.item(),
                    'loss_type': self.sft_config.loss_type
                })
            except:
                pass

        if return_outputs:
            class SimpleOutputs:
                def __init__(self, logits):
                    self.logits = logits

            return soft_loss, SimpleOutputs(logits)

        return soft_loss

    def _compute_soft_targets_loss(self, logits, soft_targets_tensor, prompt_lengths):
        """
        ğŸ¯ è®¡ç®—pure soft targets loss
        æ”¯æŒKLæ•£åº¦å’Œäº¤å‰ç†µä¸¤ç§lossç±»å‹

        Args:
            logits: [batch, seq_len, vocab_size]
            soft_targets_tensor: [batch, 26] - a-zå­—ç¬¦çš„æ¦‚ç‡åˆ†å¸ƒ
            prompt_lengths: [batch] - æ¯ä¸ªæ ·æœ¬çš„prompté•¿åº¦
        """
        try:
            batch_size = logits.size(0)
            device = logits.device

            # ç§»åŠ¨tensorsåˆ°æ­£ç¡®çš„è®¾å¤‡
            if soft_targets_tensor.device != device:
                soft_targets_tensor = soft_targets_tensor.to(device)
            if prompt_lengths.device != device:
                prompt_lengths = prompt_lengths.to(device)

            valid_losses = []

            for i in range(batch_size):
                # è·å–completioné¢„æµ‹ä½ç½®çš„logits
                prompt_len = prompt_lengths[i].item()
                if prompt_len <= 0 or prompt_len > logits.size(1):
                    continue

                # åœ¨promptæœ€åä½ç½®é¢„æµ‹ä¸‹ä¸€ä¸ªå­—ç¬¦ï¼ˆcompletionï¼‰
                completion_logits = logits[i, prompt_len - 1, :]  # [vocab_size]

                # ğŸ¯ æå–a-zå­—ç¬¦çš„logits
                if len(self.a_z_token_indices) == 0:
                    continue

                a_z_logits = completion_logits[self.a_z_token_indices]  # [26]

                # åº”ç”¨æ¸©åº¦ç¼©æ”¾
                if self.sft_config.temperature_scaling != 1.0:
                    a_z_logits = a_z_logits / self.sft_config.temperature_scaling

                # è·å–å¯¹åº”çš„soft targets
                target_dist = soft_targets_tensor[i]  # [26]

                # ç¡®ä¿target distributionæ˜¯æœ‰æ•ˆçš„
                if target_dist.sum() <= 0:
                    continue

                # ğŸ¯ æ ¹æ®loss_typeè®¡ç®—loss
                if self.sft_config.loss_type == "kl_divergence":
                    # KLæ•£åº¦loss
                    log_probs = F.log_softmax(a_z_logits, dim=-1)
                    sample_loss = F.kl_div(log_probs, target_dist, reduction='sum')

                elif self.sft_config.loss_type == "cross_entropy":
                    # äº¤å‰ç†µloss
                    if self.sft_config.label_smoothing > 0:
                        # å¸¦æ ‡ç­¾å¹³æ»‘çš„äº¤å‰ç†µ
                        smoothed_targets = self._apply_label_smoothing(target_dist, self.sft_config.label_smoothing)
                        log_probs = F.log_softmax(a_z_logits, dim=-1)
                        sample_loss = -(smoothed_targets * log_probs).sum()
                    else:
                        # æ ‡å‡†äº¤å‰ç†µ
                        log_probs = F.log_softmax(a_z_logits, dim=-1)
                        sample_loss = -(target_dist * log_probs).sum()

                else:
                    raise ValueError(f"Unsupported loss type: {self.sft_config.loss_type}")

                valid_losses.append(sample_loss)

            if len(valid_losses) == 0:
                return None

            # å¹³å‡æ‰€æœ‰valid samplesçš„loss
            avg_soft_loss = torch.stack(valid_losses).mean()

            return avg_soft_loss

        except Exception as e:
            print(f"âŒ Error computing soft targets loss: {e}")
            return None

    def _apply_label_smoothing(self, targets, smoothing):
        """åº”ç”¨æ ‡ç­¾å¹³æ»‘"""
        n_classes = targets.size(-1)
        smoothed = targets * (1 - smoothing) + smoothing / n_classes
        return smoothed

    def prediction_step(self, model, inputs, prediction_loss_only=False, ignore_keys=None):
        """é¢„æµ‹æ­¥éª¤"""
        # å°†è¾“å…¥ç§»åŠ¨åˆ°æ­£ç¡®çš„è®¾å¤‡
        input_ids = inputs['input_ids']
        attention_mask = inputs.get('attention_mask')

        model.eval()

        try:
            model_device = next(model.parameters()).device
        except StopIteration:
            model_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        input_ids = input_ids.to(model_device)
        if attention_mask is not None:
            attention_mask = attention_mask.to(model_device)

        with torch.no_grad():
            try:
                logits = model(x=input_ids, attn_mask=attention_mask)

                # è®¡ç®—loss
                soft_loss = self._compute_soft_targets_loss(
                    logits,
                    inputs['soft_targets_tensor'],
                    inputs['prompt_length']
                )

                if soft_loss is None:
                    soft_loss = torch.tensor(0.0, device=model_device)

                if prediction_loss_only:
                    return (soft_loss, None, None)
                else:
                    return (soft_loss, logits, None)

            except Exception as e:
                dummy_loss = torch.tensor(0.0, device=model_device, requires_grad=True)
                dummy_logits = torch.zeros((input_ids.size(0), input_ids.size(1), self.model.vocab_size),
                                           device=model_device)
                return (dummy_loss, dummy_logits, None)

    def evaluation_loop(self, dataloader, description, prediction_loss_only=None, ignore_keys=None,
                        metric_key_prefix="eval"):
        """è¯„ä¼°å¾ªç¯"""

        try:
            # è°ƒç”¨çˆ¶ç±»çš„è¯„ä¼°å¾ªç¯
            output = super().evaluation_loop(dataloader, description, prediction_loss_only, ignore_keys,
                                             metric_key_prefix)

            # ç¡®ä¿output.metricså­˜åœ¨
            if not hasattr(output, 'metrics') or output.metrics is None:
                from transformers.trainer_utils import EvalLoopOutput
                output = EvalLoopOutput(
                    predictions=None,
                    label_ids=None,
                    metrics={},
                    num_samples=len(dataloader.dataset) if hasattr(dataloader, 'dataset') else 0
                )

            # ç¡®ä¿åŸºæœ¬çš„eval_losså­˜åœ¨
            if f'{metric_key_prefix}_loss' not in output.metrics:
                output.metrics[f'{metric_key_prefix}_loss'] = 0.0

            # æ·»åŠ è‡ªå®šä¹‰è¯„ä¼°æŒ‡æ ‡
            custom_metrics = self._compute_eval_metrics(dataloader, metric_key_prefix)

            # åˆå¹¶è‡ªå®šä¹‰æŒ‡æ ‡
            if custom_metrics:
                output.metrics.update(custom_metrics)

            return output

        except Exception as e:
            print(f"âŒ Error in evaluation_loop: {e}")
            import traceback
            traceback.print_exc()

            # è¿”å›ä¸€ä¸ªåŸºæœ¬çš„ç»“æœé¿å…è®­ç»ƒä¸­æ–­
            from transformers.trainer_utils import EvalLoopOutput
            basic_metrics = {
                f'{metric_key_prefix}_loss': 0.0,
                f'{metric_key_prefix}_char_accuracy': 0.0,
                f'{metric_key_prefix}_runtime': 0.0,
                f'{metric_key_prefix}_samples_per_second': 0.0,
                f'{metric_key_prefix}_steps_per_second': 0.0,
            }

            return EvalLoopOutput(
                predictions=None,
                label_ids=None,
                metrics=basic_metrics,
                num_samples=len(dataloader.dataset) if hasattr(dataloader, 'dataset') else 0
            )

    def _compute_eval_metrics(self, dataloader, metric_key_prefix="eval"):
        """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
        model = self.model
        model.eval()

        total_samples = 0
        correct_predictions = 0
        total_loss = 0.0
        num_batches = 0

        try:
            model_device = next(model.parameters()).device
        except StopIteration:
            model_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        print(f"ğŸ” Computing eval metrics on device: {model_device}")

        with torch.no_grad():
            for batch_idx, batch in enumerate(dataloader):
                try:
                    # é™åˆ¶è¯„ä¼°batchæ•°é‡ä»¥èŠ‚çœæ—¶é—´
                    if batch_idx >= 50:
                        break

                    # ç§»åŠ¨åˆ°è®¾å¤‡
                    input_ids = batch['input_ids'].to(model_device)
                    attention_mask = batch['attention_mask'].to(model_device)
                    soft_targets_tensor = batch['soft_targets_tensor'].to(model_device)
                    prompt_lengths = batch['prompt_length'].to(model_device)

                    if input_ids.size(0) == 0:
                        continue

                    # ä½¿ç”¨æˆ‘ä»¬GPTæ¨¡å‹çš„å‚æ•°å
                    logits = model(x=input_ids, attn_mask=attention_mask)

                    # è®¡ç®—loss
                    batch_loss = self._compute_soft_targets_loss(
                        logits, soft_targets_tensor, prompt_lengths
                    )

                    if batch_loss is not None:
                        total_loss += batch_loss.item()
                        num_batches += 1

                    # é€æ ·æœ¬åˆ†æ
                    batch_size = input_ids.size(0)
                    for i in range(batch_size):
                        try:
                            # è·å–æ ·æœ¬ä¿¡æ¯
                            prompt = batch['prompt'][i]
                            true_completion = batch['completion'][i]
                            prompt_length = prompt_lengths[i].item()

                            if prompt_length > 0 and prompt_length <= logits.size(1):
                                # é¢„æµ‹completion
                                pred_logits = logits[i, prompt_length - 1, :]

                                # é™åˆ¶è¾“å‡ºåˆ°a-zå­—ç¬¦
                                a_z_indices = [self.enhanced_tokenizer.char_to_id[c] for c in
                                               'abcdefghijklmnopqrstuvwxyz'
                                               if c in self.enhanced_tokenizer.char_to_id]

                                # åˆ›å»ºmask
                                masked_logits = torch.full_like(pred_logits, float('-inf'))
                                if len(a_z_indices) > 0:
                                    masked_logits[a_z_indices] = pred_logits[a_z_indices]

                                pred_char_id = torch.argmax(masked_logits).item()
                                pred_char = self.enhanced_tokenizer.decode([pred_char_id])

                                # ç»Ÿè®¡
                                total_samples += 1
                                if pred_char == true_completion:
                                    correct_predictions += 1

                        except Exception as e:
                            continue

                except Exception as e:
                    print(f"âš ï¸  Warning: Error in eval batch {batch_idx}: {e}")
                    continue

        # è®¡ç®—æœ€ç»ˆæŒ‡æ ‡
        char_accuracy = correct_predictions / total_samples if total_samples > 0 else 0.0
        avg_loss = total_loss / num_batches if num_batches > 0 else 0.0

        # è®°å½•æŒ‡æ ‡
        metrics = {
            f'{metric_key_prefix}_char_accuracy': char_accuracy,
            f'{metric_key_prefix}_total_samples': total_samples,
            f'{metric_key_prefix}_correct_predictions': correct_predictions,
            f'{metric_key_prefix}_avg_loss': avg_loss,
        }

        print(f"\nğŸ“Š {metric_key_prefix.title()} Custom Metrics:")
        print(f"   Character Accuracy: {char_accuracy:.4f} ({correct_predictions}/{total_samples})")
        print(f"   Average Loss: {avg_loss:.4f}")
        print(f"   ğŸ“¤ Returning metrics: {list(metrics.keys())}")

        return metrics


def create_data_collator(tokenizer):
    """åˆ›å»ºæ•°æ®æ•´ç†å™¨ - Pure Soft Targetsç‰ˆæœ¬"""

    def collate_fn(batch):
        # æå–æ‰€æœ‰å­—æ®µ
        input_ids = torch.stack([item['input_ids'] for item in batch])
        attention_mask = torch.stack([item['attention_mask'] for item in batch])

        # ğŸ¯ Soft targetsç›¸å…³å­—æ®µ
        soft_targets_tensor = torch.stack([item['soft_targets_tensor'] for item in batch])
        prompt_lengths = torch.tensor([item['prompt_length'] for item in batch], dtype=torch.long)

        # å…¶ä»–å­—æ®µ
        prompts = [item['prompt'] for item in batch]
        completions = [item['completion'] for item in batch]
        labels_list = [item['label'] for item in batch]
        soft_targets = [item['soft_targets'] for item in batch]
        sample_ids = [item['sample_id'] for item in batch]
        original_data = [item['original_data'] for item in batch]

        return {
            'input_ids': input_ids,
            'attention_mask': attention_mask,
            # ğŸ¯ Soft targetså­—æ®µ
            'soft_targets_tensor': soft_targets_tensor,
            'prompt_length': prompt_lengths,
            # å…¶ä»–å­—æ®µ
            'prompt': prompts,
            'completion': completions,
            'label': labels_list,
            'soft_targets': soft_targets,
            'sample_id': sample_ids,
            'original_data': original_data
        }

    return collate_fn


def main():
    parser = argparse.ArgumentParser(description="ğŸ¯ Wordle SFT Training with Pure Soft Targets + Early Stopping")

    # åŸºç¡€å‚æ•°
    parser.add_argument("--pretrain_model", default="checkpoints/pretrain.pth", help="é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„")
    parser.add_argument("--tokenizer", default="scripts/enhanced_tokenizer/tokenizer.json", help="Tokenizeré…ç½®æ–‡ä»¶")
    parser.add_argument("--train_data", default="sft/data/train.jsonl", help="è®­ç»ƒæ•°æ®è·¯å¾„")
    parser.add_argument("--test_data", default="sft/data/test.jsonl", help="æµ‹è¯•æ•°æ®è·¯å¾„")
    parser.add_argument("--output_dir", default="sft/models", help="è¾“å‡ºç›®å½•")

    # è®­ç»ƒå‚æ•° - RTX 4090ä¼˜åŒ–
    parser.add_argument("--learning_rate", type=float, default=1e-3, help="å­¦ä¹ ç‡")
    parser.add_argument("--batch_size", type=int, default=32, help="æ‰¹å¤§å°")
    parser.add_argument("--eval_batch_size", type=int, default=64, help="è¯„ä¼°æ‰¹å¤§å°")
    parser.add_argument("--num_epochs", type=int, default=3, help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--warmup_steps", type=int, default=100, help="é¢„çƒ­æ­¥æ•°")
    parser.add_argument("--max_seq_length", type=int, default=64, help="æœ€å¤§åºåˆ—é•¿åº¦")
    parser.add_argument("--gradient_accumulation_steps", type=int, default=2, help="æ¢¯åº¦ç´¯ç§¯æ­¥æ•°")
    parser.add_argument("--weight_decay", type=float, default=0.01, help="æƒé‡è¡°å‡")
    parser.add_argument("--logging_steps", type=int, default=50, help="æ—¥å¿—è®°å½•æ­¥æ•°")
    parser.add_argument("--eval_steps", type=int, default=200, help="è¯„ä¼°æ­¥æ•°")
    parser.add_argument("--save_steps", type=int, default=500, help="ä¿å­˜æ­¥æ•°")

    # ğŸ¯ Pure Soft targetså‚æ•°
    parser.add_argument("--loss_type", choices=["kl_divergence", "cross_entropy"], default="kl_divergence",
                        help="Lossç±»å‹: kl_divergence (é»˜è®¤) æˆ– cross_entropy")
    parser.add_argument("--temperature_scaling", type=float, default=1.0, help="è¾“å‡ºlogitsçš„æ¸©åº¦ç¼©æ”¾")
    parser.add_argument("--label_smoothing", type=float, default=0.0,
                        help="æ ‡ç­¾å¹³æ»‘ç³»æ•° (ä»…ç”¨äºcross_entropy)")

    # ğŸ›‘ Early Stoppingå‚æ•°
    parser.add_argument("--no_early_stopping", action="store_true", help="ç¦ç”¨early stopping")
    parser.add_argument("--early_stopping_patience", type=int, default=8,
                        help="Early stopping patience (é»˜è®¤: 8)")
    parser.add_argument("--early_stopping_threshold", type=float, default=0.0001,
                        help="Early stoppingæ”¹å–„é˜ˆå€¼ (é»˜è®¤: 0.0001)")
    parser.add_argument("--early_stopping_metric", default="eval_char_accuracy",
                        help="Early stoppingç›‘æ§æŒ‡æ ‡ (é»˜è®¤: eval_char_accuracy)")

    # å¿«é€Ÿè°ƒè¯•æ¨¡å¼
    parser.add_argument("--debug", action="store_true", help="è°ƒè¯•æ¨¡å¼ï¼šå°batchï¼Œå¿«é€Ÿevaluation")

    # é‡‡æ ·å‚æ•°
    parser.add_argument("--samples_per_epoch", type=int, default=5, help="æ¯ä¸ªepoché‡‡æ ·çš„éªŒè¯æ ·æœ¬æ•°")

    # ç­–ç•¥å‚æ•°
    parser.add_argument("--no_load_best", action="store_true", help="ç¦ç”¨load_best_model_at_end")

    # wandbå‚æ•°
    parser.add_argument("--no_wandb", action="store_true", help="ç¦ç”¨wandb")
    parser.add_argument("--project_name", default="wordle-sft-pure-soft-targets", help="wandbé¡¹ç›®å")
    parser.add_argument("--experiment_name", help="å®éªŒåç§°")
    parser.add_argument("--tags", nargs="+", help="wandbæ ‡ç­¾")

    # å…¶ä»–å‚æ•°
    parser.add_argument("--seed", type=int, default=42, help="éšæœºç§å­")
    parser.add_argument("--dataloader_num_workers", type=int, default=8, help="æ•°æ®åŠ è½½å™¨å·¥ä½œè¿›ç¨‹æ•°")

    args = parser.parse_args()

    # è°ƒè¯•æ¨¡å¼å‚æ•°è¦†ç›–
    if args.debug:
        print("ğŸ”§ Debug mode enabled: using lightweight parameters")
        args.batch_size = 4
        args.eval_batch_size = 8
        args.num_epochs = 1
        args.eval_steps = 20  # ğŸ”§ æ›´å¿«çš„evaluation
        args.logging_steps = 5
        args.dataloader_num_workers = 2
        args.samples_per_epoch = 2
        args.early_stopping_patience = 2  # è°ƒè¯•æ¨¡å¼ä¸‹é™ä½patience

    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(args.seed)
    np.random.seed(args.seed)
    random.seed(args.seed)

    # åˆ›å»ºé…ç½®
    config = SFTTrainingConfig(
        pretrain_model_path=args.pretrain_model,
        tokenizer_path=args.tokenizer,
        train_data_path=args.train_data,
        test_data_path=args.test_data,
        output_dir=args.output_dir,
        learning_rate=args.learning_rate,
        batch_size=args.batch_size,
        eval_batch_size=args.eval_batch_size,
        num_epochs=args.num_epochs,
        warmup_steps=args.warmup_steps,
        max_seq_length=args.max_seq_length,
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        weight_decay=args.weight_decay,
        logging_steps=args.logging_steps,
        eval_steps=args.eval_steps,
        save_steps=args.save_steps,
        samples_per_epoch=args.samples_per_epoch,
        load_best_model_at_end=not args.no_load_best,
        # ğŸ¯ Pure Soft targetsé…ç½®
        loss_type=args.loss_type,
        temperature_scaling=args.temperature_scaling,
        label_smoothing=args.label_smoothing,
        # ğŸ›‘ Early stoppingé…ç½®
        early_stopping_enabled=not args.no_early_stopping,
        early_stopping_patience=args.early_stopping_patience,
        early_stopping_threshold=args.early_stopping_threshold,
        early_stopping_metric=args.early_stopping_metric,
        early_stopping_greater_is_better=True if args.early_stopping_metric in ["eval_char_accuracy"] else False,
        # wandbé…ç½®
        use_wandb=not args.no_wandb,
        project_name=args.project_name,
        experiment_name=args.experiment_name,
        tags=args.tags or [],
        seed=args.seed,
        dataloader_num_workers=args.dataloader_num_workers
    )

    print("ğŸš€ Starting Wordle SFT Training with Pure Soft Targets + Early Stopping")
    print(f"ğŸ“± Device: {'cuda' if torch.cuda.is_available() else 'cpu'}")
    print(f"ğŸ”§ Configuration: batch_size={config.batch_size}, eval_steps={config.eval_steps}")
    print(f"   Effective batch size: {config.batch_size * config.gradient_accumulation_steps}")
    print(f"   ğŸ¯ Loss type: {config.loss_type}")
    print(f"   ğŸ¯ Temperature scaling: {config.temperature_scaling}")
    if config.loss_type == "cross_entropy" and config.label_smoothing > 0:
        print(f"   ğŸ¯ Label smoothing: {config.label_smoothing}")
    print(f"   ğŸ›‘ Early stopping: {'enabled' if config.early_stopping_enabled else 'disabled'}")
    if config.early_stopping_enabled:
        print(f"   ğŸ›‘ Early stopping metric: {config.early_stopping_metric}")
        print(f"   ğŸ›‘ Early stopping patience: {config.early_stopping_patience}")
    print(f"   Workers: {config.dataloader_num_workers}")

    # åˆå§‹åŒ–wandb
    if config.use_wandb:
        run_name = config.experiment_name or f"pure-soft-{config.loss_type}-{int(time.time())}"
        tags = config.tags + [config.loss_type]
        if config.early_stopping_enabled:
            tags.append("early-stopping")

        wandb.init(
            project=config.project_name,
            name=run_name,
            tags=tags,
            config=config.__dict__
        )

    # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å’Œtokenizer
    print(f"\nğŸ”¤ Loading tokenizer from {config.tokenizer_path}")
    tokenizer = EnhancedCharacterTokenizer(config_path=config.tokenizer_path)

    print(f"ğŸ§  Loading pretrained model from {config.pretrain_model_path}")
    model, _, model_config = load_pretrained_model(
        config.pretrain_model_path,
        config.tokenizer_path
    )

    # åŒ…è£…tokenizer
    wrapped_tokenizer = WordleTokenizerWrapper(tokenizer)

    print(f"   Model vocab size: {model.vocab_size}")
    print(f"   Tokenizer vocab size: {tokenizer.vocab_size}")

    # åŠ è½½æ•°æ®é›†
    print(f"\nğŸ“š Loading datasets...")
    train_dataset = WordleSFTDataset(config.train_data_path, tokenizer, config, is_training=True)
    test_dataset = WordleSFTDataset(config.test_data_path, tokenizer, config, is_training=False)

    if len(train_dataset) == 0:
        print("âŒ Error: No valid training samples found!")
        print("   Make sure your data contains soft_targets field")
        return

    if len(test_dataset) == 0:
        print("âŒ Error: No valid test samples found!")
        return

    # åˆ›å»ºæ•°æ®æ•´ç†å™¨
    data_collator = create_data_collator(tokenizer)

    # ğŸ›‘ åˆ›å»ºcallbacks
    callbacks = []

    # Validation sampling callback
    validation_callback = ValidationSamplingCallback(test_dataset, tokenizer, config)
    callbacks.append(validation_callback)

    # Early stopping callback
    if config.early_stopping_enabled:
        early_stopping_callback = EarlyStoppingCallback(config)
        callbacks.append(early_stopping_callback)

    # åˆ›å»ºè®­ç»ƒå‚æ•°
    training_args = TrainingArguments(
        output_dir=config.output_dir,
        num_train_epochs=config.num_epochs,
        per_device_train_batch_size=config.batch_size,
        per_device_eval_batch_size=config.eval_batch_size,
        gradient_accumulation_steps=config.gradient_accumulation_steps,
        learning_rate=config.learning_rate,
        weight_decay=config.weight_decay,
        warmup_steps=config.warmup_steps,
        logging_steps=config.logging_steps,
        eval_steps=config.eval_steps,
        save_steps=config.save_steps,
        eval_strategy=config.eval_strategy,
        save_strategy=config.eval_strategy if config.load_best_model_at_end else "steps",
        load_best_model_at_end=config.load_best_model_at_end,
        metric_for_best_model=config.metric_for_best_model,
        greater_is_better=config.greater_is_better,
        fp16=config.fp16,
        gradient_checkpointing=config.gradient_checkpointing,
        dataloader_num_workers=config.dataloader_num_workers,
        remove_unused_columns=config.remove_unused_columns,
        report_to="wandb" if config.use_wandb else None,
        run_name=config.experiment_name,
        seed=config.seed,
        logging_first_step=True,
        save_total_limit=3,
        eval_delay=0,
        prediction_loss_only=False,
        # RTX 4090 ä¼˜åŒ–
        dataloader_pin_memory=True,
    )

    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = WordleSFTTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=test_dataset,
        data_collator=data_collator,
        tokenizer=wrapped_tokenizer,
        callbacks=callbacks,  # ğŸ›‘ æ·»åŠ callbacks
        sft_config=config,
        enhanced_tokenizer=tokenizer
    )

    trainer.label_names = []  # ğŸ¯ Pure soft targetsä¸éœ€è¦labels

    print(f"\nğŸ”¥ Starting training...")
    print(f"   Training samples: {len(train_dataset):,}")
    print(f"   Test samples: {len(test_dataset):,}")
    print(f"   Epochs: {config.num_epochs}")
    print(f"   Effective batch size: {config.batch_size * config.gradient_accumulation_steps}")
    print(f"   ğŸ¯ Will evaluate every {config.eval_steps} steps")
    print(f"   ğŸ¯ Samples per epoch: {config.samples_per_epoch}")
    print(f"   ğŸ¯ Wandb logging: {'enabled' if config.use_wandb else 'disabled'}")
    if config.early_stopping_enabled:
        print(f"   ğŸ›‘ Early stopping: patience={config.early_stopping_patience}, metric={config.early_stopping_metric}")

    # å¼€å§‹è®­ç»ƒ
    start_time = time.time()
    trainer.train()
    training_time = time.time() - start_time

    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_model_path = os.path.join(config.output_dir, "final_model")
    trainer.save_model(final_model_path)

    # æ‰‹åŠ¨ä¿å­˜tokenizeré…ç½®
    tokenizer_save_path = os.path.join(final_model_path, "tokenizer.json")
    try:
        tokenizer.save_config(tokenizer_save_path)
        print(f"âœ… Tokenizer saved to {tokenizer_save_path}")
    except Exception as e:
        print(f"âš ï¸  Warning: Failed to save tokenizer: {e}")

    # æœ€ç»ˆè¯„ä¼°
    print(f"\nğŸ§ª Final evaluation...")
    eval_results = trainer.evaluate()

    print(f"\nğŸ† Training completed!")
    print(f"   Total time: {training_time:.2f}s ({training_time / 60:.1f}min)")
    print(f"   Final eval loss: {eval_results.get('eval_loss', 'N/A')}")
    print(f"   Final char accuracy: {eval_results.get('eval_char_accuracy', 'N/A')}")
    print(f"   Model saved to: {final_model_path}")

    # ğŸ›‘ æ˜¾ç¤ºearly stoppingä¿¡æ¯
    if config.early_stopping_enabled:
        for callback in trainer.callback_handler.callbacks:
            if isinstance(callback, EarlyStoppingCallback):
                if callback.should_stop:
                    print(f"   ğŸ›‘ Training stopped early due to early stopping")
                else:
                    print(f"   âœ… Training completed without early stopping")
                break

    # wandbæ€»ç»“
    if config.use_wandb:
        summary_data = {
            'final_eval_loss': eval_results.get('eval_loss', 0),
            'final_char_accuracy': eval_results.get('eval_char_accuracy', 0),
            'training_time': training_time,
            'total_epochs': config.num_epochs,
            'model_path': final_model_path,
            'samples_per_epoch': config.samples_per_epoch,
            'effective_batch_size': config.batch_size * config.gradient_accumulation_steps,
            'loss_type': config.loss_type,
            'temperature_scaling': config.temperature_scaling,
            'label_smoothing': config.label_smoothing,
            'early_stopping_enabled': config.early_stopping_enabled
        }

        # ğŸ›‘ æ·»åŠ early stoppingç›¸å…³ä¿¡æ¯
        if config.early_stopping_enabled:
            summary_data.update({
                'early_stopping_patience': config.early_stopping_patience,
                'early_stopping_metric': config.early_stopping_metric,
                'early_stopping_threshold': config.early_stopping_threshold
            })

        wandb.run.summary.update(summary_data)
        wandb.finish()

    return trainer, eval_results


if __name__ == "__main__":
    main()