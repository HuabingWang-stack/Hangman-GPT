#!/usr/bin/env python3
"""
generate_guess.py - ä¸ºHangmanä»»åŠ¡ç”ŸæˆSFTè®­ç»ƒæ•°æ® (Softmaxæ¸©åº¦å¹³æ»‘ç‰ˆ + å‡åŒ€æ€»çŒœæµ‹æ¬¡æ•°åˆ†å¸ƒ + è¯é•¿å¢å¼º)

æ ¸å¿ƒæ”¹è¿›ï¼š
1. Scoreè®¡ç®—ï¼šä½¿ç”¨Softmaxæ¸©åº¦å¹³æ»‘çš„è¿ç»­ä¿¡æ¯å¢ç›Šï¼ˆè¿ç»­ä¸”å¯å¯¼ï¼‰
2. æ¸¸æˆçŠ¶æ€ï¼šæ€»çŒœæµ‹æ¬¡æ•°å‡åŒ€åˆ†å¸ƒï¼Œæ¨¡æ‹Ÿå®Œæ•´æ¸¸æˆè¿‡ç¨‹
3. å¯¹äºNä¸ªuniqueå­—ç¬¦çš„è¯ï¼Œæ€»çŒœæµ‹æ¬¡æ•°ä»0åˆ°N+5å‡åŒ€åˆ†å¸ƒ
4. ğŸ¯ æ–°å¢ï¼šè¯é•¿å¢å¼º - å¯é…ç½®ç‰¹å®šé•¿åº¦èŒƒå›´çš„è¯æ±‡åœ¨æ•°æ®ä¸­çš„æ¯”ä¾‹
5. ğŸ¯ æ–°å¢ï¼šå¯é…ç½®è¾“å…¥å’Œè¾“å‡ºç›®å½•
"""

import argparse
import json
import random
import os
import multiprocessing
from multiprocessing import Pool, Manager
from collections import Counter, defaultdict
from typing import List, Dict, Tuple, Set
import math
import time
from functools import partial
import numpy as np


class HangmanSoftLabelGenerator:
    def __init__(self, word_list: List[str], random_seed: int = 42, temperature: float = 1.0):
        self.word_list = word_list
        self.temperature = temperature  # ğŸ¯ Softmaxæ¸©åº¦å‚æ•°
        self.char_frequencies = self._calculate_global_frequencies()  # ä¿ç•™ç”¨äºåˆ†æ
        self.random_seed = random_seed
        random.seed(random_seed)

    def _calculate_global_frequencies(self) -> Dict[str, float]:
        """è®¡ç®—å­—ç¬¦åœ¨æ•´ä¸ªæ•°æ®é›†ä¸­çš„é¢‘ç‡ï¼ˆä¿ç•™ç”¨äºåˆ†æï¼‰"""
        char_count = Counter()
        total_chars = 0

        for word in self.word_list:
            for char in word:
                if 'a' <= char <= 'z':
                    char_count[char] += 1
                    total_chars += 1

        # è½¬æ¢ä¸ºé¢‘ç‡
        frequencies = {}
        for char in 'abcdefghijklmnopqrstuvwxyz':
            frequencies[char] = char_count.get(char, 0) / total_chars if total_chars > 0 else 0.0

        return frequencies

    def _get_word_state(self, word: str, guessed_chars: Set[str]) -> str:
        """æ ¹æ®å·²çŒœå­—ç¬¦ç”Ÿæˆå½“å‰è¯çŠ¶æ€"""
        state = ""
        for char in word:
            if char in guessed_chars:
                state += char
            else:
                state += "_"
        return state

    def _get_available_chars(self, guessed_chars: Set[str]) -> List[str]:
        """è·å–æ‰€æœ‰æœªçŒœè¿‡çš„å­—ç¬¦"""
        return [char for char in 'abcdefghijklmnopqrstuvwxyz'
                if char not in guessed_chars]

    def _get_unguessed_chars_in_word(self, word: str, guessed_chars: Set[str]) -> List[str]:
        """è·å–åœ¨è¯ä¸­ä½†æœªçŒœè¿‡çš„å­—ç¬¦ï¼ˆèƒ½å¸¦æ¥å¢é‡çš„å­—ç¬¦ï¼‰"""
        unguessed_in_word = []
        for char in set(word):
            if char not in guessed_chars:
                unguessed_in_word.append(char)
        return unguessed_in_word

    def _calculate_soft_targets(self, word: str, guessed_chars: Set[str]) -> Dict[str, float]:
        """
        ğŸ¯ ä½¿ç”¨Softmaxæ¸©åº¦å¹³æ»‘çš„è¿ç»­ä¿¡æ¯å¢ç›Šè®¡ç®—

        å°†ç¦»æ•£çš„å­—ç¬¦è®¡æ•°é€šè¿‡softmaxè½¬æ¢ä¸ºè¿ç»­ã€å¯å¯¼çš„æ¦‚ç‡åˆ†å¸ƒ
        Score(char) = softmax(count(char) / temperature)

        ç‰¹ç‚¹ï¼š
        - å®Œå…¨è¿ç»­ä¸”å¯å¯¼
        - ä¿æŒä¿¡æ¯å¢ç›Šçš„æ ¸å¿ƒé€»è¾‘ï¼šé«˜è®¡æ•°å­—ç¬¦ä»ç„¶è·å¾—æ›´é«˜æ¦‚ç‡
        - æ¸©åº¦å‚æ•°å¯è°ƒèŠ‚åˆ†å¸ƒçš„å¹³æ»‘ç¨‹åº¦
        """
        unguessed_chars_in_word = self._get_unguessed_chars_in_word(word, guessed_chars)

        if not unguessed_chars_in_word:
            return {}

        # è®¡ç®—å­—ç¬¦è®¡æ•°
        char_counts = {}
        for char in unguessed_chars_in_word:
            char_counts[char] = word.count(char)

        # è½¬æ¢ä¸ºsoftmaxåˆ†å¸ƒ
        chars = list(char_counts.keys())
        counts = [char_counts[char] for char in chars]

        # ğŸ¯ Softmaxå˜æ¢ï¼ˆè¿ç»­ä¸”å¯å¯¼ï¼‰
        max_count = max(counts)  # æ•°å€¼ç¨³å®šæ€§
        exp_values = [math.exp((count - max_count) / self.temperature) for count in counts]
        sum_exp = sum(exp_values)

        # æ„å»ºè½¯æ ‡ç­¾
        soft_targets = {}
        for char, exp_val in zip(chars, exp_values):
            soft_targets[char] = exp_val / sum_exp

        # ç²¾ç¡®èˆå…¥å¹¶ç¡®ä¿æ€»å’Œä¸º1
        for char in soft_targets:
            soft_targets[char] = round(soft_targets[char], 6)

        # è°ƒæ•´èˆå…¥è¯¯å·®
        current_sum = sum(soft_targets.values())
        if abs(current_sum - 1.0) > 1e-6:
            max_char = max(soft_targets.items(), key=lambda x: x[1])[0]
            soft_targets[max_char] = round(soft_targets[max_char] + (1.0 - current_sum), 6)

        return soft_targets

    def _create_balanced_game_state(self, word: str, unique_chars: List[str], all_chars: List[str],
                                    total_guesses: int) -> Tuple[Set[str], int]:
        """
        ğŸ¯ æ ¹æ®æ€»çŒœæµ‹æ¬¡æ•°åˆ›å»ºå¹³è¡¡çš„æ¸¸æˆçŠ¶æ€

        Args:
            total_guesses: æ€»çš„çŒœæµ‹æ¬¡æ•°ï¼ˆæ­£ç¡®+é”™è¯¯ï¼‰

        Returns:
            (guessed_chars, wrong_count)
        """
        n_unique = len(unique_chars)

        if total_guesses == 0:
            return (set(), 0)

        # ğŸ¯ å…³é”®çº¦æŸï¼šç¡®ä¿è‡³å°‘ç•™1ä¸ªæ­£ç¡®å­—ç¬¦æœªçŒœï¼ˆç”¨äºè®­ç»ƒï¼‰
        max_correct_guesses = min(total_guesses, n_unique - 1)  # è‡³å°‘ç•™1ä¸ª
        if max_correct_guesses < 0:  # å®‰å…¨æ£€æŸ¥
            max_correct_guesses = 0

        # ğŸ¯ åœ¨çº¦æŸä¸‹éšæœºåˆ†é…æ­£ç¡®/é”™è¯¯çŒœæµ‹
        if total_guesses <= max_correct_guesses:
            # æ€»æ•°å¾ˆå°ï¼Œå…¨éƒ¨åˆ†é…ç»™æ­£ç¡®çŒœæµ‹
            num_correct = total_guesses
            num_wrong = 0
        else:
            # éšæœºåˆ†é…æ­£ç¡®å’Œé”™è¯¯çŒœæµ‹
            # é”™è¯¯çŒœæµ‹æœ€å¤š5ä¸ª
            max_wrong = min(5, total_guesses)  # æœ€å¤š5ä¸ªé”™è¯¯

            # åœ¨å¯èƒ½çš„èŒƒå›´å†…éšæœºé€‰æ‹©æ­£ç¡®çŒœæµ‹æ•°
            min_correct = max(0, total_guesses - max_wrong)
            max_correct_allowed = min(max_correct_guesses, total_guesses)

            if min_correct <= max_correct_allowed:
                num_correct = random.randint(min_correct, max_correct_allowed)
            else:
                num_correct = max_correct_allowed

            num_wrong = total_guesses - num_correct
            # ç¡®ä¿é”™è¯¯æ¬¡æ•°ä¸è¶…è¿‡5
            if num_wrong > 5:
                num_wrong = 5
                num_correct = total_guesses - num_wrong

        # ğŸ¯ ç”Ÿæˆå…·ä½“çš„çŒœæµ‹å­—ç¬¦é›†åˆ
        guessed_chars = set()

        # æ·»åŠ æ­£ç¡®å­—ç¬¦
        if num_correct > 0:
            # æŒ‰ä¿¡æ¯å¢ç›Šæ’åºï¼Œä½†å¢åŠ éšæœºæ€§
            char_gains = [(char, word.count(char)) for char in unique_chars]
            char_gains.sort(key=lambda x: x[1], reverse=True)

            # ğŸ¯ æ›´å‡åŒ€çš„é€‰æ‹©ç­–ç•¥
            if num_correct <= len(char_gains):
                # æ–¹æ³•ï¼šåŠ æƒéšæœºé€‰æ‹©
                weights = []
                for i, (char, gain) in enumerate(char_gains):
                    # å‰é¢çš„å­—ç¬¦æƒé‡æ›´é«˜ï¼Œä½†ä¸æ˜¯ç»å¯¹çš„
                    weight = max(1.0, 3.0 - i * 0.3)  # é€’å‡æƒé‡
                    weights.append(weight)

                # åŠ æƒéšæœºé€‰æ‹©ï¼Œä¸æ”¾å›
                selected_chars = []
                available_chars = char_gains.copy()
                available_weights = weights.copy()

                for _ in range(num_correct):
                    if not available_chars:
                        break

                    # åŠ æƒéšæœºé€‰æ‹©
                    selected_idx = random.choices(
                        range(len(available_chars)),
                        weights=available_weights,
                        k=1
                    )[0]

                    selected_chars.append(available_chars[selected_idx][0])
                    # ç§»é™¤å·²é€‰æ‹©çš„å­—ç¬¦
                    available_chars.pop(selected_idx)
                    available_weights.pop(selected_idx)

                guessed_chars.update(selected_chars)

        # æ·»åŠ é”™è¯¯å­—ç¬¦
        actual_wrong_count = 0
        if num_wrong > 0:
            wrong_chars = [char for char in all_chars
                           if char not in word and char not in guessed_chars]
            random.shuffle(wrong_chars)

            actual_wrong_to_add = min(num_wrong, len(wrong_chars))
            for i in range(actual_wrong_to_add):
                guessed_chars.add(wrong_chars[i])
                actual_wrong_count += 1

        return (guessed_chars, actual_wrong_count)

    def _generate_game_states(self, word: str, max_samples: int) -> List[Tuple[Set[str], int]]:
        """
        ğŸ¯ é‡æ–°è®¾è®¡ï¼šç¡®ä¿æ€»çŒœæµ‹æ¬¡æ•°å‡åŒ€åˆ†å¸ƒï¼Œæ¨¡æ‹Ÿå®Œæ•´æ¸¸æˆè¿‡ç¨‹

        å¯¹äºæœ‰Nä¸ªuniqueå­—ç¬¦çš„è¯ï¼š
        - æœ€å¤§æ€»çŒœæµ‹æ¬¡æ•° = N + 5 (Nä¸ªæ­£ç¡® + æœ€å¤š5ä¸ªé”™è¯¯)
        - æ€»çŒœæµ‹æ¬¡æ•°ä» 0 åˆ° max_total_guesses å‡åŒ€åˆ†å¸ƒ
        - åœ¨æ¯ä¸ªæ€»æ•°ä¸‹ï¼Œéšæœºåˆ†é…æ­£ç¡®/é”™è¯¯æ¯”ä¾‹
        - ç¡®ä¿è‡³å°‘ç•™1ä¸ªæ­£ç¡®å­—ç¬¦æœªçŒœï¼ˆç”¨äºè®­ç»ƒï¼‰
        """
        unique_chars = list(set(word))
        all_chars = list('abcdefghijklmnopqrstuvwxyz')
        n_unique = len(unique_chars)

        # ğŸ¯ å…³é”®ï¼šè®¡ç®—æ¸¸æˆçš„æ€»çŒœæµ‹æ¬¡æ•°èŒƒå›´
        max_total_guesses = n_unique + 5  # æœ€å¤šï¼šæ‰€æœ‰æ­£ç¡®å­—ç¬¦ + 5ä¸ªé”™è¯¯

        if max_samples == 1:
            # ğŸ¯ å•æ ·æœ¬ï¼šä»å®Œæ•´èŒƒå›´éšæœºé€‰æ‹©
            total_guesses = random.randint(0, max_total_guesses)
            state = self._create_balanced_game_state(word, unique_chars, all_chars, total_guesses)
            return [state]

        else:
            # ğŸ¯ å¤šæ ·æœ¬ï¼šç¡®ä¿å‡åŒ€è¦†ç›–æ•´ä¸ªèŒƒå›´
            states = []

            # è®¡ç®—æ¯ä¸ªæ€»çŒœæµ‹æ¬¡æ•°åº”è¯¥ç”Ÿæˆå¤šå°‘ä¸ªæ ·æœ¬
            total_guesses_range = list(range(0, max_total_guesses + 1))

            if max_samples <= len(total_guesses_range):
                # æ ·æœ¬æ•°ä¸å¤Ÿè¦†ç›–æ‰€æœ‰æ€»æ•°ï¼Œå‡åŒ€é‡‡æ ·
                sampled_totals = random.sample(total_guesses_range, max_samples)
            else:
                # æ ·æœ¬æ•°å……è¶³ï¼Œé‡å¤æŸäº›æ€»æ•°
                sampled_totals = []
                samples_per_total = max_samples // len(total_guesses_range)
                remainder = max_samples % len(total_guesses_range)

                for total in total_guesses_range:
                    # æ¯ä¸ªæ€»æ•°è‡³å°‘ç”Ÿæˆsamples_per_totalä¸ªæ ·æœ¬
                    sampled_totals.extend([total] * samples_per_total)

                # å‰©ä½™çš„æ ·æœ¬éšæœºåˆ†é…
                extra_totals = random.choices(total_guesses_range, k=remainder)
                sampled_totals.extend(extra_totals)

            # ç”Ÿæˆæ¯ä¸ªæ€»çŒœæµ‹æ¬¡æ•°å¯¹åº”çš„çŠ¶æ€
            for total_guesses in sampled_totals:
                state = self._create_balanced_game_state(word, unique_chars, all_chars, total_guesses)
                states.append(state)

            return states

    def generate_samples(self, word: str, max_samples: int) -> List[Dict]:
        """ä¸ºå•ä¸ªè¯ç”Ÿæˆè®­ç»ƒæ ·æœ¬"""
        samples = []
        game_states = self._generate_game_states(word, max_samples)

        for guessed_chars, wrong_count in game_states:
            # è·³è¿‡å·²å®Œæˆæˆ–å¤±è´¥çš„çŠ¶æ€
            if all(char in guessed_chars for char in word) or wrong_count >= 6:
                continue

            # è·å–èƒ½å¸¦æ¥å¢é‡çš„å­—ç¬¦ï¼ˆåœ¨è¯ä¸­ä¸”æœªçŒœè¿‡ï¼‰
            unguessed_chars_in_word = self._get_unguessed_chars_in_word(word, guessed_chars)
            if not unguessed_chars_in_word:
                continue  # æ²¡æœ‰èƒ½å¸¦æ¥å¢é‡çš„å­—ç¬¦

            # è®¡ç®—è½¯æ ‡ç­¾ï¼ˆåªåŒ…å«èƒ½å¸¦æ¥å¢é‡çš„å­—ç¬¦ï¼‰
            soft_targets = self._calculate_soft_targets(word, guessed_chars)
            if not soft_targets:
                continue

            # éªŒè¯soft_targetsåªåŒ…å«åœ¨è¯ä¸­çš„æœªçŒœå­—ç¬¦
            for char in soft_targets:
                assert char in word, f"Character '{char}' not in word '{word}'"
                assert char not in guessed_chars, f"Character '{char}' already guessed"

            # éªŒè¯æ‰€æœ‰åœ¨è¯ä¸­çš„æœªçŒœå­—ç¬¦éƒ½åœ¨soft_targetsä¸­
            assert set(soft_targets.keys()) == set(unguessed_chars_in_word), \
                f"Soft targets {set(soft_targets.keys())} != unguessed in word {set(unguessed_chars_in_word)}"

            # é€‰æ‹©æœ€é«˜åˆ†å­—ç¬¦ä½œä¸ºcompletion
            best_char = max(soft_targets.items(), key=lambda x: x[1])[0]

            # éªŒè¯soft_targetsæ€»å’Œä¸º1
            soft_sum = sum(soft_targets.values())
            assert abs(soft_sum - 1.0) < 0.01, f"Soft targets sum {soft_sum} != 1.0"

            # ç”Ÿæˆå½“å‰çŠ¶æ€
            word_state = self._get_word_state(word, guessed_chars)
            guessed_list = sorted(list(guessed_chars))
            guess_progress = f"{wrong_count}/6"

            prompt_parts = [
                word_state,
                ','.join(guessed_list) if guessed_list else "",
                guess_progress
            ]

            sample = {
                "guess": {
                    "prompt": "[SEP]".join(prompt_parts),
                    "completion": best_char
                },
                "label": word,
                "soft_targets": soft_targets,
                "metadata": {
                    "num_candidates": len(soft_targets),
                    "best_score": soft_targets[best_char],
                    "word_length": len(word),
                    "unique_chars": len(set(word)),
                    "guessed_chars": sorted(list(guessed_chars)),
                    "total_guesses": len(guessed_chars),
                    "unguessed_in_word": len(unguessed_chars_in_word),
                    "wrong_count": wrong_count,
                    "revealed_positions": len([c for c in word_state if c != '_']),
                    "completion_in_word": True,
                    "soft_targets_sum": round(sum(soft_targets.values()), 6),
                    "temperature": self.temperature,
                    # ğŸ¯ æ›´æ–°ï¼šæ˜¾ç¤ºSoftmaxåˆ†æ•°è®¡ç®—è¯¦æƒ…
                    "score_details": {
                        char: {
                            "count_in_word": word.count(char),
                            "softmax_logit": word.count(char) / self.temperature,
                            "final_score": soft_targets[char],
                            "calculation": f"softmax({word.count(char)}/{self.temperature})"
                        }
                        for char in soft_targets
                    }
                }
            }

            samples.append(sample)

        return samples


def analyze_word_length_distribution(word_list: List[str]) -> Dict[int, int]:
    """åˆ†æè¯æ±‡é•¿åº¦åˆ†å¸ƒ"""
    length_dist = {}
    for word in word_list:
        length = len(word)
        length_dist[length] = length_dist.get(length, 0) + 1
    return length_dist


def resample_words_by_length(word_list: List[str],
                             enhance_min_length: int = 2,
                             enhance_max_length: int = 12,
                             enhance_ratio: float = 0.7) -> List[str]:
    """
    ğŸ¯ æŒ‰è¯é•¿é‡æ–°é‡‡æ ·ï¼Œå¢åŠ æŒ‡å®šé•¿åº¦èŒƒå›´å†…è¯æ±‡çš„æ¯”ä¾‹

    Args:
        word_list: åŸå§‹è¯æ±‡åˆ—è¡¨
        enhance_min_length: å¢å¼ºè¯æ±‡çš„æœ€å°é•¿åº¦
        enhance_max_length: å¢å¼ºè¯æ±‡çš„æœ€å¤§é•¿åº¦
        enhance_ratio: å¢å¼ºè¯æ±‡åœ¨æœ€ç»ˆæ•°æ®ä¸­çš„ç›®æ ‡æ¯”ä¾‹ (0.0-1.0)

    Returns:
        é‡é‡‡æ ·åçš„è¯æ±‡åˆ—è¡¨
    """

    # æŒ‰é•¿åº¦åˆ†ç»„
    enhanced_words = []  # ç›®æ ‡å¢å¼ºçš„è¯æ±‡
    other_words = []  # å…¶ä»–è¯æ±‡

    for word in word_list:
        word_length = len(word)
        if enhance_min_length <= word_length <= enhance_max_length:
            enhanced_words.append(word)
        else:
            other_words.append(word)

    print(f"\nğŸ¯ è¯é•¿å¢å¼ºé‡é‡‡æ ·:")
    print(f"   å¢å¼ºé•¿åº¦èŒƒå›´: {enhance_min_length}-{enhance_max_length}")
    print(f"   ç›®æ ‡æ¯”ä¾‹: {enhance_ratio:.1%}")
    print(f"   åŸå§‹åˆ†å¸ƒ:")
    print(f"     å¢å¼ºèŒƒå›´è¯æ±‡: {len(enhanced_words):,} ({len(enhanced_words) / len(word_list) * 100:.1f}%)")
    print(f"     å…¶ä»–è¯æ±‡: {len(other_words):,} ({len(other_words) / len(word_list) * 100:.1f}%)")

    if len(enhanced_words) == 0:
        print(f"   âš ï¸  æ²¡æœ‰æ‰¾åˆ°é•¿åº¦åœ¨ {enhance_min_length}-{enhance_max_length} èŒƒå›´å†…çš„è¯æ±‡")
        return word_list

    if len(other_words) == 0:
        print(f"   â„¹ï¸  æ‰€æœ‰è¯æ±‡éƒ½åœ¨å¢å¼ºèŒƒå›´å†…ï¼Œæ— éœ€é‡é‡‡æ ·")
        return word_list

    # è®¡ç®—ç›®æ ‡æ ·æœ¬æ•°
    original_total = len(word_list)
    target_enhanced_count = int(original_total * enhance_ratio)
    target_other_count = original_total - target_enhanced_count

    # è®¡ç®—é‡é‡‡æ ·å€æ•°
    enhanced_multiplier = target_enhanced_count / len(enhanced_words)
    other_multiplier = target_other_count / len(other_words)

    print(f"   é‡é‡‡æ ·ç­–ç•¥:")
    print(f"     å¢å¼ºè¯æ±‡é‡‡æ ·å€æ•°: {enhanced_multiplier:.2f}x")
    print(f"     å…¶ä»–è¯æ±‡é‡‡æ ·å€æ•°: {other_multiplier:.2f}x")

    # æ‰§è¡Œé‡é‡‡æ ·
    resampled_words = []

    # é‡é‡‡æ ·å¢å¼ºè¯æ±‡
    if enhanced_multiplier >= 1.0:
        # éœ€è¦å¢åŠ é‡‡æ ·
        base_repeats = int(enhanced_multiplier)
        extra_probability = enhanced_multiplier - base_repeats

        for word in enhanced_words:
            # åŸºç¡€é‡å¤
            resampled_words.extend([word] * base_repeats)
            # é¢å¤–é‡å¤ï¼ˆæ¦‚ç‡æ€§ï¼‰
            if random.random() < extra_probability:
                resampled_words.append(word)
    else:
        # éœ€è¦å‡å°‘é‡‡æ ·
        num_to_sample = target_enhanced_count
        resampled_enhanced = random.sample(enhanced_words, min(num_to_sample, len(enhanced_words)))
        resampled_words.extend(resampled_enhanced)

    # é‡é‡‡æ ·å…¶ä»–è¯æ±‡
    if other_multiplier >= 1.0:
        # éœ€è¦å¢åŠ é‡‡æ ·
        base_repeats = int(other_multiplier)
        extra_probability = other_multiplier - base_repeats

        for word in other_words:
            # åŸºç¡€é‡å¤
            resampled_words.extend([word] * base_repeats)
            # é¢å¤–é‡å¤ï¼ˆæ¦‚ç‡æ€§ï¼‰
            if random.random() < extra_probability:
                resampled_words.append(word)
    else:
        # éœ€è¦å‡å°‘é‡‡æ ·
        num_to_sample = target_other_count
        resampled_other = random.sample(other_words, min(num_to_sample, len(other_words)))
        resampled_words.extend(resampled_other)

    # éšæœºæ‰“ä¹±
    random.shuffle(resampled_words)

    # ç»Ÿè®¡æœ€ç»ˆåˆ†å¸ƒ
    final_enhanced = sum(1 for word in resampled_words
                         if enhance_min_length <= len(word) <= enhance_max_length)
    final_other = len(resampled_words) - final_enhanced

    print(f"   æœ€ç»ˆåˆ†å¸ƒ:")
    print(f"     å¢å¼ºèŒƒå›´è¯æ±‡: {final_enhanced:,} ({final_enhanced / len(resampled_words) * 100:.1f}%)")
    print(f"     å…¶ä»–è¯æ±‡: {final_other:,} ({final_other / len(resampled_words) * 100:.1f}%)")
    print(f"     æ€»è¯æ±‡æ•°: {len(resampled_words):,}")

    # éªŒè¯æ˜¯å¦è¾¾åˆ°ç›®æ ‡
    actual_ratio = final_enhanced / len(resampled_words) if len(resampled_words) > 0 else 0
    if abs(actual_ratio - enhance_ratio) < 0.05:  # å…è®¸5%çš„è¯¯å·®
        print(f"   âœ… æˆåŠŸè¾¾åˆ°ç›®æ ‡æ¯”ä¾‹: {actual_ratio:.1%} â‰ˆ {enhance_ratio:.1%}")
    else:
        print(f"   âš ï¸  ä¸ç›®æ ‡æ¯”ä¾‹æœ‰åå·®: {actual_ratio:.1%} vs {enhance_ratio:.1%}")

    return resampled_words


def analyze_length_distribution_comparison(original_words: List[str],
                                           resampled_words: List[str],
                                           enhance_min_length: int,
                                           enhance_max_length: int):
    """å¯¹æ¯”é‡é‡‡æ ·å‰åçš„é•¿åº¦åˆ†å¸ƒ"""

    print(f"\nğŸ“Š è¯æ±‡é•¿åº¦åˆ†å¸ƒå¯¹æ¯”:")

    # è®¡ç®—åˆ†å¸ƒ
    original_dist = analyze_word_length_distribution(original_words)
    resampled_dist = analyze_word_length_distribution(resampled_words)

    # åˆå¹¶æ‰€æœ‰é•¿åº¦
    all_lengths = sorted(set(original_dist.keys()) | set(resampled_dist.keys()))

    print(f"{'é•¿åº¦':<4} {'åŸå§‹æ•°é‡':<8} {'åŸå§‹æ¯”ä¾‹':<8} {'é‡é‡‡æ ·æ•°é‡':<10} {'é‡é‡‡æ ·æ¯”ä¾‹':<10} {'å˜åŒ–':<8}")
    print("-" * 60)

    for length in all_lengths:
        orig_count = original_dist.get(length, 0)
        resamp_count = resampled_dist.get(length, 0)

        orig_ratio = orig_count / len(original_words) * 100 if len(original_words) > 0 else 0
        resamp_ratio = resamp_count / len(resampled_words) * 100 if len(resampled_words) > 0 else 0

        # åˆ¤æ–­æ˜¯å¦åœ¨å¢å¼ºèŒƒå›´å†…
        is_enhanced = enhance_min_length <= length <= enhance_max_length
        change_indicator = "ğŸ“ˆ" if is_enhanced and resamp_ratio > orig_ratio else "ğŸ“‰" if not is_enhanced and resamp_ratio < orig_ratio else "â¡ï¸"

        print(
            f"{length:<4} {orig_count:<8} {orig_ratio:<7.1f}% {resamp_count:<10} {resamp_ratio:<9.1f}% {change_indicator}")

    # æ±‡æ€»ç»Ÿè®¡
    orig_enhanced = sum(original_dist.get(l, 0) for l in range(enhance_min_length, enhance_max_length + 1))
    resamp_enhanced = sum(resampled_dist.get(l, 0) for l in range(enhance_min_length, enhance_max_length + 1))

    print(f"\nğŸ“ˆ æ±‡æ€»å¯¹æ¯”:")
    print(f"å¢å¼ºèŒƒå›´({enhance_min_length}-{enhance_max_length}):")
    print(f"  åŸå§‹: {orig_enhanced:,} ({orig_enhanced / len(original_words) * 100:.1f}%)")
    print(f"  é‡é‡‡æ ·: {resamp_enhanced:,} ({resamp_enhanced / len(resampled_words) * 100:.1f}%)")
    print(
        f"  å¢é•¿: {resamp_enhanced - orig_enhanced:+,} ({(resamp_enhanced / len(resampled_words) - orig_enhanced / len(original_words)) * 100:+.1f}%)")


def analyze_total_guesses_distribution(samples: List[Dict]):
    """ğŸ¯ åˆ†ææ€»çŒœæµ‹æ¬¡æ•°åˆ†å¸ƒ"""
    print(f"\n=== æ€»çŒœæµ‹æ¬¡æ•°åˆ†å¸ƒåˆ†æ ===")

    total_guesses_data = []

    for sample in samples:
        # è§£ææ¸¸æˆçŠ¶æ€
        prompt = sample["guess"]["prompt"]
        parts = prompt.split("[SEP]")

        if len(parts) >= 2 and parts[1]:
            guessed_chars = set(parts[1].split(','))
        else:
            guessed_chars = set()

        word = sample["label"]
        unique_chars = len(set(word))

        # è®¡ç®—æ€»çŒœæµ‹æ¬¡æ•°
        total_guesses = len(guessed_chars)

        # è®¡ç®—ç†è®ºæœ€å¤§å€¼
        theoretical_max = unique_chars + 5

        total_guesses_data.append({
            'total_guesses': total_guesses,
            'unique_chars': unique_chars,
            'theoretical_max': theoretical_max,
            'progress_ratio': total_guesses / theoretical_max if theoretical_max > 0 else 0
        })

    # ğŸ¯ æŒ‰unique_charsåˆ†ç»„åˆ†æ
    by_unique_chars = defaultdict(list)

    for data in total_guesses_data:
        by_unique_chars[data['unique_chars']].append(data['total_guesses'])

    print("ğŸ“Š æŒ‰è¯æ±‡uniqueå­—ç¬¦æ•°åˆ†ç»„çš„æ€»çŒœæµ‹æ¬¡æ•°åˆ†å¸ƒ:")
    for unique_count in sorted(by_unique_chars.keys()):
        guesses_list = by_unique_chars[unique_count]
        theoretical_max = unique_count + 5

        print(f"\n  {unique_count}ä¸ªuniqueå­—ç¬¦çš„è¯ (ç†è®ºæœ€å¤§çŒœæµ‹æ¬¡æ•°: {theoretical_max}):")
        print(f"    æ ·æœ¬æ•°: {len(guesses_list)}")
        print(f"    æ€»çŒœæµ‹æ¬¡æ•°èŒƒå›´: {min(guesses_list)} - {max(guesses_list)}")
        print(f"    å¹³å‡: {sum(guesses_list) / len(guesses_list):.1f}")

        # åˆ†å¸ƒç»Ÿè®¡
        guesses_dist = Counter(guesses_list)
        print(f"    åˆ†å¸ƒ: ", end="")
        for guesses in sorted(guesses_dist.keys()):
            count = guesses_dist[guesses]
            percentage = count / len(guesses_list) * 100
            print(f"{guesses}æ¬¡({count}, {percentage:.1f}%) ", end="")
        print()

        # æ£€æŸ¥æ˜¯å¦è¦†ç›–äº†å®Œæ•´èŒƒå›´
        expected_range = set(range(0, theoretical_max + 1))
        actual_range = set(guesses_list)
        missing = expected_range - actual_range

        if missing:
            print(f"    âš ï¸  ç¼ºå¤±çš„çŒœæµ‹æ¬¡æ•°: {sorted(missing)}")
        else:
            print(f"    âœ… å®Œæ•´è¦†ç›– 0-{theoretical_max} çš„æ‰€æœ‰çŒœæµ‹æ¬¡æ•°")

    # ğŸ¯ æ•´ä½“åˆ†å¸ƒåˆ†æ
    all_total_guesses = [data['total_guesses'] for data in total_guesses_data]
    all_progress_ratios = [data['progress_ratio'] for data in total_guesses_data]

    print(f"\nğŸ“Š æ•´ä½“ç»Ÿè®¡:")
    print(f"  æ€»æ ·æœ¬æ•°: {len(total_guesses_data)}")
    print(f"  æ€»çŒœæµ‹æ¬¡æ•°èŒƒå›´: {min(all_total_guesses)} - {max(all_total_guesses)}")
    print(f"  å¹³å‡çŒœæµ‹æ¬¡æ•°: {sum(all_total_guesses) / len(all_total_guesses):.1f}")

    # è¿›åº¦æ¯”ä¾‹åˆ†æ
    progress_ranges = {
        "0-20%": sum(1 for r in all_progress_ratios if 0 <= r < 0.2),
        "20-40%": sum(1 for r in all_progress_ratios if 0.2 <= r < 0.4),
        "40-60%": sum(1 for r in all_progress_ratios if 0.4 <= r < 0.6),
        "60-80%": sum(1 for r in all_progress_ratios if 0.6 <= r < 0.8),
        "80-100%": sum(1 for r in all_progress_ratios if 0.8 <= r <= 1.0)
    }

    print(f"\nğŸ“Š æ¸¸æˆè¿›åº¦åˆ†å¸ƒ (æ€»çŒœæµ‹æ¬¡æ•°/ç†è®ºæœ€å¤§å€¼):")
    for range_name, count in progress_ranges.items():
        percentage = count / len(total_guesses_data) * 100
        print(f"  {range_name}: {count:5d} æ ·æœ¬ ({percentage:5.1f}%)")


def analyze_game_state_distribution(samples: List[Dict]):
    """ğŸ¯ åˆ†ææ¸¸æˆçŠ¶æ€åˆ†å¸ƒ - ç¡®ä¿è¦†ç›–å…¨è°±"""
    print(f"\n=== æ¸¸æˆçŠ¶æ€åˆ†å¸ƒåˆ†æ ===")

    # åˆ†ææ­ç¤ºè¿›åº¦åˆ†å¸ƒ
    reveal_ratios = []
    wrong_counts = []

    for sample in samples:
        # è§£ææ¸¸æˆçŠ¶æ€
        prompt = sample["guess"]["prompt"]
        parts = prompt.split("[SEP]")

        if len(parts) >= 2 and parts[1]:
            guessed_chars = set(parts[1].split(','))
        else:
            guessed_chars = set()

        if len(parts) >= 3:
            wrong_count = int(parts[2].split('/')[0])
        else:
            wrong_count = 0

        word = sample["label"]
        unique_chars = len(set(word))

        # è®¡ç®—æ­ç¤ºçš„æ­£ç¡®å­—ç¬¦æ•°
        correct_chars = len([c for c in guessed_chars if c in word])
        reveal_ratio = correct_chars / unique_chars if unique_chars > 0 else 0

        reveal_ratios.append(reveal_ratio)
        wrong_counts.append(wrong_count)

    # ğŸ¯ æ­ç¤ºè¿›åº¦åˆ†å¸ƒ
    reveal_ranges = {
        "0-10%": sum(1 for r in reveal_ratios if 0 <= r < 0.1),
        "10-30%": sum(1 for r in reveal_ratios if 0.1 <= r < 0.3),
        "30-50%": sum(1 for r in reveal_ratios if 0.3 <= r < 0.5),
        "50-70%": sum(1 for r in reveal_ratios if 0.5 <= r < 0.7),
        "70-90%": sum(1 for r in reveal_ratios if 0.7 <= r < 0.9),
        "90%+": sum(1 for r in reveal_ratios if r >= 0.9)
    }

    print(f"ğŸ“Š æ­ç¤ºè¿›åº¦åˆ†å¸ƒ:")
    for range_name, count in reveal_ranges.items():
        percentage = count / len(samples) * 100
        print(f"  {range_name}: {count:5d} æ ·æœ¬ ({percentage:5.1f}%)")

    # ğŸ¯ é”™è¯¯æ¬¡æ•°åˆ†å¸ƒ
    wrong_dist = Counter(wrong_counts)
    print(f"\nğŸ“Š é”™è¯¯æ¬¡æ•°åˆ†å¸ƒ:")
    for wrong in range(6):
        count = wrong_dist.get(wrong, 0)
        percentage = count / len(samples) * 100
        print(f"  {wrong}é”™è¯¯: {count:5d} æ ·æœ¬ ({percentage:5.1f}%)")

    # ğŸ¯ çŠ¶æ€å¤šæ ·æ€§
    print(f"\nğŸ“Š çŠ¶æ€å¤šæ ·æ€§:")
    state_combinations = set()
    for r, w in zip(reveal_ratios, wrong_counts):
        reveal_bucket = int(r * 10) * 10  # 0, 10, 20, ..., 90
        state_combinations.add((reveal_bucket, w))

    print(f"  ä¸åŒçŠ¶æ€ç»„åˆæ•°: {len(state_combinations)}")
    print(f"  å¹³å‡æ¯ä¸ªç»„åˆ: {len(samples) / len(state_combinations):.1f} æ ·æœ¬")

    # æ£€æŸ¥è¦†ç›–åº¦
    missing_early = reveal_ranges["0-10%"] == 0
    missing_late = reveal_ranges["70-90%"] + reveal_ranges["90%+"] == 0
    missing_no_errors = wrong_dist.get(0, 0) == 0
    missing_high_errors = wrong_dist.get(4, 0) + wrong_dist.get(5, 0) == 0

    if missing_early or missing_late or missing_no_errors or missing_high_errors:
        print(f"\nâš ï¸  è¦†ç›–åº¦ä¸è¶³:")
        if missing_early: print(f"  ç¼ºå°‘æ—©æœŸçŠ¶æ€ (0-10%)")
        if missing_late: print(f"  ç¼ºå°‘åæœŸçŠ¶æ€ (70%+)")
        if missing_no_errors: print(f"  ç¼ºå°‘æ— é”™è¯¯çŠ¶æ€")
        if missing_high_errors: print(f"  ç¼ºå°‘é«˜é”™è¯¯çŠ¶æ€ (4-5é”™)")
    else:
        print(f"\nâœ… çŠ¶æ€è¦†ç›–åº¦è‰¯å¥½:")
        print(f"  âœ“ æ—©æœŸåˆ°åæœŸçŠ¶æ€å®Œæ•´")
        print(f"  âœ“ 0-5é”™è¯¯æ¬¡æ•°å®Œæ•´")
        print(f"  âœ“ å¤šæ ·åŒ–çŠ¶æ€ç»„åˆ")


def process_word_batch(args_tuple):
    """å¤„ç†ä¸€æ‰¹è¯æ±‡çš„å‡½æ•°ï¼Œç”¨äºå¤šè¿›ç¨‹"""
    word_batch, char_frequencies, max_samples, process_id, random_seed, temperature = args_tuple

    # ä¸ºæ¯ä¸ªè¿›ç¨‹è®¾ç½®ä¸åŒçš„éšæœºç§å­
    process_seed = random_seed + process_id * 1000
    random.seed(process_seed)

    # åˆ›å»ºç”Ÿæˆå™¨å®ä¾‹
    generator = HangmanSoftLabelGenerator(word_batch, process_seed, temperature)
    generator.char_frequencies = char_frequencies  # ä½¿ç”¨å…¨å±€é¢‘ç‡ç»Ÿè®¡

    all_samples = []
    error_count = 0

    for i, word in enumerate(word_batch):
        try:
            word_samples = generator.generate_samples(word, max_samples)
            all_samples.extend(word_samples)
        except Exception as e:
            error_count += 1
            continue

    return all_samples, error_count, process_id, len(word_batch)


def split_list(lst: List, n_chunks: int) -> List[List]:
    """å°†åˆ—è¡¨åˆ†å‰²ä¸ºnä¸ªè¿‘ä¼¼ç›¸ç­‰çš„å—"""
    chunk_size = len(lst) // n_chunks
    remainder = len(lst) % n_chunks

    chunks = []
    start = 0

    for i in range(n_chunks):
        # å‰remainderä¸ªå—å¤šåˆ†é…ä¸€ä¸ªå…ƒç´ 
        current_chunk_size = chunk_size + (1 if i < remainder else 0)
        end = start + current_chunk_size
        chunks.append(lst[start:end])
        start = end

    return chunks


def generate_dataset_multiprocess(word_list: List[str], char_frequencies: Dict[str, float],
                                  max_samples: int, max_processes: int, random_seed: int = 42,
                                  temperature: float = 1.0) -> List[Dict]:
    """ä½¿ç”¨å¤šè¿›ç¨‹ç”Ÿæˆæ•°æ®é›†"""

    if len(word_list) == 0:
        return []

    # ç¡®å®šå®é™…ä½¿ç”¨çš„è¿›ç¨‹æ•°
    actual_processes = min(max_processes, len(word_list), multiprocessing.cpu_count())
    print(f"ä½¿ç”¨ {actual_processes} ä¸ªè¿›ç¨‹å¤„ç† {len(word_list)} ä¸ªè¯æ±‡")

    # åˆ†å‰²è¯æ±‡åˆ—è¡¨
    word_batches = split_list(word_list, actual_processes)

    # å‡†å¤‡å‚æ•°
    args_list = []
    for i, batch in enumerate(word_batches):
        if batch:  # ç¡®ä¿æ‰¹æ¬¡ä¸ä¸ºç©º
            args_list.append((batch, char_frequencies, max_samples, i, random_seed, temperature))

    # å¤šè¿›ç¨‹å¤„ç†
    print("å¼€å§‹å¤šè¿›ç¨‹æ•°æ®ç”Ÿæˆ...")
    start_time = time.time()

    all_samples = []
    total_errors = 0

    with Pool(processes=actual_processes) as pool:
        # ä½¿ç”¨map_asyncæ¥è·å–è¿›åº¦
        result = pool.map_async(process_word_batch, args_list)

        # è·å–ç»“æœ
        results = result.get()

    # åˆå¹¶ç»“æœ
    print("åˆå¹¶ç»“æœ...")
    for samples, error_count, process_id, batch_size in results:
        all_samples.extend(samples)
        total_errors += error_count
        print(f"è¿›ç¨‹ {process_id}: å¤„ç† {batch_size} ä¸ªè¯æ±‡, ç”Ÿæˆ {len(samples)} ä¸ªæ ·æœ¬, é”™è¯¯ {error_count} ä¸ª")

    end_time = time.time()
    print(f"å¤šè¿›ç¨‹ç”Ÿæˆå®Œæˆ! è€—æ—¶: {end_time - start_time:.2f}s")
    print(f"æ€»æ ·æœ¬æ•°: {len(all_samples)}, æ€»é”™è¯¯æ•°: {total_errors}")

    # Shuffleæ‰€æœ‰ç»“æœ
    print("Shufflingæ•°æ®...")
    random.seed(random_seed)
    random.shuffle(all_samples)

    return all_samples


def load_words(file_path: str) -> List[str]:
    """åŠ è½½è¯æ±‡åˆ—è¡¨"""
    words = []
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                word = line.strip().lower()
                if word and word.isalpha():
                    words.append(word)
    except FileNotFoundError:
        print(f"æ–‡ä»¶æœªæ‰¾åˆ°: {file_path}")
        return []
    except Exception as e:
        print(f"åŠ è½½æ–‡ä»¶å‡ºé”™ {file_path}: {e}")
        return []

    print(f"æˆåŠŸåŠ è½½ {len(words)} ä¸ªè¯æ±‡ä» {file_path}")
    return words


def calculate_global_frequencies(word_list: List[str]) -> Dict[str, float]:
    """è®¡ç®—å…¨å±€å­—ç¬¦é¢‘ç‡"""
    char_count = Counter()
    total_chars = 0

    for word in word_list:
        for char in word:
            if 'a' <= char <= 'z':
                char_count[char] += 1
                total_chars += 1

    frequencies = {}
    for char in 'abcdefghijklmnopqrstuvwxyz':
        frequencies[char] = char_count.get(char, 0) / total_chars if total_chars > 0 else 0.0

    return frequencies


def save_samples(samples: List[Dict], output_path: str, include_metadata: bool = False):
    """ä¿å­˜æ ·æœ¬åˆ°æ–‡ä»¶"""
    os.makedirs(os.path.dirname(output_path), exist_ok=True)

    print(f"ä¿å­˜ {len(samples)} ä¸ªæ ·æœ¬åˆ° {output_path}")

    with open(output_path, 'w', encoding='utf-8') as f:
        for sample in samples:
            if include_metadata:
                # åŒ…å«å®Œæ•´ä¿¡æ¯çš„æ–‡ä»¶
                f.write(json.dumps(sample, ensure_ascii=False) + '\n')
            else:
                # SFTè®­ç»ƒæ–‡ä»¶ï¼šåªåŒ…å«å¿…è¦å­—æ®µ
                clean_sample = {
                    "guess": sample["guess"],
                    "label": sample["label"],
                    "soft_targets": sample["soft_targets"]
                }
                f.write(json.dumps(clean_sample, ensure_ascii=False) + '\n')


def validate_dataset(samples: List[Dict], sample_size: int = 100):
    """éªŒè¯æ•°æ®é›†æ ¼å¼å’Œé€»è¾‘"""
    print(f"\n=== æ•°æ®é›†éªŒè¯ (æ£€æŸ¥{min(sample_size, len(samples))}ä¸ªæ ·æœ¬) ===")

    format_errors = 0
    logic_errors = 0
    soft_target_errors = 0
    sum_errors = 0
    incremental_errors = 0
    completion_errors = 0

    check_samples = samples[:sample_size] if len(samples) > sample_size else samples

    for i, sample in enumerate(check_samples):
        # æ£€æŸ¥åŸºæœ¬æ ¼å¼
        if not all(key in sample for key in ["guess", "label", "soft_targets"]):
            format_errors += 1
            continue

        if not all(key in sample["guess"] for key in ["prompt", "completion"]):
            format_errors += 1
            continue

        prompt = sample["guess"]["prompt"]
        completion = sample["guess"]["completion"]
        soft_targets = sample["soft_targets"]
        word = sample["label"]

        # è§£æå·²çŒœå­—ç¬¦
        parts = prompt.split("[SEP]")
        if len(parts) >= 2 and parts[1]:
            guessed_chars = set(parts[1].split(','))
        else:
            guessed_chars = set()

        # æ£€æŸ¥completionæ˜¯å¦åœ¨soft_targetsä¸­
        if completion not in soft_targets:
            logic_errors += 1
            if logic_errors <= 3:
                print(f"é€»è¾‘é”™è¯¯ {i}: completion '{completion}' ä¸åœ¨ soft_targets {list(soft_targets.keys())}")

        # æ£€æŸ¥completionæ˜¯å¦é‡å¤çŒœæµ‹
        if completion in guessed_chars:
            logic_errors += 1
            if logic_errors <= 3:
                print(f"é‡å¤çŒœæµ‹é”™è¯¯ {i}: completion '{completion}' å·²åœ¨ guessed_chars {guessed_chars}")

        # æ£€æŸ¥completionæ˜¯å¦æ˜¯æœ€é«˜åˆ†
        if soft_targets:
            max_score_char = max(soft_targets.items(), key=lambda x: x[1])[0]
            if completion != max_score_char:
                completion_errors += 1
                if completion_errors <= 3:
                    print(
                        f"completioné”™è¯¯ {i}: completion '{completion}' (score={soft_targets.get(completion, 0):.3f}) != æœ€é«˜åˆ†å­—ç¬¦ '{max_score_char}' (score={soft_targets[max_score_char]:.3f})")

        # æ£€æŸ¥soft_targetsä¸­çš„å­—ç¬¦æ˜¯å¦éƒ½åœ¨è¯ä¸­ä¸”æœªçŒœè¿‡
        unguessed_in_word = set(word) - guessed_chars

        # æ£€æŸ¥soft_targetsæ˜¯å¦åªåŒ…å«åœ¨è¯ä¸­çš„æœªçŒœå­—ç¬¦
        if set(soft_targets.keys()) != unguessed_in_word:
            incremental_errors += 1
            if incremental_errors <= 3:
                print(f"å¢é‡é”™è¯¯ {i}: soft_targets={set(soft_targets.keys())} != åœ¨è¯ä¸­æœªçŒœå­—ç¬¦={unguessed_in_word}")

        # æ£€æŸ¥soft_targetsæ ¼å¼å’Œåˆ†æ•°æ€»å’Œ
        if not isinstance(soft_targets, dict) or not soft_targets:
            soft_target_errors += 1
        else:
            # æ£€æŸ¥åˆ†æ•°æ˜¯å¦åˆç†
            scores = list(soft_targets.values())
            if any(score < 0 or score > 1 for score in scores):
                soft_target_errors += 1
                if soft_target_errors <= 3:
                    print(f"åˆ†æ•°èŒƒå›´é”™è¯¯ {i}: {soft_targets}")

            # æ£€æŸ¥æ€»å’Œæ˜¯å¦ä¸º1
            total_sum = sum(scores)
            if abs(total_sum - 1.0) > 0.01:  # å…è®¸1%çš„è¯¯å·®
                sum_errors += 1
                if sum_errors <= 3:
                    print(f"æ€»å’Œé”™è¯¯ {i}: soft_targetsæ€»å’Œ={total_sum:.3f}, åº”ä¸º1.0")
                    print(f"  soft_targets: {soft_targets}")

    print(f"æ ¼å¼é”™è¯¯: {format_errors}")
    print(f"é€»è¾‘é”™è¯¯: {logic_errors}")
    print(f"è½¯æ ‡ç­¾é”™è¯¯: {soft_target_errors}")
    print(f"æ€»å’Œé”™è¯¯: {sum_errors}")
    print(f"å¢é‡å­—ç¬¦é”™è¯¯: {incremental_errors}")
    print(f"completionä¸æ˜¯æœ€é«˜åˆ†é”™è¯¯: {completion_errors}")

    return (format_errors == 0 and logic_errors == 0 and soft_target_errors == 0
            and sum_errors == 0 and incremental_errors == 0 and completion_errors == 0)


def analyze_dataset(samples: List[Dict]):
    """åˆ†æç”Ÿæˆçš„æ•°æ®é›†"""
    print(f"\n=== æ•°æ®é›†åˆ†æ ===")
    print(f"æ€»æ ·æœ¬æ•°: {len(samples)}")

    # åˆ†æcompletionå­—ç¬¦åˆ†å¸ƒ
    completion_chars = [s["guess"]["completion"] for s in samples]
    char_dist = Counter(completion_chars)
    print(f"æœ€å¸¸è§çš„completionå­—ç¬¦: {char_dist.most_common(10)}")

    # åˆ†æè½¯æ ‡ç­¾æ•°é‡åˆ†å¸ƒ
    soft_target_counts = [len(s["soft_targets"]) for s in samples]
    count_dist = Counter(soft_target_counts)
    print(f"è½¯æ ‡ç­¾æ•°é‡åˆ†å¸ƒ: {dict(sorted(count_dist.items()))}")

    # åˆ†ææœ€é«˜åˆ†åˆ†å¸ƒ
    best_scores = [max(s["soft_targets"].values()) for s in samples]
    score_ranges = {
        "0.9-1.0": sum(1 for s in best_scores if s >= 0.9),
        "0.7-0.9": sum(1 for s in best_scores if 0.7 <= s < 0.9),
        "0.5-0.7": sum(1 for s in best_scores if 0.5 <= s < 0.7),
        "<0.5": sum(1 for s in best_scores if s < 0.5)
    }
    print(f"æœ€é«˜åˆ†åˆ†å¸ƒ: {score_ranges}")

    # ğŸ¯ æ–°å¢åˆ†æ
    analyze_total_guesses_distribution(samples)  # æ€»çŒœæµ‹æ¬¡æ•°åˆ†å¸ƒ
    analyze_game_state_distribution(samples)  # æ­ç¤ºè¿›åº¦å’Œé”™è¯¯æ¬¡æ•°åˆ†å¸ƒ

    print(f"\nâœ… æ‰€æœ‰completionéƒ½æ˜¯æœ€é«˜åˆ†å­—ç¬¦")
    print(f"âœ… æ‰€æœ‰soft_targetsåªåŒ…å«èƒ½å¸¦æ¥å¢é‡çš„å­—ç¬¦")
    print(f"âœ… Scoreä½¿ç”¨Softmaxæ¸©åº¦å¹³æ»‘ï¼ˆè¿ç»­ä¸”å¯å¯¼ï¼‰")
    print(f"âœ… æ€»çŒœæµ‹æ¬¡æ•°å‡åŒ€åˆ†å¸ƒï¼ˆæ¨¡æ‹Ÿå®Œæ•´æ¸¸æˆè¿‡ç¨‹ï¼‰")


def show_sample_examples(samples: List[Dict], num_examples: int = 3):
    """æ˜¾ç¤ºæ ·æœ¬æ ¼å¼ç¤ºä¾‹"""
    print(f"\n=== æ•°æ®æ ¼å¼ç¤ºä¾‹ ===")

    for i, sample in enumerate(samples[:num_examples]):
        print(f"\nç¤ºä¾‹ {i + 1}:")
        print(f"Word: {sample['label']} (é•¿åº¦: {len(sample['label'])})")
        print(f"Prompt: {sample['guess']['prompt']}")
        print(f"Completion: {sample['guess']['completion']}")
        print(f"Soft targets: {sample['soft_targets']}")

        # éªŒè¯ä¿¡æ¯
        soft_sum = sum(sample['soft_targets'].values())
        print(f"è½¯æ ‡ç­¾æ€»å’Œ: {soft_sum:.6f}")

        # è§£æçŠ¶æ€
        parts = sample['guess']['prompt'].split("[SEP]")
        if len(parts) >= 2 and parts[1]:
            guessed_chars = set(parts[1].split(','))
        else:
            guessed_chars = set()

        word = sample['label']
        unguessed_in_word = set(word) - guessed_chars

        print(f"å·²çŒœå­—ç¬¦: {sorted(guessed_chars)}")
        print(f"æ€»çŒœæµ‹æ¬¡æ•°: {len(guessed_chars)}")
        print(f"åœ¨è¯ä¸­æœªçŒœå­—ç¬¦: {sorted(unguessed_in_word)}")
        print(f"è½¯æ ‡ç­¾å­—ç¬¦: {sorted(sample['soft_targets'].keys())}")

        # ğŸ¯ æ˜¾ç¤ºSoftmaxåˆ†æ•°è®¡ç®—è¯¦æƒ…
        if "metadata" in sample and "score_details" in sample["metadata"]:
            print(f"Softmaxåˆ†æ•°è®¡ç®—è¯¦æƒ…:")
            temperature = sample["metadata"].get("temperature", 1.0)
            print(f"  æ¸©åº¦å‚æ•°: {temperature}")
            for char, details in sample["metadata"]["score_details"].items():
                print(f"  {char}: è¯ä¸­å‡ºç°{details['count_in_word']}æ¬¡, "
                      f"logit={details['softmax_logit']:.3f}, "
                      f"æœ€ç»ˆåˆ†æ•°={details['final_score']:.6f}")

        # éªŒè¯é€»è¾‘
        if set(sample['soft_targets'].keys()) == unguessed_in_word:
            print("âœ… è½¯æ ‡ç­¾=åœ¨è¯ä¸­æœªçŒœå­—ç¬¦")
        else:
            print(f"âš ï¸  è½¯æ ‡ç­¾ä¸åŒ¹é…åœ¨è¯ä¸­æœªçŒœå­—ç¬¦")

        best_char = max(sample['soft_targets'].items(), key=lambda x: x[1])[0]
        if sample['guess']['completion'] == best_char:
            print("âœ… completionæ˜¯æœ€é«˜åˆ†å­—ç¬¦")
        else:
            print(f"âš ï¸  completionä¸æ˜¯æœ€é«˜åˆ†å­—ç¬¦")


def main():
    parser = argparse.ArgumentParser(
        description="ç”ŸæˆHangman SFTè®­ç»ƒæ•°æ® (Softmaxæ¸©åº¦å¹³æ»‘ç‰ˆ + å‡åŒ€æ€»çŒœæµ‹æ¬¡æ•°åˆ†å¸ƒ + è¯é•¿å¢å¼º)")

    # ğŸ¯ æ–°å¢ï¼šè¾“å…¥è¾“å‡ºç›®å½•å‚æ•°
    parser.add_argument("--word_dir", type=str, default="dataset/180k_10k_10k_50k/",
                        help="è¯æ±‡æ–‡ä»¶æ‰€åœ¨ç›®å½• (é»˜è®¤: dataset/180k_10k_10k_50k/)")
    parser.add_argument("--out_dir", type=str, default="sft/data/",
                        help="è¾“å‡ºJSONLæ–‡ä»¶ç›®å½• (é»˜è®¤: sft/data/)")

    # åŸºç¡€å‚æ•°
    parser.add_argument("--max_samples", type=int, default=6,
                        help="æ¯ä¸ªè¯æœ€å¤šç”Ÿæˆçš„æ ·æœ¬æ•°")
    parser.add_argument("--max_processes", type=int, default=6,
                        help="æœ€å¤§è¿›ç¨‹æ•° (é»˜è®¤: CPUæ ¸å¿ƒæ•°)")
    parser.add_argument("--temperature", type=float, default=1.0,
                        help="Softmaxæ¸©åº¦å‚æ•° (é»˜è®¤: 1.0)")

    # ğŸ¯ æ–°å¢ï¼šè¯é•¿å¢å¼ºå‚æ•°
    parser.add_argument("--enhance_min_length", type=int, default=5,
                        help="å¢å¼ºè¯æ±‡çš„æœ€å°é•¿åº¦ (é»˜è®¤: 5)")
    parser.add_argument("--enhance_max_length", type=int, default=13,
                        help="å¢å¼ºè¯æ±‡çš„æœ€å¤§é•¿åº¦ (é»˜è®¤: 13)")
    parser.add_argument("--enhance_ratio", type=float, default=0.95,
                        help="å¢å¼ºè¯æ±‡åœ¨æ•°æ®ä¸­çš„ç›®æ ‡æ¯”ä¾‹ (0.0-1.0, é»˜è®¤: 0.95)")

    # å…¶ä»–å‚æ•°
    parser.add_argument("--include_metadata", action="store_true",
                        help="åœ¨è¾“å‡ºæ–‡ä»¶ä¸­åŒ…å«metadataä¿¡æ¯")
    parser.add_argument("--validate", action="store_true",
                        help="éªŒè¯ç”Ÿæˆçš„æ•°æ®")
    parser.add_argument("--show_examples", action="store_true",
                        help="æ˜¾ç¤ºæ•°æ®æ ¼å¼ç¤ºä¾‹")
    parser.add_argument("--seed", type=int, default=42,
                        help="éšæœºç§å­")
    parser.add_argument("--no_enhance", action="store_true",
                        help="ç¦ç”¨è¯é•¿å¢å¼ºï¼Œä½¿ç”¨åŸå§‹åˆ†å¸ƒ")

    args = parser.parse_args()

    # ğŸ¯ å¤„ç†ç›®å½•è·¯å¾„ï¼Œç¡®ä¿ä»¥æ–œæ ç»“å°¾
    word_dir = args.word_dir.rstrip('/') + '/'
    out_dir = args.out_dir.rstrip('/') + '/'

    # å‚æ•°éªŒè¯
    if not 0.0 <= args.enhance_ratio <= 1.0:
        print("é”™è¯¯: enhance_ratio å¿…é¡»åœ¨ 0.0-1.0 èŒƒå›´å†…")
        return

    if args.enhance_min_length > args.enhance_max_length:
        print("é”™è¯¯: enhance_min_length ä¸èƒ½å¤§äº enhance_max_length")
        return

    # æ£€æŸ¥è¾“å…¥ç›®å½•æ˜¯å¦å­˜åœ¨
    if not os.path.exists(word_dir):
        print(f"é”™è¯¯: è¯æ±‡ç›®å½•ä¸å­˜åœ¨: {word_dir}")
        return

    print("=" * 60)
    print("Hangman SFTæ•°æ®ç”Ÿæˆå™¨ (Softmaxæ¸©åº¦å¹³æ»‘ç‰ˆ + å‡åŒ€æ€»çŒœæµ‹æ¬¡æ•°åˆ†å¸ƒ + è¯é•¿å¢å¼º)")
    print("=" * 60)

    # è®¾ç½®éšæœºç§å­
    random.seed(args.seed)

    print(f"é…ç½®å‚æ•°:")
    print(f"  è·¯å¾„: word_dir={word_dir}, out_dir={out_dir}")
    print(f"  åŸºç¡€: max_samples={args.max_samples}, max_processes={args.max_processes}")
    print(f"  æ¸©åº¦: temperature={args.temperature}, seed={args.seed}")

    if not args.no_enhance:
        print(
            f"  ğŸ¯ è¯é•¿å¢å¼º: é•¿åº¦{args.enhance_min_length}-{args.enhance_max_length}, ç›®æ ‡æ¯”ä¾‹{args.enhance_ratio:.1%}")
    else:
        print(f"  ğŸ”§ è¯é•¿å¢å¼º: ç¦ç”¨ï¼Œä½¿ç”¨åŸå§‹åˆ†å¸ƒ")

    # ğŸ¯ æ„å»ºè¾“å…¥æ–‡ä»¶è·¯å¾„
    train_file_path = os.path.join(word_dir, "pretrain.txt")
    test_file_path = os.path.join(word_dir, "sft.txt")

    # åŠ è½½æ•°æ®
    print("\n" + "=" * 50)
    print("åŠ è½½è¯æ±‡æ•°æ®...")
    start_time = time.time()

    train_words = load_words(train_file_path)
    test_words = load_words(test_file_path)

    if not train_words:
        print(f"é”™è¯¯ï¼šæ— æ³•åŠ è½½è®­ç»ƒè¯æ±‡ä» {train_file_path}")
        return
    if not test_words:
        print(f"é”™è¯¯ï¼šæ— æ³•åŠ è½½æµ‹è¯•è¯æ±‡ä» {test_file_path}")
        return

    load_time = time.time() - start_time
    print(f"è¯æ±‡åŠ è½½å®Œæˆï¼Œè€—æ—¶: {load_time:.2f}s")

    # æ˜¾ç¤ºåŸå§‹è¯æ±‡ç»Ÿè®¡
    print(f"\nè¯æ±‡ç»Ÿè®¡:")
    print(f"  è®­ç»ƒè¯æ±‡: {len(train_words):,} ä¸ª (æ¥æº: {train_file_path})")
    print(f"  æµ‹è¯•è¯æ±‡: {len(test_words):,} ä¸ª (æ¥æº: {test_file_path})")

    # ğŸ¯ è¯é•¿å¢å¼ºå¤„ç†
    if not args.no_enhance:
        print("\n" + "=" * 50)
        print("è¯é•¿å¢å¼ºé‡é‡‡æ ·...")

        # é‡é‡‡æ ·è®­ç»ƒè¯æ±‡
        original_train_words = train_words.copy()
        train_words = resample_words_by_length(
            train_words,
            args.enhance_min_length,
            args.enhance_max_length,
            args.enhance_ratio
        )

        # é‡é‡‡æ ·æµ‹è¯•è¯æ±‡
        original_test_words = test_words.copy()
        test_words = resample_words_by_length(
            test_words,
            args.enhance_min_length,
            args.enhance_max_length,
            args.enhance_ratio
        )

        # æ˜¾ç¤ºé‡é‡‡æ ·å¯¹æ¯”
        print("\nğŸ“Š è®­ç»ƒé›†é‡é‡‡æ ·å¯¹æ¯”:")
        analyze_length_distribution_comparison(
            original_train_words, train_words,
            args.enhance_min_length, args.enhance_max_length
        )

        print("\nğŸ“Š æµ‹è¯•é›†é‡é‡‡æ ·å¯¹æ¯”:")
        analyze_length_distribution_comparison(
            original_test_words, test_words,
            args.enhance_min_length, args.enhance_max_length
        )

    else:
        print(f"\nğŸ”§ è·³è¿‡è¯é•¿å¢å¼ºï¼Œä½¿ç”¨åŸå§‹åˆ†å¸ƒ")

    # è®¡ç®—å…¨å±€å­—ç¬¦é¢‘ç‡ï¼ˆä¿ç•™ç”¨äºåˆ†æï¼‰
    print("\nè®¡ç®—å…¨å±€å­—ç¬¦é¢‘ç‡...")
    char_frequencies = calculate_global_frequencies(train_words)
    print("å­—ç¬¦é¢‘ç‡è®¡ç®—å®Œæˆ")

    # æ˜¾ç¤ºå­—ç¬¦é¢‘ç‡å‰10
    sorted_freqs = sorted(char_frequencies.items(), key=lambda x: x[1], reverse=True)
    print(f"æœ€å¸¸è§å­—ç¬¦: {[(c, f'{f:.4f}') for c, f in sorted_freqs[:10]]}")
    print(f"æœ€ç¨€æœ‰å­—ç¬¦: {[(c, f'{f:.6f}') for c, f in sorted_freqs[-10:]]}")

    # ç”Ÿæˆæ•°æ®
    print("\n" + "=" * 50)
    print("å¼€å§‹å¤šè¿›ç¨‹æ•°æ®ç”Ÿæˆ...")

    # ç”Ÿæˆè®­ç»ƒæ•°æ®
    print("\nç”Ÿæˆè®­ç»ƒæ•°æ®...")
    train_samples = generate_dataset_multiprocess(
        train_words, char_frequencies, args.max_samples, args.max_processes, args.seed, args.temperature
    )

    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    print("\nç”Ÿæˆæµ‹è¯•æ•°æ®...")
    test_samples = generate_dataset_multiprocess(
        test_words, char_frequencies, args.max_samples, args.max_processes, args.seed + 1, args.temperature
    )

    # ğŸ¯ æ„å»ºè¾“å‡ºæ–‡ä»¶è·¯å¾„
    os.makedirs(out_dir, exist_ok=True)

    train_output_path = os.path.join(out_dir, "train.jsonl")
    test_output_path = os.path.join(out_dir, "test.jsonl")
    train_metadata_path = os.path.join(out_dir, "train_with_metadata.jsonl")
    test_metadata_path = os.path.join(out_dir, "test_with_metadata.jsonl")

    # ä¿å­˜æ•°æ®
    print("\n" + "=" * 50)
    print("ä¿å­˜æ•°æ®...")

    # SFTè®­ç»ƒæ–‡ä»¶
    save_samples(train_samples, train_output_path, include_metadata=args.include_metadata)
    save_samples(test_samples, test_output_path, include_metadata=args.include_metadata)

    # å¸¦metadataçš„åˆ†ææ–‡ä»¶
    save_samples(train_samples, train_metadata_path, include_metadata=True)
    save_samples(test_samples, test_metadata_path, include_metadata=True)

    print(f"âœ… æ–‡ä»¶ä¿å­˜å®Œæˆ:")
    print(f"  è®­ç»ƒæ•°æ®: {train_output_path}")
    print(f"  æµ‹è¯•æ•°æ®: {test_output_path}")
    print(f"  è®­ç»ƒæ•°æ®(å«metadata): {train_metadata_path}")
    print(f"  æµ‹è¯•æ•°æ®(å«metadata): {test_metadata_path}")

    # éªŒè¯å’Œåˆ†æ
    if args.validate:
        print("\n" + "=" * 50)
        print("éªŒè¯æ•°æ®...")
        train_valid = validate_dataset(train_samples)
        test_valid = validate_dataset(test_samples)

        if train_valid and test_valid:
            print("âœ… æ‰€æœ‰éªŒè¯é€šè¿‡!")
        else:
            print("âš ï¸  éªŒè¯å‘ç°é—®é¢˜ï¼Œè¯·æ£€æŸ¥æ•°æ®")

    # åˆ†ææ•°æ®
    print("\n" + "=" * 50)
    print("è®­ç»ƒé›†åˆ†æ:")
    analyze_dataset(train_samples)

    print("\næµ‹è¯•é›†åˆ†æ:")
    analyze_dataset(test_samples)

    # æ˜¾ç¤ºç¤ºä¾‹
    if args.show_examples:
        show_sample_examples(train_samples)

    # è®¡ç®—æ€»è€—æ—¶
    total_time = time.time() - start_time

    # æœ€ç»ˆæ€»ç»“
    print("\n" + "=" * 60)
    print("âœ… æ•°æ®ç”Ÿæˆå®Œæˆ!")
    print(f"æ€»è€—æ—¶: {total_time:.2f}s")
    print(f"å¹³å‡é€Ÿåº¦: {(len(train_samples) + len(test_samples)) / total_time:.1f} æ ·æœ¬/ç§’")
    print(f"\nğŸ“ è¾“å…¥è¾“å‡ºè·¯å¾„:")
    print(f"  è¯æ±‡ç›®å½•: {word_dir}")
    print(f"  è¾“å‡ºç›®å½•: {out_dir}")
    print(f"\nå…³é”®ç‰¹æ€§:")
    print(f"  âœ… Score = Softmax(count/temperature) - è¿ç»­ä¸”å¯å¯¼")
    print(f"  âœ… æ¸©åº¦å‚æ•° = {args.temperature}")
    print(f"  âœ… åªåŒ…å«åœ¨è¯ä¸­çš„æœªçŒœå­—ç¬¦")
    print(f"  âœ… completionæ€»æ˜¯æœ€é«˜åˆ†å­—ç¬¦")
    print(f"  âœ… soft_targetsæ€»å’Œä¸º1.0")
    print(f"  âœ… æ€»çŒœæµ‹æ¬¡æ•°å‡åŒ€åˆ†å¸ƒ (0åˆ°N+5)")
    print(f"  âœ… æ¨¡æ‹Ÿå®Œæ•´æ¸¸æˆè¿‡ç¨‹")
    print(f"  âœ… å¤šè¿›ç¨‹åŠ é€Ÿç”Ÿæˆ")
    print(f"  âœ… è¯¦ç»†çš„åˆ†å¸ƒåˆ†æ")
    print(f"  ğŸ¯ å¯é…ç½®è¾“å…¥è¾“å‡ºç›®å½•")

    if not args.no_enhance:
        print(f"  ğŸ¯ è¯é•¿å¢å¼º: {args.enhance_min_length}-{args.enhance_max_length} é•¿åº¦èŒƒå›´ {args.enhance_ratio:.1%}")
    else:
        print(f"  ğŸ”§ ä½¿ç”¨åŸå§‹è¯æ±‡åˆ†å¸ƒ")


if __name__ == "__main__":
    main()