python trainer/pretrain.py --data dataset/180k_10k_10k_50k/pretrain.txt --tokenizer scripts/tokenizer/tokenizer.json \
--out checkpoints/pretrain.pth --d-model 256 --n-layer 8 --n-head 8 --max-seq-len 64 --batch-size 512 --lr 1e-3 --epochs 70 --experiment-name "hangman-pretrain"

python sft/generate_guess.py

python sft/sft_train.py \
    --batch_size 1024 \
    --eval_batch_size 1024 \
    --gradient_accumulation_steps 1 \
    --num_epochs 30 \
    --eval_steps 500 \
    --logging_steps 50 \
    --dataloader_num_workers 6 \
    --max_seq_length 64 \
    --experiment_name "single-word-pred-sft"
python sft/sft_train.py     --debug     --train_data sft/data/train.jsonl     --test_data sft/data/test.jsonl     --output_dir sft/models/full_test     --eval_steps 30     --samples_per_epoch 3     --experiment_name "full_validation_test"

python sft/sft_train.py    --batch_size 32    --eval_batch_size 32    --learning_rate 1e-3    --gradient_accumulation_steps 1    --num_epochs 10   --eval_steps 500    --logging_steps 50    --dataloader_num_workers 6    --max_seq_length 64    --experiment_name "single-word-pred-sft-imbalance-dataset"    --output_dir sft/models/imbalance/

python sft/sft_train.py    --batch_size 2048    --eval_batch_size 2048    --learning_rate 1e-3    --gradient_accumulation_steps 1    --num_epochs 20   --eval_steps 500    --logging_steps 50    --dataloader_num_workers 6    --max_seq_length 64    --experiment_name "single-word-pred-sft-4xdataset"    --output_dir sft/models/4xdataset/

python sft/sft_train.py   --pretrain_model checkpoints/final/pretrain_dp02_lr1e-3.pth  --train_data sft/data/final/train.jsonl --test_data sft/data/final/test.jsonl --batch_size 2048    --eval_batch_size 2048    --learning_rate 1e-3    --gradient_accumulation_steps 1    --num_epochs 20   --eval_steps 500    --logging_steps 50    --dataloader_num_workers 6    --max_seq_length 64    --experiment_name "single-word-pred-sf-4xdataset-8layer"    --output_dir sft/models/4xdataset-8layer-final/

# üî• ÊøÄËøõÂ≠¶‰π†ÂèÇÊï∞
python grpo/grpo.py  --beta 0.1   --learning_rate 1e-4   --min_reward_diff 0.5   --kl_weight 0.05   --alpha 1.5   --batch_size 32

python grpo/grpo.py   --beta 0.01   --kl_weight 0.0   --learning_rate 1e-4   --min_reward_diff 0.1   --wandb_project "hangman-grpo-kl-fixed"

python grpo/grpo.py   --learning_rate 3e-4        --beta 0.05                --kl_weight 0.02            --batch_size 16             --comparison_strategy top_bottom_half  --wandb_project "hangman-grpo-aggressive-fix"

python grpo/grpo.py   --kl_coeff 0.1   --clip_epsilon 0.2   --advantage_mode group_baseline   --learning_rate 2e-4   --batch_size 16   --k_trajectories 8   --num_epochs 15

python grpo/grpo.py   --kl_coeff 0.06   --clip_epsilon 0.12   --base_learning_rate 8e-5   --temperature 1.0   --record_probabilities   --enable_probability_logging

python trainer/pretrain.py --data dataset/225k_10k_5k_10k/pretrain.txt --tokenizer scripts/tokenizer/tokenizer.json --out checkpoints/final/pretrain_dp02_lr1e-3.pth --d-model 256 --n-layer 8 --n-head 8 --dropout 0.2 --max-seq-len 64 --batch-size 512 --lr 1e-3 --epochs 70 --min-lr 1e-6 --experiment-name "hangman-pretrain-dp02"