{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-11T13:52:36.796984400Z",
     "start_time": "2025-07-11T13:51:55.086402600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split sizes:\n",
      "{\n",
      "  \"pretrain\": 204570,\n",
      "  \"sft\": 9092,\n",
      "  \"grpo\": 4546,\n",
      "  \"test\": 9092\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Utility script to shuffle and split a word list into four subsets:\n",
    "- 180k pre‑training words\n",
    "- 10k SFT training words\n",
    "- 10k RL validation words\n",
    "- 50k held‑out test words\n",
    "\n",
    "\n",
    "Usage inside a notebook\n",
    "-----------------------\n",
    "```python\n",
    "import split_dataset_no_argparse as sds\n",
    "\n",
    "#quick start – uses the defaults below\n",
    "sds.run_split()\n",
    "\n",
    "#custom paths / seed\n",
    "sds.run_split(\n",
    "    input_path=\"data/words_250000_train.txt\",\n",
    "    outdir=\"splits\",\n",
    "    seed=123,\n",
    ")\n",
    "```\n",
    "\n",
    "Running as a standalone script from the command line still works; it\n",
    "just falls back to the same default arguments when you *don’t* supply\n",
    "any.\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import pathlib\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "from typing import Dict, List\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Core helpers\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def split_dataset(words: List[str], seed: int = 42) -> Dict[str, List[str]]:\n",
    "    \"\"\"Shuffle *words* deterministically and return the 4 requested subsets.\n",
    "\n",
    "    The target sizes follow the required 180k:10k:10k:50k pattern.\n",
    "    If the corpus is smaller than 250k, sizes are shrunk proportionally\n",
    "    but the *ratios* are preserved.\n",
    "    \"\"\"\n",
    "    rng = random.Random(seed)\n",
    "    rng.shuffle(words)\n",
    "\n",
    "    targets = OrderedDict([\n",
    "        (\"pretrain\", 225_000),\n",
    "        (\"sft\", 10_000),\n",
    "        (\"grpo\", 5_000),\n",
    "        (\"test\", 10_000),\n",
    "    ])\n",
    "\n",
    "    total_needed = sum(targets.values())\n",
    "    if len(words) < total_needed:\n",
    "        scale = len(words) / total_needed\n",
    "        for k in targets:\n",
    "            targets[k] = int(targets[k] * scale)\n",
    "\n",
    "    out, idx = {}, 0\n",
    "    for name, count in targets.items():\n",
    "        out[name] = words[idx : idx + count]\n",
    "        idx += count\n",
    "\n",
    "    #Sanity‑check that there are no duplicates across splits\n",
    "    union_size = len(set().union(*out.values()))\n",
    "    assert union_size == sum(len(v) for v in out.values()), (\n",
    "        \"Overlapping words detected across splits\"\n",
    "    )\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def write_splits(outdir: pathlib.Path, splits: Dict[str, List[str]]):\n",
    "    outdir.mkdir(parents=True, exist_ok=True)\n",
    "    for name, subset in splits.items():\n",
    "        path = outdir / f\"{name}.txt\"\n",
    "        path.write_text(\"\\n\".join(subset), encoding=\"utf‑8\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Public entry point (no argparse)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def run_split(\n",
    "    input_path: str | pathlib.Path = \"dataset/words_250000_train.txt\",\n",
    "    outdir: str | pathlib.Path = \"dataset/225k_10k_5k_10k\",\n",
    "    *,\n",
    "    seed: int = 42,\n",
    ") -> Dict[str, List[str]]:\n",
    "    \"\"\"Convenience wrapper to load words, split them, and write the files.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_path : str or Path\n",
    "        Text file with one word per line.\n",
    "    outdir : str or Path\n",
    "        Directory where the split files will be saved. Will be created\n",
    "        if it doesn’t exist.\n",
    "    seed : int, default 42\n",
    "        RNG seed for deterministic shuffling.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Mapping split‑name → list of words. Useful for in‑notebook checks.\n",
    "    \"\"\"\n",
    "    input_path = pathlib.Path(input_path)\n",
    "    words = [w.strip() for w in input_path.read_text(encoding=\"utf‑8\").splitlines() if w.strip()]\n",
    "\n",
    "    splits = split_dataset(words, seed=seed)\n",
    "    write_splits(pathlib.Path(outdir), splits)\n",
    "\n",
    "    counts = {k: len(v) for k, v in splits.items()}\n",
    "    print(\"Split sizes:\")\n",
    "    print(json.dumps(counts, indent=2))\n",
    "    return splits\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Optional CLI fallback (still without argparse)\n",
    "# -----------------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    #Use defaults so the script can be run without parameters:\n",
    "    #   python split_dataset_no_argparse.py\n",
    "    run_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ.get(\"DATA_DIR\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-04T23:44:14.841245700Z",
     "start_time": "2025-07-04T23:44:14.827724500Z"
    }
   },
   "id": "a92275bdb1b7ab67"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
