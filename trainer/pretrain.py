"""
trainer/pretrain.py
å­—ç¬¦çº§ GPT é¢„è®­ç»ƒ - é€‚é… EnhancedCharacterTokenizer
æ¯ä¸ªæ ·æœ¬æ˜¯ä¸€è¡Œå•è¯ï¼›ç›®æ ‡æ˜¯ä¸‹ä¸€å­—ç¬¦é¢„æµ‹ã€‚
é›†æˆwandbè¿›è¡Œå®éªŒç®¡ç†å’Œè·Ÿè¸ªï¼Œæ”¯æŒå‘½ä»¤è¡Œå‚æ•°é…ç½®
é‡æ„ç‰ˆæœ¬ï¼šè§£è€¦æ¨¡å‹é…ç½®ä¸è®­ç»ƒé…ç½®ï¼Œæ”¯æŒçµæ´»çš„æ¨¡å‹åŠ è½½
å¢å¼ºEarly Stoppingè¾“å‡º
æ”¯æŒéšæœºæ•°æ®åˆ†å‰²
"""
from __future__ import annotations
import argparse, os, math, json, time, pathlib, random
import torch
from torch import nn
from torch.utils.data import Dataset, DataLoader
import wandb

# ğŸ¯ ä½¿ç”¨æˆ‘ä»¬çš„ EnhancedCharacterTokenizer
import sys

sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'scripts'))
from tokenizer import CharacterTokenizer  # è¿™æ˜¯ EnhancedCharacterTokenizer çš„åˆ«å


# ---------------- æ•°æ®åˆ†å‰²å‡½æ•° ----------------
def split_data_randomly(data_file, train_ratio, val_ratio, output_dir, random_seed=42):
    """éšæœºåˆ†å‰²æ•°æ®"""
    print(f"ğŸ”€ Randomly splitting data: {data_file}")

    with open(data_file, 'r', encoding='utf-8') as f:
        lines = [line.strip() for line in f if line.strip()]

    total_lines = len(lines)
    print(f"   Total words: {total_lines:,}")

    random.seed(random_seed)
    random.shuffle(lines)

    train_size = int(total_lines * train_ratio)
    val_size = int(total_lines * val_ratio)

    # ç¡®ä¿ä¸è¶…è¿‡æ€»æ•°æ®é‡
    if train_size + val_size > total_lines:
        val_size = total_lines - train_size

    train_lines = lines[:train_size]
    val_lines = lines[train_size:train_size + val_size]

    print(f"   Train words: {len(train_lines):,} ({len(train_lines) / total_lines:.1%})")
    print(f"   Val words: {len(val_lines):,} ({len(val_lines) / total_lines:.1%})")
    print(f"   Unused words: {total_lines - train_size - val_size:,}")

    os.makedirs(output_dir, exist_ok=True)

    train_file = os.path.join(output_dir, 'train_split.txt')
    val_file = os.path.join(output_dir, 'val_split.txt')

    with open(train_file, 'w', encoding='utf-8') as f:
        f.write('\n'.join(train_lines))

    with open(val_file, 'w', encoding='utf-8') as f:
        f.write('\n'.join(val_lines))

    print(f"âœ… Split data saved: {train_file}, {val_file}")
    return train_file, val_file


# ---------------- é…ç½®ç±» ----------------
class ModelConfig:
    """æ¨¡å‹æ¶æ„é…ç½® - ç‹¬ç«‹äºè®­ç»ƒé…ç½®"""

    def __init__(self,
                 d_model: int = 256,
                 n_layer: int = 8,
                 n_head: int = 8,
                 dropout: float = 0.15,
                 max_seq_len: int = 32):
        self.d_model = d_model
        self.n_layer = n_layer
        self.n_head = n_head
        self.dropout = dropout
        self.max_seq_len = max_seq_len

    def to_dict(self):
        return {
            'd_model': self.d_model,
            'n_layer': self.n_layer,
            'n_head': self.n_head,
            'dropout': self.dropout,
            'max_seq_len': self.max_seq_len
        }

    @classmethod
    def from_dict(cls, config_dict):
        return cls(
            d_model=config_dict.get('d_model', 256),
            n_layer=config_dict.get('n_layer', 8),
            n_head=config_dict.get('n_head', 8),
            dropout=config_dict.get('dropout', 0.15),
            max_seq_len=config_dict.get('max_seq_len', 32)
        )


class TrainingConfig:
    """è®­ç»ƒé…ç½® - ç”¨äºè®­ç»ƒè¿‡ç¨‹çš„å‚æ•°"""
    # åŸºç¡€é…ç½® - é»˜è®¤å€¼ï¼Œå¯é€šè¿‡å‘½ä»¤è¡Œå‚æ•°è¦†ç›–
    batch_size = 512
    lr = 1e-3
    weight_decay = 0.1
    warmup = 300
    decay_epochs = 30
    min_lr = 1e-5
    epochs = 50
    patience = 8
    grad_clip = 1.0

    # ğŸ¯ æ•°æ®åˆ†å‰²é…ç½®
    train_ratio = 0.8
    val_ratio = 0.1
    random_seed = 42

    # wandbé…ç½®
    project_name = "enhanced-char-gpt-pretrain"
    experiment_name = None  # å¦‚æœNoneåˆ™è‡ªåŠ¨ç”Ÿæˆ
    use_wandb = True
    wandb_entity = None  # ä½ çš„wandbç”¨æˆ·åæˆ–å›¢é˜Ÿå
    tags = []  # å®éªŒæ ‡ç­¾
    notes = ""  # å®éªŒç¬”è®°

    # ç›‘æ§é…ç½®
    log_freq = 10  # æ¯Nä¸ªbatchè®°å½•ä¸€æ¬¡
    save_freq = 5  # æ¯Nä¸ªepochä¿å­˜æ ·æœ¬
    watch_model = True  # ç›‘æ§æ¨¡å‹æ¢¯åº¦å’Œå‚æ•°
    eval_every = 5  # æ¯nä¸ªepochåšä¸€æ¬¡è¯¦ç»†è¯„ä¼°
    save_samples = True  # ä¿å­˜ç”Ÿæˆæ ·æœ¬åˆ°wandb
    samples_per_epoch = 5  # ğŸ¯ æ¯ä¸ªepochç”Ÿæˆçš„æ ·æœ¬æ•°


# ä¿æŒå‘åå…¼å®¹æ€§çš„Configç±»
Config = TrainingConfig


# ---------------- æ•°æ® ----------------
class WordDataset(Dataset):
    def __init__(self, path: str, tokenizer: CharacterTokenizer, max_len: int):
        self.tok = tokenizer
        self.words = [w.strip() for w in pathlib.Path(path).read_text().splitlines() if w.strip()]
        self.max_len = max_len

        # ğŸ¯ ä½¿ç”¨ EnhancedCharacterTokenizer çš„æ¥å£
        self.bos_id = self.tok.bos_token_id
        self.eos_id = self.tok.eos_token_id
        self.pad_id = self.tok.pad_token_id
        self.unk_id = self.tok.unk_token_id

        print(f"ğŸ“Š WordDataset initialized:")
        print(f"   Words: {len(self.words):,}")
        print(f"   Max length: {max_len}")
        print(f"   Special tokens: bos={self.bos_id}, eos={self.eos_id}, pad={self.pad_id}, unk={self.unk_id}")

        # éªŒè¯ä¸€äº›æ ·æœ¬
        self._verify_samples()

    def _verify_samples(self):
        """éªŒè¯æ ·æœ¬ç¼–ç æ˜¯å¦æ­£ç¡®"""
        print("ğŸ” Verifying samples...")
        sample_size = min(3, len(self.words))

        for i in range(sample_size):
            word = self.words[i]
            encoded = self.tok.encode(word, add_special_tokens=True)
            decoded = self.tok.decode(encoded, skip_special_tokens=True)

            print(f"   Sample {i + 1}: '{word}' â†’ {encoded} â†’ '{decoded}'")

            if word.lower() != decoded.lower():
                print(f"   âš ï¸  Encoding mismatch!")

    def __len__(self):
        return len(self.words)

    def __getitem__(self, idx):
        word = self.words[idx].lower()

        # ğŸ¯ ä½¿ç”¨ EnhancedCharacterTokenizer ç¼–ç 
        try:
            # ç›´æ¥ç¼–ç å•è¯ï¼ˆåŒ…å« bos/eosï¼‰
            ids = self.tok.encode(word, add_special_tokens=True)
            ids = ids[:self.max_len]  # æˆªæ–­

            # å‡†å¤‡è¾“å…¥å’Œç›®æ ‡åºåˆ—
            x = ids[:-1] + [self.pad_id] * (self.max_len - len(ids) + 1)
            y = ids[1:] + [self.pad_id] * (self.max_len - len(ids) + 1)
            attn_mask = [1] * (len(ids) - 1) + [0] * (self.max_len - len(ids) + 1)

            # ç¡®ä¿é•¿åº¦æ­£ç¡®
            x = x[:self.max_len]
            y = y[:self.max_len]
            attn_mask = attn_mask[:self.max_len]

            return torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.long), torch.tensor(attn_mask,
                                                                                                      dtype=torch.long)

        except Exception as e:
            print(f"âŒ Error encoding word '{word}': {e}")
            # è¿”å›é»˜è®¤çš„ç©ºåºåˆ—
            x = [self.bos_id, self.eos_id] + [self.pad_id] * (self.max_len - 2)
            y = [self.eos_id] + [self.pad_id] * (self.max_len - 1)
            attn_mask = [1] + [0] * (self.max_len - 1)

            return torch.tensor(x[:self.max_len], dtype=torch.long), \
                torch.tensor(y[:self.max_len], dtype=torch.long), \
                torch.tensor(attn_mask[:self.max_len], dtype=torch.long)


def make_loader(path, tok, max_len, batch, shuffle=False, num_workers=4):
    """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
    ds = WordDataset(path, tok, max_len)
    return DataLoader(ds, batch_size=batch, shuffle=shuffle, drop_last=True,
                      num_workers=num_workers, pin_memory=True)


# ---------------- æ¨¡å‹ ----------------
class GPT(nn.Module):
    def __init__(self, vocab_size: int, model_config: ModelConfig = None):
        super().__init__()
        # å‘åå…¼å®¹ï¼šå¦‚æœæ²¡æœ‰æä¾›model_configï¼Œä½¿ç”¨å…¨å±€Config
        if model_config is None:
            model_config = ModelConfig(
                d_model=getattr(Config, 'd_model', 256),
                n_layer=getattr(Config, 'n_layer', 8),
                n_head=getattr(Config, 'n_head', 8),
                dropout=getattr(Config, 'dropout', 0.15),
                max_seq_len=getattr(Config, 'max_seq_len', 64)
            )

        self.config = model_config
        self.max_seq_len = model_config.max_seq_len
        self.vocab_size = vocab_size

        print(f"ğŸ§  Creating GPT model:")
        print(f"   Vocab size: {vocab_size}")
        print(f"   Model config: {model_config.to_dict()}")

        self.tok_emb = nn.Embedding(vocab_size, model_config.d_model)
        self.pos_emb = nn.Embedding(model_config.max_seq_len, model_config.d_model)
        self.drop = nn.Dropout(model_config.dropout)

        self.blocks = nn.ModuleList([
            nn.TransformerEncoderLayer(
                d_model=model_config.d_model,
                nhead=model_config.n_head,
                dim_feedforward=model_config.d_model * 4,
                dropout=model_config.dropout,
                activation="gelu",
                batch_first=True,
                norm_first=True,  # Pre-norm
            )
            for _ in range(model_config.n_layer)
        ])

        self.norm = nn.LayerNorm(model_config.d_model)
        self.lm_head = nn.Linear(model_config.d_model, vocab_size, bias=False)

        self.register_buffer(
            "causal_mask",
            torch.triu(torch.ones(model_config.max_seq_len, model_config.max_seq_len, dtype=torch.bool), diagonal=1)
        )

        # æƒé‡åˆå§‹åŒ–
        self.apply(self._init_weights)

    def _init_weights(self, module):
        """æƒé‡åˆå§‹åŒ–"""
        if isinstance(module, nn.Linear):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
        elif isinstance(module, nn.LayerNorm):
            torch.nn.init.zeros_(module.bias)
            torch.nn.init.ones_(module.weight)

    def create_causal_mask(self, seq_len: int) -> torch.Tensor:
        return self.causal_mask[:seq_len, :seq_len]

    def forward(self, x, attn_mask=None):
        B, T = x.shape
        device = x.device

        if T > self.max_seq_len:
            raise ValueError(f"Sequence length {T} exceeds max_seq_len {self.max_seq_len}")

        positions = torch.arange(T, device=device).unsqueeze(0).expand(B, -1)
        token_embeddings = self.tok_emb(x)
        position_embeddings = self.pos_emb(positions)
        h = self.drop(token_embeddings + position_embeddings)

        causal_mask = self.create_causal_mask(T)
        src_key_padding_mask = None
        if attn_mask is not None:
            src_key_padding_mask = (attn_mask == 0)

        for block in self.blocks:
            h = block(h, src_mask=causal_mask, src_key_padding_mask=src_key_padding_mask)

        h = self.norm(h)
        logits = self.lm_head(h)
        return logits


# ---------------- æ¨¡å‹åŠ è½½å‡½æ•° ----------------
def load_pretrained_model(checkpoint_path: str, tokenizer_path: str, device: str = None):
    """é€šç”¨çš„æ¨¡å‹åŠ è½½å‡½æ•°ï¼Œè‡ªåŠ¨å¤„ç†é…ç½®"""
    if device is None:
        device = "cuda" if torch.cuda.is_available() else "cpu"

    checkpoint = torch.load(checkpoint_path, map_location='cpu')

    # ä»checkpointæ¢å¤é…ç½®
    if 'config' in checkpoint:
        saved_config = checkpoint['config']
        model_config = ModelConfig.from_dict(saved_config)
    else:
        # ä»æ¨¡å‹æƒé‡æ¨æ–­é…ç½®
        state_dict = checkpoint.get('model_state_dict', checkpoint)

        # æ¨æ–­é…ç½®
        d_model = state_dict['tok_emb.weight'].shape[1]
        vocab_size = state_dict['tok_emb.weight'].shape[0]
        max_seq_len = state_dict['pos_emb.weight'].shape[0]

        # æ¨æ–­n_layer
        n_layer = 0
        for key in state_dict.keys():
            if key.startswith('blocks.') and '.self_attn.in_proj_weight' in key:
                layer_idx = int(key.split('.')[1])
                n_layer = max(n_layer, layer_idx + 1)

        # æ¨æ–­n_head (å‡è®¾æ ‡å‡†é…ç½®)
        n_head = 8 if d_model >= 256 else 4

        model_config = ModelConfig(
            d_model=d_model,
            max_seq_len=max_seq_len,
            n_head=n_head,
            n_layer=n_layer,
            dropout=0.15  # é»˜è®¤å€¼
        )

        print(f"âš ï¸  æ¨æ–­æ¨¡å‹é…ç½®: {model_config.to_dict()}")

    # ğŸ¯ åŠ è½½ EnhancedCharacterTokenizer
    tokenizer = CharacterTokenizer(config_path=tokenizer_path)
    vocab_size = tokenizer.get_vocab_size()

    # åˆ›å»ºæ¨¡å‹
    model = GPT(vocab_size, model_config)

    # åŠ è½½æƒé‡
    if 'model_state_dict' in checkpoint:
        model.load_state_dict(checkpoint['model_state_dict'])
    else:
        model.load_state_dict(checkpoint)

    model = model.to(device)

    return model, tokenizer, model_config


# ---------------- è¯„ä¼°å’Œé‡‡æ ·å‡½æ•° ----------------
def generate_samples(model, tokenizer, device, num_samples=5, max_len=20, temperature=0.8, top_p=0.9):
    """ğŸ¯ ç”Ÿæˆæ ·æœ¬ç”¨äºè¯„ä¼° - ä¼˜åŒ–ç‰ˆæœ¬"""
    model.eval()
    samples = []

    bos_id = tokenizer.bos_token_id
    eos_id = tokenizer.eos_token_id
    pad_id = tokenizer.pad_token_id

    with torch.no_grad():
        for sample_idx in range(num_samples):
            # ä»<bos>å¼€å§‹ç”Ÿæˆ
            generated = [bos_id]

            for step in range(max_len):
                # å‡†å¤‡è¾“å…¥
                x = torch.tensor([generated], dtype=torch.long, device=device)
                if x.size(1) >= model.max_seq_len:
                    break

                # è·å–logits
                logits = model(x)
                logits = logits[0, -1, :] / temperature  # æœ€åä¸€ä¸ªä½ç½®çš„logits

                # Top-pé‡‡æ ·
                sorted_logits, sorted_indices = torch.sort(logits, descending=True)
                cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)

                # æ‰¾åˆ°ç´¯ç§¯æ¦‚ç‡è¶…è¿‡top_pçš„ä½ç½®
                sorted_indices_to_remove = cumulative_probs > top_p
                sorted_indices_to_remove[1:] = sorted_indices_to_remove[:-1].clone()
                sorted_indices_to_remove[0] = 0

                indices_to_remove = sorted_indices[sorted_indices_to_remove]
                logits[indices_to_remove] = float('-inf')

                # é‡‡æ ·
                probs = torch.softmax(logits, dim=-1)
                next_token = torch.multinomial(probs, 1).item()

                generated.append(next_token)

                if next_token == eos_id:
                    break

            # ğŸ¯ ä½¿ç”¨ EnhancedCharacterTokenizer è§£ç 
            try:
                word = tokenizer.decode(generated, skip_special_tokens=True)
                if word:
                    samples.append(word)
                else:
                    samples.append("[EMPTY]")
            except Exception as e:
                samples.append(f"[ERROR: {str(e)[:20]}]")

    return samples


def evaluate_model_detailed(model, val_loader, tokenizer, device, loss_fn):
    """è¯¦ç»†è¯„ä¼°æ¨¡å‹"""
    model.eval()
    total_loss = 0
    total_samples = 0

    with torch.no_grad():
        for x, y, attn in val_loader:
            x, y, attn = x.to(device), y.to(device), attn.to(device)
            logits = model(x, attn_mask=attn)
            loss = loss_fn(logits.view(-1, model.vocab_size), y.view(-1))
            total_loss += loss.item() * x.size(0)
            total_samples += x.size(0)

    val_ppl = math.exp(total_loss / total_samples)

    # ç”Ÿæˆæ›´å¤šæ ·æœ¬ç”¨äºè¯¦ç»†è¯„ä¼°
    samples = generate_samples(model, tokenizer, device, num_samples=10)

    return {
        'val_ppl': val_ppl,
        'val_loss': total_loss / total_samples,
        'samples': samples
    }


def quick_evaluate_with_samples(model, val_loader, tokenizer, device, loss_fn):
    """ğŸ¯ å¿«é€ŸéªŒè¯å¹¶ç”Ÿæˆå°‘é‡æ ·æœ¬"""
    model.eval()
    total_loss = 0
    total_samples = 0

    with torch.no_grad():
        # å¿«é€Ÿè®¡ç®—éªŒè¯æŸå¤±
        for x, y, attn in val_loader:
            x, y, attn = x.to(device), y.to(device), attn.to(device)
            logits = model(x, attn_mask=attn)
            loss = loss_fn(logits.view(-1, model.vocab_size), y.view(-1))
            total_loss += loss.item() * x.size(0)
            total_samples += x.size(0)

    val_ppl = math.exp(total_loss / total_samples)

    # ğŸ¯ æ¯ä¸ªepochéƒ½ç”Ÿæˆ5ä¸ªæ ·æœ¬
    samples = generate_samples(model, tokenizer, device, num_samples=Config.samples_per_epoch)

    return {
        'val_ppl': val_ppl,
        'val_loss': total_loss / total_samples,
        'samples': samples
    }


# ---------------- ä¸»è®­ç»ƒå‡½æ•° ----------------
def main(args):
    # è®°å½•å¼€å§‹æ—¶é—´
    start_time = time.time()

    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸš€ Starting Enhanced Character GPT Training")
    print(f"ğŸ“± Using device: {device}")

    # ğŸ¯ æ•°æ®åˆ†å‰²
    print(f"\nğŸ“Š Data Splitting Configuration:")
    print(f"   Source file: {args.data}")
    print(f"   Train ratio: {Config.train_ratio} ({Config.train_ratio:.1%})")
    print(f"   Val ratio: {Config.val_ratio} ({Config.val_ratio:.1%})")
    print(f"   Random seed: {Config.random_seed}")

    # åˆ›å»ºä¸´æ—¶ç›®å½•ç”¨äºå­˜æ”¾åˆ†å‰²åçš„æ•°æ®
    split_dir = os.path.join(os.path.dirname(args.data), 'splits')

    # æ‰§è¡Œæ•°æ®åˆ†å‰²
    train_file, val_file = split_data_randomly(
        data_file=args.data,
        train_ratio=Config.train_ratio,
        val_ratio=Config.val_ratio,
        output_dir=split_dir,
        random_seed=Config.random_seed
    )

    # ä»å‘½ä»¤è¡Œå‚æ•°åˆ›å»ºæ¨¡å‹é…ç½®
    model_config = ModelConfig(
        d_model=args.d_model,
        n_layer=args.n_layer,
        n_head=args.n_head,
        dropout=args.dropout,
        max_seq_len=args.max_seq_len
    )

    # æ‰“å°å½“å‰é…ç½®
    print(f"\nğŸ”§ Model Configuration:")
    print(f"   Layers: {model_config.n_layer}")
    print(f"   Model dim: {model_config.d_model}")
    print(f"   Attention heads: {model_config.n_head}")
    print(f"   Max sequence length: {model_config.max_seq_len}")
    print(f"   Dropout: {model_config.dropout}")

    print(f"\nâš™ï¸  Training Configuration:")
    print(f"   Learning rate: {Config.lr}")
    print(f"   Batch size: {Config.batch_size}")
    print(f"   Epochs: {Config.epochs}")
    print(f"   ğŸ¯ Early Stopping Patience: {Config.patience}")
    print(f"   ğŸ¯ Samples per epoch: {Config.samples_per_epoch}")

    # ğŸ¯ åŠ è½½ EnhancedCharacterTokenizer
    print(f"\nğŸ”¤ Loading Enhanced Character Tokenizer...")
    tok = CharacterTokenizer(config_path=args.tokenizer)
    vocab_size = tok.get_vocab_size()
    print(f"   Vocab size: {vocab_size}")

    # åŠ è½½æ•°æ® - ä½¿ç”¨åˆ†å‰²åçš„æ–‡ä»¶
    print(f"\nğŸ“š Loading datasets...")
    train_loader = make_loader(train_file, tok, model_config.max_seq_len, Config.batch_size, shuffle=True)
    val_loader = make_loader(val_file, tok, model_config.max_seq_len, Config.batch_size, shuffle=False)
    print(f"   Train batches: {len(train_loader):,}")
    print(f"   Val batches: {len(val_loader):,}")

    # æ¨¡å‹å’Œä¼˜åŒ–å™¨
    print(f"\nğŸ§  Creating model...")
    model = GPT(vocab_size, model_config).to(device)
    param_count = sum(p.numel() for p in model.parameters())
    print(f"   Parameters: {param_count:,} ({param_count / 1e6:.1f}M)")

    # æµ‹è¯•å‰å‘ä¼ æ’­
    print(f"\nğŸ§ª Testing model...")
    test_batch = next(iter(train_loader))
    test_x, test_y, test_attn = test_batch
    test_x = test_x[:4].to(device)  # åªç”¨4ä¸ªæ ·æœ¬æµ‹è¯•
    test_attn = test_attn[:4].to(device)

    with torch.no_grad():
        test_logits = model(test_x, attn_mask=test_attn)
        print(f"   Test output shape: {test_logits.shape}")
        print(f"   Memory allocated: {torch.cuda.memory_allocated() / 1024 ** 3:.2f}GB")

    # åˆå§‹åŒ–wandb
    if Config.use_wandb:
        print(f"\nğŸ“Š Initializing wandb...")
        try:
            wandb_api = wandb.Api()
            print(f"   âœ… Logged in as: {wandb_api.viewer.username}")
        except Exception as e:
            print(f"   âŒ wandb login failed: {e}")
            print("   Please run: wandb login")
            return

        run_name = Config.experiment_name or f"enhanced-char-gpt-{int(time.time())}"

        # å‡†å¤‡æ ‡ç­¾
        tags = Config.tags.copy()
        if args.tags:
            tags.extend(args.tags)
        tags.append("enhanced-tokenizer")
        tags.append("samples-per-epoch")
        tags.append("random-split")  # ğŸ¯ æ–°æ ‡ç­¾

        run = wandb.init(
            project=Config.project_name,
            entity=Config.wandb_entity,
            name=run_name,
            tags=tags,
            notes=Config.notes or args.notes or "",
            config={
                # æ¨¡å‹é…ç½®
                **model_config.to_dict(),

                # è®­ç»ƒé…ç½®
                'batch_size': Config.batch_size,
                'lr': Config.lr,
                'weight_decay': Config.weight_decay,
                'warmup': Config.warmup,
                'decay_epochs': Config.decay_epochs,
                'min_lr': Config.min_lr,
                'epochs': Config.epochs,
                'patience': Config.patience,
                'grad_clip': Config.grad_clip,
                'samples_per_epoch': Config.samples_per_epoch,

                # ğŸ¯ æ•°æ®åˆ†å‰²é…ç½®
                'data_file': args.data,
                'train_ratio': Config.train_ratio,
                'val_ratio': Config.val_ratio,
                'random_seed': Config.random_seed,
                'train_file': train_file,
                'val_file': val_file,

                # æ•°æ®é…ç½®
                'tokenizer_file': args.tokenizer,
                'vocab_size': vocab_size,
                'param_count': param_count,

                # tokenizeré…ç½®
                'tokenizer_type': 'EnhancedCharacterTokenizer',
                'special_tokens': {
                    'pad': tok.pad_token_id,
                    'bos': tok.bos_token_id,
                    'eos': tok.eos_token_id,
                    'unk': tok.unk_token_id,
                    'sep': tok.sep_token_id,
                },

                # ç³»ç»Ÿé…ç½®
                'device': str(device),
                'pytorch_version': torch.__version__,
            }
        )

        # ç›‘æ§æ¨¡å‹ï¼ˆå¯é€‰ï¼Œä¼šå ç”¨ä¸€äº›èµ„æºï¼‰
        if Config.watch_model:
            wandb.watch(model, log="gradients", log_freq=100)

        wandb.log({"model_parameters": param_count})

    # ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
    opt = torch.optim.AdamW(model.parameters(), lr=Config.lr, weight_decay=Config.weight_decay)

    # ä¿®å¤å­¦ä¹ ç‡è°ƒåº¦ - ç¡®ä¿ä¸ä¼šé™¤é›¶
    def get_lr_schedule(step):
        total_steps = Config.epochs * len(train_loader)
        warmup_steps = Config.warmup
        decay_start_step = Config.decay_epochs * len(train_loader)

        if step < warmup_steps:
            return step / warmup_steps
        elif step < decay_start_step:
            return 1.0
        else:
            # ä¿®å¤é™¤é›¶é”™è¯¯
            remaining_steps = total_steps - decay_start_step
            if remaining_steps <= 0:
                return Config.min_lr / Config.lr

            progress = (step - decay_start_step) / remaining_steps
            progress = min(progress, 1.0)  # ç¡®ä¿progressä¸è¶…è¿‡1

            return Config.min_lr / Config.lr + 0.5 * (1 - Config.min_lr / Config.lr) * (
                    1 + math.cos(math.pi * progress))

    scheduler = torch.optim.lr_scheduler.LambdaLR(opt, get_lr_schedule)

    # ğŸ¯ ä½¿ç”¨tokenizerçš„pad_token_id
    loss_fn = nn.CrossEntropyLoss(ignore_index=tok.pad_token_id)

    # ğŸ¯ Early Stopping åˆå§‹åŒ–
    print(f"\nğŸ›‘ Early Stopping Configuration:")
    print(f"   Patience: {Config.patience} epochs")
    print(f"   Monitoring: Validation Perplexity (lower is better)")
    print(f"   Will save best model automatically")

    # è®­ç»ƒå¾ªç¯
    print(f"\nğŸ”¥ Starting training...")
    best_ppl, bad_epochs = 1e9, 0
    best_epoch = 0
    ppl_history = []  # ğŸ¯ è¿½è¸ªPPLå†å²

    for ep in range(1, Config.epochs + 1):
        epoch_start_time = time.time()

        # ğŸ¯ ç´§å‡‘çš„epochæ ‡é¢˜
        print(f"\n{'=' * 60}")
        print(f"ğŸ”¥ EPOCH {ep}/{Config.epochs}")
        print(f"{'=' * 60}")

        # è®­ç»ƒ
        model.train()
        train_loss = 0
        train_samples = 0

        for batch_idx, (x, y, attn) in enumerate(train_loader):
            x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)
            attn = attn.to(device, non_blocking=True)

            logits = model(x, attn_mask=attn)
            loss = loss_fn(logits.view(-1, vocab_size), y.view(-1))

            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), Config.grad_clip)
            opt.step()
            scheduler.step()
            opt.zero_grad()

            train_loss += loss.item() * x.size(0)
            train_samples += x.size(0)

            # å®æ—¶æ—¥å¿—è®°å½•
            if Config.use_wandb and (batch_idx + 1) % Config.log_freq == 0:
                wandb.log({
                    'batch_loss': loss.item(),
                    'batch_lr': scheduler.get_last_lr()[0],
                    'batch_step': ep * len(train_loader) + batch_idx,
                })

            # ğŸ¯ ç´§å‡‘çš„batchè¿›åº¦æ˜¾ç¤º
            if (batch_idx + 1) % 100 == 0:
                current_lr = scheduler.get_last_lr()[0]
                current_memory = torch.cuda.memory_allocated() / 1024 ** 3
                throughput = (batch_idx + 1) * Config.batch_size / (time.time() - epoch_start_time)

                print(f"   Batch {batch_idx + 1:4d}/{len(train_loader)} | "
                      f"Loss: {loss.item():.4f} | "
                      f"LR: {current_lr:.2e} | "
                      f"Mem: {current_memory:.1f}GB | "
                      f"Speed: {throughput:.0f} samples/s")

        train_ppl = math.exp(train_loss / train_samples)
        epoch_time = time.time() - epoch_start_time

        # ğŸ¯ éªŒè¯ - æ¯ä¸ªepochéƒ½ç”Ÿæˆæ ·æœ¬
        if ep % Config.eval_every == 0 or ep == 1:
            # è¯¦ç»†è¯„ä¼°
            eval_results = evaluate_model_detailed(model, val_loader, tok, device, loss_fn)
            val_ppl = eval_results['val_ppl']
            samples = eval_results['samples']
            eval_type = "è¯¦ç»†"
        else:
            # ğŸ¯ å¿«é€ŸéªŒè¯ä½†ä»ç”Ÿæˆæ ·æœ¬
            eval_results = quick_evaluate_with_samples(model, val_loader, tok, device, loss_fn)
            val_ppl = eval_results['val_ppl']
            samples = eval_results['samples']
            eval_type = "å¿«é€Ÿ"

        # ğŸ¯ è¿½è¸ªå†å²
        ppl_history.append(val_ppl)

        # è®°å½•æŒ‡æ ‡
        elapsed_time = time.time() - start_time

        # ğŸ¯ åˆ†ææ ·æœ¬è´¨é‡
        valid_samples = [s for s in samples if len(s) > 0 and not s.startswith("[ERROR") and s != "[EMPTY]"]
        empty_samples = len([s for s in samples if s == "[EMPTY]"])
        error_samples = len([s for s in samples if s.startswith("[ERROR")])

        sample_stats = {
            'total': len(samples),
            'valid': len(valid_samples),
            'empty': empty_samples,
            'error': error_samples,
            'valid_rate': len(valid_samples) / len(samples) if samples else 0
        }

        # ğŸ¯ Early Stopping é€»è¾‘ä¸è¯¦ç»†è¾“å‡º
        is_best = val_ppl < best_ppl
        if is_best:
            improvement = val_ppl - best_ppl  # è´Ÿæ•°è¡¨ç¤ºæ”¹è¿›
            best_ppl = val_ppl
            best_epoch = ep
            bad_epochs = 0
            status_icon = "ğŸ’¾"
            status_text = f"BEST! (â¬‡{abs(improvement):.3f})"
        else:
            bad_epochs += 1
            status_icon = "â³"
            status_text = f"No improvement ({bad_epochs}/{Config.patience})"

        # ğŸ¯ è®¡ç®—æ”¹è¿›è¶‹åŠ¿
        if len(ppl_history) >= 3:
            recent_trend = ppl_history[-1] - ppl_history[-3]
            trend_arrow = "ğŸ“ˆ" if recent_trend > 0 else "ğŸ“‰"
            trend_text = f"{trend_arrow} {recent_trend:+.3f}"
        else:
            trend_text = "ğŸ”„ åˆå§‹"

        # ğŸ¯ wandbæ—¥å¿—è®°å½• - æ¯ä¸ªepochéƒ½è®°å½•æ ·æœ¬
        if Config.use_wandb:
            log_dict = {
                'epoch': ep,
                'train_ppl': train_ppl,
                'val_ppl': val_ppl,
                'learning_rate': scheduler.get_last_lr()[0],
                'epoch_time': epoch_time,
                'elapsed_time': elapsed_time,
                'best_ppl': best_ppl,
                'patience_counter': bad_epochs,
                'patience_remaining': Config.patience - bad_epochs,
                'best_epoch': best_epoch,
                'is_best': is_best,

                # ğŸ¯ æ ·æœ¬ç»Ÿè®¡
                'sample_stats/total': sample_stats['total'],
                'sample_stats/valid': sample_stats['valid'],
                'sample_stats/empty': sample_stats['empty'],
                'sample_stats/error': sample_stats['error'],
                'sample_stats/valid_rate': sample_stats['valid_rate'],
            }

            # ğŸ¯ æ¯ä¸ªepochéƒ½æ·»åŠ æ ·æœ¬è¡¨æ ¼
            if samples and Config.save_samples:
                sample_table = wandb.Table(columns=["epoch", "sample", "length", "valid", "type"])
                for i, sample in enumerate(samples):
                    is_valid = sample in valid_samples
                    sample_type = "valid" if is_valid else ("empty" if sample == "[EMPTY]" else "error")
                    sample_table.add_data(ep, sample, len(sample), is_valid, sample_type)
                log_dict[f"samples"] = sample_table

                # ä¹Ÿè®°å½•æ ·æœ¬åˆ—è¡¨ï¼ˆç”¨äºæœç´¢ï¼‰
                log_dict['sample_list'] = samples

            wandb.log(log_dict)

        # ğŸ¯ ç´§å‡‘çš„ä¸€è¡Œç»“æœæ˜¾ç¤º + Early Stoppingä¿¡æ¯
        samples_str = " | ".join([f"'{s}'" for s in valid_samples[:3]])  # åªæ˜¾ç¤ºå‰3ä¸ªæœ‰æ•ˆæ ·æœ¬
        if len(valid_samples) > 3:
            samples_str += f" | +{len(valid_samples) - 3}more"
        if not samples_str:
            samples_str = f"æ— æœ‰æ•ˆæ ·æœ¬({sample_stats['empty']}ç©º, {sample_stats['error']}é”™)"

        print(f"ğŸ“Š Train PPL: {train_ppl:.3f} | Val PPL: {val_ppl:.3f} | LR: {scheduler.get_last_lr()[0]:.2e} | "
              f"Time: {epoch_time:.1f}s | {eval_type}è¯„ä¼°")
        print(f"ğŸ’­ Samples: {samples_str}")

        # ğŸ¯ Early Stopping è¯¦ç»†çŠ¶æ€
        print(f"ğŸ›‘ Early Stop: {status_icon} {status_text} | Best: {best_ppl:.3f} (Epoch {best_epoch}) | "
              f"Trend: {trend_text} | Remaining: {Config.patience - bad_epochs}")

        # Early stopping - ä¿å­˜æœ€ä½³æ¨¡å‹
        if is_best:
            # ä¿å­˜å®Œæ•´çš„é…ç½®å’Œè®­ç»ƒä¿¡æ¯
            config_dict = {
                **model_config.to_dict(),
                'batch_size': Config.batch_size,
                'lr': Config.lr,
                'warmup': Config.warmup,
                'decay_epochs': Config.decay_epochs,
                'min_lr': Config.min_lr,
                'epochs': Config.epochs,
                'patience': Config.patience,
                'grad_clip': Config.grad_clip,
                'weight_decay': Config.weight_decay
            }

            torch.save({
                'model_state_dict': model.state_dict(),
                'config': config_dict,
                'model_config': model_config.to_dict(),  # ä¸“é—¨çš„æ¨¡å‹é…ç½®
                'vocab_size': vocab_size,
                'tokenizer_config': {
                    'type': 'EnhancedCharacterTokenizer',
                    'vocab_size': vocab_size,
                    'special_tokens': {
                        'pad_token_id': tok.pad_token_id,
                        'bos_token_id': tok.bos_token_id,
                        'eos_token_id': tok.eos_token_id,
                        'unk_token_id': tok.unk_token_id,
                        'sep_token_id': tok.sep_token_id,
                    }
                },
                # ğŸ¯ ä¿å­˜æ•°æ®åˆ†å‰²ä¿¡æ¯
                'data_split_info': {
                    'source_file': args.data,
                    'train_file': train_file,
                    'val_file': val_file,
                    'train_ratio': Config.train_ratio,
                    'val_ratio': Config.val_ratio,
                    'random_seed': Config.random_seed,
                },
                'epoch': ep,
                'val_ppl': val_ppl,
                'train_ppl': train_ppl,  # ğŸ¯ ä¹Ÿä¿å­˜è®­ç»ƒPPL
                'train_time': elapsed_time,
                'param_count': param_count,
                'best_samples': samples,  # ğŸ¯ ä¿å­˜å½“å‰æœ€ä½³æ ·æœ¬
                'sample_stats': sample_stats,  # ğŸ¯ ä¿å­˜æ ·æœ¬ç»Ÿè®¡
                'best_epoch': best_epoch,
                'ppl_history': ppl_history,  # ğŸ¯ ä¿å­˜PPLå†å²
            }, args.out)

        # ğŸ¯ Early stopping è§¦å‘
        if bad_epochs >= Config.patience:
            print(f"\nğŸ›‘ EARLY STOPPING TRIGGERED!")
            print(f"   ğŸ”´ No improvement for {Config.patience} consecutive epochs")
            print(f"   ğŸ’¾ Best model saved at epoch {best_epoch} with PPL {best_ppl:.3f}")
            print(f"   ğŸ“Š PPL History (last 5): {ppl_history[-5:]}")
            break

    # ğŸ¯ è®­ç»ƒå®Œæˆæ€»ç»“
    total_time = time.time() - start_time
    final_status = "Early Stopped" if bad_epochs >= Config.patience else "Completed"

    print(f"\nğŸ† TRAINING {final_status.upper()}!")
    print(f"   ğŸ¯ Best validation perplexity: {best_ppl:.3f} (Epoch {best_epoch})")
    print(f"   ğŸ“Š Final validation perplexity: {val_ppl:.3f} (Epoch {ep})")
    print(f"   â±ï¸  Total training time: {total_time:.1f}s ({total_time / 60:.1f}min)")
    print(f"   ğŸ’¾ Model saved to: {args.out}")
    print(f"   ğŸ›‘ Early stopping: Used {bad_epochs}/{Config.patience} patience")
    print(f"   ğŸ“ Split files: {train_file}, {val_file}")

    # ğŸ¯ æ˜¾ç¤ºPPLæ”¹è¿›å†å²
    if len(ppl_history) > 1:
        total_improvement = ppl_history[0] - best_ppl
        print(f"   ğŸ“ˆ Total PPL improvement: {total_improvement:.3f} ({ppl_history[0]:.3f} â†’ {best_ppl:.3f})")

    if Config.use_wandb:
        # åˆ›å»ºæœ€ç»ˆæ±‡æ€»
        wandb.run.summary.update({
            'final_best_ppl': best_ppl,
            'final_val_ppl': val_ppl,
            'best_epoch': best_epoch,
            'total_training_time': total_time,
            'total_epochs': ep,
            'final_epoch': ep,
            'training_completed': final_status == "Completed",
            'early_stopped': final_status == "Early Stopped",
            'early_stop_patience_used': bad_epochs,
            'early_stop_patience_total': Config.patience,
            'model_size_mb': param_count * 4 / (1024 * 1024),
            'tokenizer_type': 'EnhancedCharacterTokenizer',
            'samples_per_epoch': Config.samples_per_epoch,
            'ppl_improvement': ppl_history[0] - best_ppl if len(ppl_history) > 1 else 0,
            # ğŸ¯ æ•°æ®åˆ†å‰²ä¿¡æ¯
            'data_split_seed': Config.random_seed,
            'train_ratio': Config.train_ratio,
            'val_ratio': Config.val_ratio,
        })

        # è‡ªåŠ¨æ³¨å†Œæœ€ä½³æ¨¡å‹ï¼ˆå¦‚æœæ•ˆæœè¶³å¤Ÿå¥½ï¼‰
        if best_ppl < 10.0:  # åªæœ‰è¶³å¤Ÿå¥½çš„æ¨¡å‹æ‰æ³¨å†Œ
            model_artifact = wandb.Artifact(
                name=f"model-{run.name}",
                type="model",
                description=f"Enhanced Character GPT with PPL {best_ppl:.3f} ({final_status})",
                metadata={
                    "val_ppl": best_ppl,
                    "best_epoch": best_epoch,
                    "final_epoch": ep,
                    "param_count": param_count,
                    "architecture": "transformer",
                    "tokenizer": "EnhancedCharacterTokenizer",
                    "samples_per_epoch": Config.samples_per_epoch,
                    "early_stopped": final_status == "Early Stopped",
                    "random_split": True,
                    "train_ratio": Config.train_ratio,
                    "val_ratio": Config.val_ratio,
                    **model_config.to_dict(),
                }
            )
            model_artifact.add_file(args.out)
            wandb.log_artifact(model_artifact, aliases=["latest", "best"])
            print(f"âœ… Model registered to wandb as model-{run.name}")

        wandb.finish()

    # ğŸ¯ æ¸…ç†ä¸´æ—¶æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
    if not args.keep_splits:
        try:
            os.remove(train_file)
            os.remove(val_file)
            os.rmdir(split_dir)
            print(f"ğŸ§¹ Cleaned up split files")
        except OSError:
            print(f"âš ï¸  Could not clean up split files")


if __name__ == "__main__":
    p = argparse.ArgumentParser(description="Enhanced Character GPT Pretraining with Random Data Splitting")

    # ğŸ¯ æ•°æ®å‚æ•° - æ”¹ä¸ºå•ä¸ªæ•°æ®æ–‡ä»¶
    p.add_argument("--data", default="dataset/180k_10k_10k_50k/pretrain.txt", help="Source data file for splitting")
    p.add_argument("--tokenizer", default="scripts/enhanced_tokenizer/tokenizer.json",
                   help="Enhanced tokenizer config file")
    p.add_argument("--out", default="checkpoints/enhanced_char_gpt_pretrain.pth", help="Output model file")

    # ğŸ¯ æ•°æ®åˆ†å‰²å‚æ•°
    p.add_argument("--train-ratio", type=float, default=0.95, help="Training data ratio (default: 0.8)")
    p.add_argument("--val-ratio", type=float, default=0.05, help="Validation data ratio (default: 0.1)")
    p.add_argument("--random-seed", type=int, default=42, help="Random seed for data splitting")
    p.add_argument("--keep-splits", action="store_true", help="Keep split files after training")

    # ğŸ”´ æ¨¡å‹æ¶æ„å‚æ•°
    p.add_argument("--n-layer", type=int, default=8, help="Number of transformer layers")
    p.add_argument("--d-model", type=int, default=256, help="Model dimension")
    p.add_argument("--n-head", type=int, default=8, help="Number of attention heads")
    p.add_argument("--dropout", type=float, default=0.15, help="Dropout rate")
    p.add_argument("--max-seq-len", type=int, default=64, help="Maximum sequence length")

    # ğŸ”´ è®­ç»ƒå‚æ•°
    p.add_argument("--lr", type=float, default=1e-3, help="Learning rate")
    p.add_argument("--batch-size", type=int, default=512, help="Batch size")
    p.add_argument("--epochs", type=int, default=50, help="Number of epochs")
    p.add_argument("--weight-decay", type=float, default=0.1, help="Weight decay")
    p.add_argument("--grad-clip", type=float, default=1.0, help="Gradient clipping")
    p.add_argument("--patience", type=int, default=8, help="Early stopping patience")
    p.add_argument("--warmup", type=int, default=300, help="Warmup steps")
    p.add_argument("--decay-epochs", type=int, default=30, help="Epochs before LR decay")
    p.add_argument("--min-lr", type=float, default=1e-6, help="Minimum learning rate")

    # wandbå‚æ•°
    p.add_argument("--no-wandb", action="store_true", help="Disable wandb logging")
    p.add_argument("--experiment-name", type=str, help="Custom experiment name")
    p.add_argument("--tags", nargs="+", help="Experiment tags")
    p.add_argument("--notes", help="Experiment notes")
    p.add_argument("--wandb-entity", help="Wandb entity/team name")

    args = p.parse_args()

    # ğŸ”´ åº”ç”¨å‘½ä»¤è¡Œå‚æ•°åˆ°Configç±»
    Config.batch_size = args.batch_size
    Config.lr = args.lr
    Config.epochs = args.epochs
    Config.weight_decay = args.weight_decay
    Config.grad_clip = args.grad_clip
    Config.patience = args.patience
    Config.warmup = args.warmup
    Config.decay_epochs = args.decay_epochs
    Config.min_lr = args.min_lr

    # ğŸ¯ æ•°æ®åˆ†å‰²é…ç½®
    Config.train_ratio = args.train_ratio
    Config.val_ratio = args.val_ratio
    Config.random_seed = args.random_seed

    # wandbé…ç½®
    if args.no_wandb:
        Config.use_wandb = False
    if args.experiment_name:
        Config.experiment_name = args.experiment_name
    if args.wandb_entity:
        Config.wandb_entity = args.wandb_entity
    if args.tags:
        Config.tags = args.tags
    if args.notes:
        Config.notes = args.notes

    main(args)